<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Feature Scaling - Machine Learning with TensorFlow</title>
        

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="An introductory look at implementing machine learning algorithms using TensorFlow in Python.">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="shortcut icon" href="/public/book_deps/favicon.png">
        <link rel="stylesheet" href="/public/book_deps/css/variables.css">
        <link rel="stylesheet" href="/public/book_deps/css/general.css">
        <link rel="stylesheet" href="/public/book_deps/css/chrome.css">
        <link rel="stylesheet" href="/public/book_deps/css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="/public/book_deps/FontAwesome/css/font-awesome.css">
        <link href="/public/book_deps/fonts/SourceCodePro/source-code-pro.css" rel="stylesheet" type="text/css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="/public/book_deps/css/highlight.css">
        <link rel="stylesheet" href="/public/book_deps/css/tomorrow-night.css">
        <link rel="stylesheet" href="/public/book_deps/css/ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    </head>
    <body class="light">
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_book_root = "../";
            var path_to_site_root = "/public/book_deps/";
            var default_theme = "light";
            window.is_book_mode = true;
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            document.body.className = theme;
            document.querySelector('html').className = theme + ' js';
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li><a href="../ch1-setup/intro.html"><strong aria-hidden="true">1.</strong> Introduction and Setup</a></li><li><ol class="section"><li><a href="../ch1-setup/mac.html"><strong aria-hidden="true">1.1.</strong> macOS Setup</a></li><li><a href="../ch1-setup/linux.html"><strong aria-hidden="true">1.2.</strong> Linux Setup</a></li><li><a href="../ch1-setup/windows.html"><strong aria-hidden="true">1.3.</strong> Windows Setup</a></li></ol></li><li><a href="../ch2-linreg/intro.html"><strong aria-hidden="true">2.</strong> Linear Regression</a></li><li><ol class="section"><li><a href="../ch2-linreg/2017-12-03-single-variable.html"><strong aria-hidden="true">2.1.</strong> Single Variable Regression</a></li><li><a href="../ch2-linreg/2017-12-27-optimization.html"><strong aria-hidden="true">2.2.</strong> Optimization Convergence</a></li><li><a href="../ch2-linreg/2018-03-21-multi-variable.html"><strong aria-hidden="true">2.3.</strong> Multi Variable Regression</a></li><li><a href="../ch2-linreg/2018-11-15-feature-scaling.html" class="active"><strong aria-hidden="true">2.4.</strong> Feature Scaling</a></li><li><a href="../ch2-linreg/nonlinear.html"><strong aria-hidden="true">2.5.</strong> Nonlinear Regression</a></li><li><a href="../ch2-linreg/regularization.html"><strong aria-hidden="true">2.6.</strong> Regularization</a></li><li><a href="../ch2-linreg/evaluation.html"><strong aria-hidden="true">2.7.</strong> Evaluation</a></li><li><a href="../ch2-linreg/mle.html"><strong aria-hidden="true">2.8.</strong> Bonus: Why we square errors</a></li><li class="spacer"></li></ol></li></ol>
                <ol class="chapter">
                    <li><a href="/"><i class="fa fa-arrow-circle-left"></i> Home</a></li>
                </ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar" class="menu-bar">
                    <div id="menu-bar-sticky-container">
                        <div class="left-buttons">
                            <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                                <i class="fa fa-bars"></i>
                            </button>
                            <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                                <i class="fa fa-paint-brush"></i>
                            </button>
                            <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                                <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                            </ul>
                            
                            <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                                <i class="fa fa-search"></i>
                            </button>
                            
                        </div>

                        <h1 class="menu-title">Machine Learning with TensorFlow</h1>

                        <div class="right-buttons">
                            <a href="../print.html" title="Print this book" aria-label="Print this book">
                                <i id="print-button" class="fa fa-print"></i>
                            </a>
                            
                        </div>
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<a class="header" href="#feature-scaling" id="feature-scaling"><h1>Feature Scaling</h1></a>
<p>In chapters <a href="/books/tensorflow/book/ch2-linreg/2017-12-03-single-variable.html">2.1</a>, <a href="/books/tensorflow/book/ch2-linreg/2017-12-27-optimization.html">2.2</a>, <a href="/books/tensorflow/book/ch2-linreg/2018-03-21-multi-variable.html">2.3</a> we used the gradient descent algorithm (or variants of) to minimize a loss function, and thus achieve a line of best fit. However, it turns out that the optimization in chapter 2.3 was much, much slower than it needed to be. While this isn’t a big problem for these fairly simple linear regression models that we can train in seconds anyways, this inefficiency becomes a much more drastic problem when dealing with large data sets and models.</p>
<a class="header" href="#example-of-the-problem" id="example-of-the-problem"><h2>Example of the Problem</h2></a>
<p>First, let’s look at a concrete example of the problem, by again considering a synthetic data set. Like in chapter 2.3 I generated a simple <a href="/books/tensorflow/book/ch2-linreg/code/linreg-scaling-synthetic.csv">synthetic data set</a> consisting of 2 independent variables \(x_1\) and \(x_2\), and one dependent variable \(y = 2x_1 + 0.013x_2\). However, note that the range for \(x_1\) is 0 to 10, but the range for \(x_2\) is 0 to 1000. Let's call this data set \(D\). A few sample data points look like this:</p>
<table><thead><tr><th> \(x_1\) </th><th> \(x_2\) </th><th> \(y\)</th></tr></thead><tbody>
<tr><td>7.36</td><td>    1000</td><td>        34.25</td></tr>
<tr><td>9.47</td><td>    0</td><td>            19.24</td></tr>
<tr><td>0.52</td><td>    315.78</td><td>        3.50</td></tr>
<tr><td>1.57</td><td>    315.78</td><td>        11.02</td></tr>
<tr><td>6.31</td><td>    263.15</td><td>        13.93</td></tr>
<tr><td>1.57</td><td>    526.31</td><td>        10.21</td></tr>
<tr><td>0.52</td><td>    105.26</td><td>        3.41</td></tr>
<tr><td>4.21</td><td>    842.10</td><td>        16.27</td></tr>
<tr><td>2.63</td><td>    894.73</td><td>        19.04</td></tr>
<tr><td>3.68</td><td>    210.52</td><td>        4.60</td></tr>
<tr><td> ... </td><td> ... </td><td> ... </td></tr>
</tbody></table>
<p>For simplification I have excluded a constant term from this synthetic data set, and when training models I &quot;cheated&quot; by not training the constant term \(b\) and just setting it to 0, so as to simplify visualization. The model that I used was simply:</p>
<pre><code class="language-python">y_predicted = tf.matmul(A, x) # A is a 1x2 matrix
</code></pre>
<p>Since this is just a simple application of the ideas from chapter 2.3, one would expect this to work fairly easily. However, when using a <code>GradientDescentOptimizer</code>, training goes rather poorly. Here is a plot showing the training progress of <code>A[0]</code> and the loss function side-by-side:</p>
<p><img src="/books/tensorflow/book/ch2-linreg/assets/scaling_plot1.png" alt="Training progress without scaling" /></p>
<p>This optimization was performed with a learning rate of 0.0000025, which is about the largest before it would diverge. The loss function quickly decreases at first, but then quickly stalls, and decreases quite slowly. Meanwhile, \(A_0\) is increasing towards the expected value of approximately 2.0, but very slowly. Within 5000 iterations this model fails to finish training.</p>
<p>Now let's do something rather strange: take the data set \(D\), and create a new data set \(D'\) by dividing all the \(x_2\) values by 100. The resulting data set \(D'\) looks roughly like this:</p>
<table><thead><tr><th> \(x_1'\) </th><th> \(x_2'\) </th><th> \(y'\)</th></tr></thead><tbody>
<tr><td>7.36</td><td>    10</td><td>        34.25</td></tr>
<tr><td>9.47</td><td>    0</td><td>            19.24</td></tr>
<tr><td>0.52</td><td>    3.1578</td><td>        3.50</td></tr>
<tr><td>1.57</td><td>    3.1578</td><td>        11.02</td></tr>
<tr><td>6.31</td><td>    2.6315</td><td>        13.93</td></tr>
<tr><td>1.57</td><td>    5.2631</td><td>        10.21</td></tr>
<tr><td>0.52</td><td>    1.0526</td><td>        3.41</td></tr>
<tr><td>4.21</td><td>    8.4210</td><td>        16.27</td></tr>
<tr><td>2.63</td><td>    8.9473</td><td>        19.04</td></tr>
<tr><td>3.68</td><td>    2.1052</td><td>        4.60</td></tr>
<tr><td> ... </td><td> ... </td><td> ... </td></tr>
</tbody></table>
<p>A crucial note is that while \(D'\) is technically different from \(D\), it contains exactly the same information: one can convert between them freely, by dividing or multiplying the second column by 100. In fact, since this transformation is linear, and we are using a linear model, we can train our model on \(D'\) instead. We would just expect to obtain a value of 1.3 for <code>A[1]</code> rather than 0.013. So let's give it a try!</p>
<p>The first interesting observation is that we can use a much larger learning rate. The largest learning rate we could use with \(D\) was 0.0000025, but with \(D'\) we can use a learning rate of 0.01. And when we plot <code>A[0]</code> and the loss function for both \(D\) and \(D'\) we see something pretty crazy:</p>
<p><img src="/books/tensorflow/book/ch2-linreg/assets/scaling_plot2.png" alt="Training progress with scaling" /></p>
<p>While training on \(D\) wasn't even close to done after 5000 iterations, training on \(D'\) seems to have completed nearly instantly. If we zoom in on the first 60 iterations, we can see the training more clearly:</p>
<p><img src="/books/tensorflow/book/ch2-linreg/assets/scaling_plot3.png" alt="Training progress with scaling zoom" /></p>
<p>So this incredibly simple data set transformation has changed a problem that was untrainable within 5000 iterations to one that can be trained practically instantly with 50 iterations. What is this black magic, and how does it work?</p>
<a class="header" href="#visualizing-gradient-descent-with-level-sets" id="visualizing-gradient-descent-with-level-sets"><h2>Visualizing Gradient Descent with Level Sets</h2></a>
<p>One of the best ways to gain insight in machine learning is by visualization. As seen in chapter 2.1 we can visualize loss functions using plots. Since we have 2 parameters <code>A[0]</code> and <code>A[1]</code> of the loss function, it would be a 3D plot. We used this for visualization in chapter 2.1, but frankly it's a bit messy looking. Instead, we will use <a href="https://en.wikipedia.org/wiki/Level_set">level sets</a> (also called contour plots), which use lines to indicate where the loss function has a constant value. An example is easiest, so here is the contour plot for the loss function for \(D'\), the one that converges quickly:</p>
<p><img src="/books/tensorflow/book/ch2-linreg/assets/contour2.png" alt="Contour plot of D'" /></p>
<p>Each ellipse border is where the loss function has a particular constant value (the innermost ones are labeled), and the red X marks the spot of the optimal values of <code>A[0]</code> and <code>A[1]</code>. By convention the particular constant values of the loss function are evenly spaced, which means that contour lines that are closer together indicate a &quot;steeper&quot; slope. In this plot, the center near the X is quite shallow, while far away is pretty steep. In addition, one diagonal axis of the ellipses is steeper than the other diagonal axis.</p>
<p>We can also plot how <code>A[0]</code> and <code>A[1]</code> evolve during training on the contour plot, to get a feel for how gradient descent is working:</p>
<p><img src="/books/tensorflow/book/ch2-linreg/assets/contour2_dots.png" alt="Contour plot of D' with training" /></p>
<p>Here, we can see that the initial values for <code>A[0]</code> and <code>A[1]</code> start pretty far off, but gradient descent quickly zeros in towards the X. As a side note, the line connecting each pair of dots is perpendicular to the line of the level set: see if you can figure out why.</p>
<p>So if that is what the contour plot looks like for the &quot;good&quot; case of \(D'\), how does it look for \(D\)? Well, it looks rather strange:</p>
<p><img src="/books/tensorflow/book/ch2-linreg/assets/contour1.png" alt="Contour plot of D" /></p>
<p>Don't be fooled: like the previous plot the level sets also form ellipses, but they are so stretched that they are nearly straight lines at this scale. This means that vertically (<code>A[0]</code> is constant and we vary <code>A[1]</code>) there is substantial gradient, but horizontally (<code>A[1]</code> is constant and we vary <code>A[0]</code>) there is practically no slope. Put another way, gradient descent only knows to vary <code>A[1]</code>, and (almost) doesn't vary <code>A[0]</code>.  We can test this hypothesis by plotting how gradient descent updates <code>A[0]</code> and <code>A[1]</code>, like above. Since gradient descent makes such little progress though, we have to zoom in a lot to see what is going on:</p>
<p><img src="/books/tensorflow/book/ch2-linreg/assets/contour1_dots.png" alt="Contour plot of D" /></p>
<p>We can clearly see that gradient descent applies large updates to <code>A[1]</code> (a bit too large, a smaller learning rate would have been a bit better) due to the large gradient in the <code>A[1]</code> direction. But due to the (comparatively) tiny gradient in the <code>A[0]</code> direction very small updates are done to <code>A[0]</code>. Gradient descent quickly converges on the optimal value of <code>A[1]</code>, but is very very far away from finding the optimal value of <code>A[0]</code>.</p>
<p>Let's take a quick look at what is going on mathematically to see why this happens. The model we are using is:
\[
y'(x, A) = Ax = a_1 x_1 + a_2 x_2
\]
Here, \(a_1\) is in the role of <code>A[0]</code> and \(a_2\) is <code>A[1]</code>. We substitute this into the loss function to get:
\[
L(a_1, a_2) = \sum_{i=1}^m (a_1 x_1^{(i)} + a_2 x_2^{(i)} - y^{(i)})^2
\]
Now if we differentiate \(L\) in the direction of \(a_1\) and \(a_2\) separately, we get:
\[
\frac{\partial L}{\partial a_1} = \sum_{i=1}^m 2(a_1 x_1^{(i)} + a_2 x_2^{(i)} - y^{(i)})x_1^{(i)} \\
\frac{\partial L}{\partial a_2} = \sum_{i=1}^m 2(a_1 x_1^{(i)} + a_2 x_2^{(i)} - y^{(i)})x_2^{(i)}
\]
Here we can see the problem: the inner terms of the derivatives are the same, except one is multiplied by \(x_1^{(i)}\) and the other by \(x_2^{(i)}\). If \(x_2\) is on average 100 times bigger than \(x_1\) (which it is in the original data set), then we would expect \(\frac{\partial L}{\partial a_2}\) to be roughly 100 times bigger than \(\frac{\partial L}{\partial a_1}\). It isn't exactly 100 times larger, but with any reasonable data set it should be close. Since the derivatives in the directions of \(a_1\) and \(a_2\) are scaled completely differently, gradient descent fails to update both of them adequately.</p>
<p>The solution is simple: we need to rescale the input features before training. This is exactly what happened when we mysteriously divided by 100: we rescaled \(x_2\) to be comparable to \(x_1\). But we should work out a more methodical way of rescaling, rather than randomly dividing by 100.</p>
<a class="header" href="#rescaling-features" id="rescaling-features"><h2>Rescaling Features</h2></a>
<p>This wasn't explored above, but there are really two ways that we potentially need to rescale features. Consider an example where \(x_1\) ranges between -1 and 1, but \(x_2\) ranges between 99 and 101: both of these features have (at least approximately) the same <a href="https://en.wikipedia.org/wiki/Standard_deviation">standard deviation</a>, but \(x_2\) has a much larger <a href="https://en.wikipedia.org/wiki/Expected_value">mean</a>. On the other hand, consider an example where \(x_1\) still ranges between -1 and 1, but \(x_2\) ranges between -100 and 100. This time, they have the same mean, but \(x_2\) has a much larger standard deviation. Both of these situations can make gradient descent and related algorithms slower and less reliable. So, our goal is to ensure that all features have the same mean and standard deviation.</p>
<blockquote>
<p><strong>Note:</strong> There are other methods to measure how different features are, and then subsequently rescale them. For a look at more of them, feel free to consult the <a href="https://en.wikipedia.org/wiki/Feature_scaling">Wikipedia article</a>. However, after implementing the technique presented here, any other technique is fairly similar and easy to implement if needed.</p>
</blockquote>
<p>Without digressing too far into statistics, let's quickly review how to calculate the mean \(\mu\) and standard deviation \(\sigma\). Suppose we have already read in our data set in Python into a Numpy vector / matrix, and have all the values for the feature \(x_j\), for each \(j\). Mathematically, the mean for feature \(x_j\) is just the average:
\[
\mu_j = \frac{\sum_{i=1}^m x_j^{(i)}}{m}
\]
In Numpy there is a convenient <code>np.mean()</code> function we will use.</p>
<p>The standard deviation \(\sigma\) is a bit more tricky. We measure how far each point \(x_j^{(i)}\) is from the mean \(\mu_j\), square this, then take the mean of all of this, and finally square root it:
\[
\sigma_j = \sqrt{\frac{\sum_{i=1}^m (x_j^{(i)} - \mu_j)^2 }{m}}
\]
Again, Numpy provides a convenient <code>np.std()</code> function.</p>
<blockquote>
<p><strong>Note:</strong> Statistics nerds might point out that in the above equation we should divide by \(m - 1\) instead of \(m\) to obtain an unbiased estimate of the standard deviation. This might well be true, but in this usage it does not matter since we are only using this to rescale features relative to each other, and not make a rigorous statistical inference. To be more precise, doing so would involve multiplying each scaled feature by only a constant factor, and will not change any of their standard deviations or means relative to each other.</p>
</blockquote>
<p>Once we have every mean \(\mu_j\) and standard deviation \(\sigma_j\), rescaling is easy: we simply rescale every feature like so:</p>
<p>\[
\begin{equation} \label{eq:scaling}
x_j' = \frac{x_j - \mu_j}{\sigma_j}
\end{equation}
\]</p>
<p>This will force every feature to have a mean of 0 and a standard deviation of 1, and thus be scaled well relative to each other.</p>
<blockquote>
<p><strong>Note:</strong> Be careful to make sure to perform the rescaling at both training time and prediction time. That is, we first have to perform the rescaling on the whole training data set, and then train the model so as to achieve good training performance. Once the model is trained and we have a new, never before seen input \(x\), we also need to rescale its features to \(x'\) because the trained model only understands inputs that have already been rescaled (because we trained it that way).</p>
</blockquote>
<a class="header" href="#implementation-and-experiments" id="implementation-and-experiments"><h2>Implementation and Experiments</h2></a>
<p>Feature scaling can be applied to just about any multi-variable model. Since the only multi-variable model we have seen so far is multi-variable linear regression, we'll look at implementing feature scaling in that context, but the ideas and the code are pretty much the same for other models. First let's just copy-paste the original multi-variable linear regression code, and modify it to load the <a href="/books/tensorflow/book/ch2-linreg/code/linreg-scaling-synthetic.csv">new synthetic data set</a>:</p>
<pre><code class="language-python">import numpy as np
import tensorflow as tf
import pandas as pd
import matplotlib.pyplot as plt

# First we load the entire CSV file into an m x n matrix
D = np.matrix(pd.read_csv(&quot;linreg-scaling-synthetic.csv&quot;, header=None).values)

# Make a convenient variable to remember the number of input columns
n = 2

# We extract all rows and the first n columns into X_data
# Then we flip it
X_data = D[:, 0:n].transpose()

# We extract all rows and the last column into y_data
# Then we flip it
y_data = D[:, n].transpose()



# Define data placeholders
x = tf.placeholder(tf.float32, shape=(n, None))
y = tf.placeholder(tf.float32, shape=(1, None))

# Define trainable variables
A = tf.get_variable(&quot;A&quot;, shape=(1, n))
b = tf.get_variable(&quot;b&quot;, shape=())

# Define model output
y_predicted = tf.matmul(A, x) + b

# Define the loss function
L = tf.reduce_sum((y_predicted - y)**2)

# Define optimizer object
optimizer = tf.train.AdamOptimizer(learning_rate=0.1).minimize(L)

# Create a session and initialize variables
session = tf.Session()
session.run(tf.global_variables_initializer())

# Main optimization loop
for t in range(2000):
    _, current_loss, current_A, current_b = session.run([optimizer, L, A, b], feed_dict={
        x: X_data,
        y: y_data
    })
    print(&quot;t = %g, loss = %g, A = %s, b = %g&quot; % (t, current_loss, str(current_A), current_b))

</code></pre>
<p>If you run this code right now, you might see output like the following:</p>
<pre><code>...
t = 1995, loss = 0.505296, A = [[1.9920335  0.01292674]], b = 0.089939
t = 1996, loss = 0.5042, A = [[1.9920422  0.01292683]], b = 0.0898404
t = 1997, loss = 0.5031, A = [[1.9920509  0.01292691]], b = 0.0897419
t = 1998, loss = 0.501984, A = [[1.9920596  0.01292699]], b = 0.0896435
t = 1999, loss = 0.50089, A = [[1.9920683  0.01292707]], b = 0.0895451
</code></pre>
<p>Note that if you run this code multiple times you will get different results each time due to the random initialization. The synthetic data was generated with the equation \(y = 2x_1 + 0.013x_2 + 0\). So after 2000 iterations of training we are getting close-ish to the correct values, but it's not fully trained. So let's implement feature scaling to fix this.</p>
<p>The first step is to compute the mean and standard deviation for each feature in the training data set. Add the following after <code>X_data</code> is loaded:</p>
<pre><code class="language-python">means = X_data.mean(axis=1)
deviations = X_data.std(axis=1)
</code></pre>
<p>Using <code>axis=1</code> means that we compute the mean for each feature, averaged over all data points. That is, <code>X_data</code> is a \(2 \times 400\) matrix, <code>means</code> is a \(2 \times 1\) matrix (vector). If we had used <code>axis=0</code>, <code>means</code> would be a \(1 \times 400\) matrix, which is not what we want.</p>
<p>Now that we know the means and standard deviations of each feature, we have to use them to transform the inputs via Equation \((\ref{eq:scaling})\). We have two options: we could implement the transformation directly using <code>numpy</code> to transform <code>X_data</code> before training, or we could include the transformation within the TensorFlow computations. But since we need to do the transformation again when we want to predict output given new inputs, including it in the TensorFlow computations will be a bit more convenient.</p>
<p>Just like before we setup <code>x</code> as a placeholder, so this code is identical:</p>
<pre><code class="language-python"># Define data placeholders
x = tf.placeholder(tf.float32, shape=(n, None))
</code></pre>
<p>Now we want to transform <code>x</code> according to Equation \((\ref{eq:scaling})\). We can define a new TensorFlow value <code>x_scaled</code>:</p>
<pre><code class="language-python"># Apply the rescaling
x_scaled = (x - means) / deviations
</code></pre>
<p>One important concept is that when this line of code runs, <code>means</code> and <code>deviations</code> have already been computed: they are actual matrices. So to TensorFlow they are <strong>constants</strong>: they are neither trainable variables that will be updated during optimization, nor are they placeholders that need to be fed values later. They have already been computed, and TensorFlow just uses the already computed values directly.</p>
<p>Also, note that since <code>x</code> and <code>means</code> are compatibly sized matrices, the subtraction (and division) will be done separately for each feature, automatically. So while Equation \((\ref{eq:scaling})\) technically says how to scale one feature individually, this single line of code scales all the features.</p>
<p>Now, everywhere that we use <code>x</code> in the model we should now use <code>x_scaled</code> instead. For us the only code that needs to change is in defining the model's prediction:</p>
<pre><code class="language-python"># Define model output, using the scaled features
y_predicted = tf.matmul(A, x_scaled) + b
</code></pre>
<p>Note that when we run the session we do <em>not</em> feed values to <code>x_scaled</code>, because we want to feed unscaled values to <code>x</code> which will then automatically get scaled when <code>x_scaled</code> is computed based on what is fed to <code>x</code>. We instead continue to feed values to <code>x</code> as before.</p>
<p>That's all that we need to implement for feature scaling, it's really pretty easy. If we run this code now we should see:</p>
<pre><code>...
t = 1995, loss = 1.17733e-07, A = [[6.0697703 3.9453504]], b = 16.5
t = 1996, loss = 1.17733e-07, A = [[6.0697703 3.9453504]], b = 16.5
t = 1997, loss = 1.17733e-07, A = [[6.0697703 3.9453504]], b = 16.5
t = 1998, loss = 1.17733e-07, A = [[6.0697703 3.9453504]], b = 16.5
t = 1999, loss = 1.17733e-07, A = [[6.0697703 3.9453504]], b = 16.5
</code></pre>
<p>The fact that none of the trained weights are updating anymore and that the loss function is very small is a good sign that we have achieved convergence. But the weights are not the values we expect... shouldn't we get <code>A = [[2, 0.013]], b = 0</code>, since that is the equation that generated the synthetic data? Actually, the weights we got <em>are</em> correct because the model is now being trained on the rescaled data set, so we get different weights at the end. See Challenge Problem 3 below to explore this more.</p>
<a class="header" href="#concluding-remarks" id="concluding-remarks"><h1>Concluding Remarks</h1></a>
<p>We've seen how to implement feature scaling for a simple multi-variable linear regression model.  While this is a fairly simple model, the principles are basically the same for applying feature scaling to more complex models (which we will see very soon). In fact, the code is pretty much identical: just compute the means and standard deviations, apply the formula to <code>x</code> to compute <code>x_scaled</code>, and anywhere you use <code>x</code> in the model, just use <code>x_scaled</code> instead.</p>
<p>Whether or not feature scaling helps is dependent on the problem and the model. If you have having trouble getting your model to converge, you can always implement feature scaling and see how it affects the training. While it wasn't used in this chapter, using TensorBoard (see <a href="/books/tensorflow/book/ch2-linreg/2017-12-27-optimization.html">chapter 2.2</a>) is a great way to run experiments to compare training with and without feature scaling.</p>
<a class="header" href="#challenge-problems" id="challenge-problems"><h1>Challenge Problems</h1></a>
<!-- <ol>
    <li>**Coding:** Take one of the challenge problems from [the previous chapter](/books/tensorflow/book/ch2-linreg/2018-03-21-multi-variable.html#challenge-problems), and implement it with and without feature scaling, and then compare how training performs.</li>
</ol> -->
<ol>
<li><strong>Coding:</strong> Take one of the challenge problems from <a href="/books/tensorflow/book/ch2-linreg/2018-03-21-multi-variable.html#challenge-problems">the previous chapter</a>, and implement it with and without feature scaling, and then compare how training performs.</li>
<li><strong>Coding:</strong> Add TensorBoard support to the code from this chapter, and use it to explore for yourself the effect feature scaling has. Also, you can compare with different optimization algorithms.</li>
<li><strong>Theory:</strong> One problem with feature scaling is that we learn different model parameters. In this chapter, the original data set was generated with \(y = 2x_1 + 0.013x_2 + 0\), but the parameters that the model learned were \(a_1' = 6.0697703, a_2' = 3.9453504, b' = 16.5\). Note that I have called these learned parameters \(a_1'\) rather than \(a_1\) to indicate that these learned parameters are for the <em>rescaled</em> data.<br/><br />One important use of linear regression is to explore and understand a data set, not just predict outputs. After doing a regression, one can understand through the learned weights how severely each feature affects the output. For example, if we have two features \(A\) and \(B\) which are used to predict rates of cancer, and the learned weight for \(A\) is much larger than the learned weight for \(B\), this indicates that occurrence of \(A\) is more correlated with cancer than occurrence of \(B\) is, which is interesting in its own right. Unfortunately, performing feature scaling destroys this: since we have rescaled the training data, the weight for \(A\) (\(a_1'\)) has lost its relevance to the values of \(A\) in the real world. But all is not lost, since we can actually recover the &quot;not-rescaled&quot; parameters from the learned parameters, which we will derive now.<br/><br/>First, suppose that we have learned the &quot;rescaled&quot; parameters \(a_1', a_2', b'\). That is, we predict the output \(y\) to be: \[\begin{equation}\label{eq:start}y = a_1' x_1' + a_2' x_2' + b'\end{equation}\]where \(x_1', x_2'\) are the rescaled features. Now, we <em>want</em> a model that uses some (yet unknown) weights \(a_1, a_2, b\) to predict the same output \(y\), but using the not-rescaled features \(x_1, x_2\). That is, we want to find \(a_1, a_2, b\) such that this is true:\[\begin{equation}\label{eq:goal}y = a_1 x_1 + a_2 x_2 + b\end{equation}\]To go about doing this, substitute for \(x_1'\) and \(x_2'\) using Equation \((\ref{eq:scaling})\) into Equation \((\ref{eq:start})\), and then compare it to Equation \((\ref{eq:goal})\) to obtain 3 equations: one to relate each scaled and not-scaled parameter. Finally, you can use these equations to be able to compute the &quot;not-scaled&quot; parameters in terms of the learned parameters. You can check your work by computing the not-scaled parameters in terms of the learned parameters for the synthetic data set, and verify that they match the expected values.</li>
</ol>
<a class="header" href="#complete-code" id="complete-code"><h1>Complete Code</h1></a>
<p>The <a href="https://github.com/donald-pinckney/donald-pinckney.github.io/blob/src/books/tensorflow/src/ch2-linreg/code/feature_scaling.py">complete example code is available on GitHub</a>, as well as directly here:</p>
<pre><code class="language-python">import numpy as np
import tensorflow as tf
import pandas as pd
import matplotlib.pyplot as plt

# First we load the entire CSV file into an m x n matrix
D = np.matrix(pd.read_csv(&quot;linreg-scaling-synthetic.csv&quot;, header=None).values)

# Make a convenient variable to remember the number of input columns
n = 2

# We extract all rows and the first n columns into X_data
# Then we flip it
X_data = D[:, 0:n].transpose()

# We extract all rows and the last column into y_data
# Then we flip it
y_data = D[:, n].transpose()

# We compute the mean and standard deviation of each feature
means = X_data.mean(axis=1)
deviations = X_data.std(axis=1)

print(means)
print(deviations)

# Define data placeholders
x = tf.placeholder(tf.float32, shape=(n, None))
y = tf.placeholder(tf.float32, shape=(1, None))

# Apply the rescaling
x_scaled = (x - means) / deviations

# Define trainable variables
A = tf.get_variable(&quot;A&quot;, shape=(1, n))
b = tf.get_variable(&quot;b&quot;, shape=())

# Define model output, using the scaled features
y_predicted = tf.matmul(A, x_scaled) + b

# Define the loss function
L = tf.reduce_sum((y_predicted - y)**2)

# Define optimizer object
optimizer = tf.train.AdamOptimizer(learning_rate=0.1).minimize(L)

# Create a session and initialize variables
session = tf.Session()
session.run(tf.global_variables_initializer())

# Main optimization loop
for t in range(2000):
    _, current_loss, current_A, current_b = session.run([optimizer, L, A, b], feed_dict={
        x: X_data,
        y: y_data
    })
    print(&quot;t = %g, loss = %g, A = %s, b = %g&quot; % (t, current_loss, str(current_A), current_b))

</code></pre>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        
                            <a rel="prev" href="../ch2-linreg/2018-03-21-multi-variable.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>
                        

                        
                            <a rel="next" href="../ch2-linreg/nonlinear.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>
                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                
                    <a href="../ch2-linreg/2018-03-21-multi-variable.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>
                

                
                    <a href="../ch2-linreg/nonlinear.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
                
            </nav>

        </div>

        

        

        

        
        <script src="/public/book_deps/js/elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="/public/book_deps/js/mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="/public/book_deps/js/searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="/public/book_deps/js/clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="/public/book_deps/js/highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="/public/book_deps/js/book.js" type="text/javascript" charset="utf-8"></script>
        <script src="/public/js/code-snippets.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        

    </body>
</html>
