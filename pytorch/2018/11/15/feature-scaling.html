<!DOCTYPE html>
<html lang="en-us">

  <head>
  <!-- <link href="https://gmpg.org/xfn/11" rel="profile"> -->
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="/public/fonts/abril_pt.css">
  <!-- <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface"> -->

  


  <!-- CMU Fonts -->
  <style type="text/css">
    @font-face {
      font-family: "Computer Modern";
      src: url('/public/fonts/cmunss.otf');
    }
    @font-face {
      font-family: "Computer Modern";
      src: url('/public/fonts/cmunsx.otf');
      font-weight: bold;
    }
    @font-face {
      font-family: "Computer Modern";
      src: url('/public/fonts/cmunsi.otf');
      font-style: italic, oblique;
    }
    @font-face {
      font-family: "Computer Modern";
      src: url('/public/fonts/cmunbxo.otf');
      font-weight: bold;
      font-style: italic, oblique;
    }
    @font-face {
      font-family: "Classical Computer Modern";
      src: url('/public/fonts/cmunci.otf');
      /* font-style: italic, oblique; */
    }
  </style>

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- <link rel="stylesheet" href="/public/book_deps/FontAwesome/css/font-awesome.css"> -->
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">

  <!-- MathJax -->
  <!-- <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML'></script> -->
  <!-- <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML'></script> -->
  <!-- <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"],
        bcancel: ["Extension","cancel"],
        xcancel: ["Extension","cancel"],
        cancelto: ["Extension","cancel"]
      });
    });
  </script> -->

  <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>



  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- jQuery -->
  <script src="/public/js/jquery.js"></script>

  <!-- TOC -->
  <script src="/public/js/toc.js"></script>

  <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Feature Scaling | Donald Pinckney</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Feature Scaling" />
<meta name="author" content="Donald Pinckney" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="1st year programming languages PhD student at UMass Amherst." />
<meta property="og:description" content="1st year programming languages PhD student at UMass Amherst." />
<link rel="canonical" href="https://donaldpinckney.com/pytorch/2018/11/15/feature-scaling.html" />
<meta property="og:url" content="https://donaldpinckney.com/pytorch/2018/11/15/feature-scaling.html" />
<meta property="og:site_name" content="Donald Pinckney" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-11-15T00:00:00-08:00" />
<script type="application/ld+json">
{"headline":"Feature Scaling","dateModified":"2018-11-15T00:00:00-08:00","datePublished":"2018-11-15T00:00:00-08:00","url":"https://donaldpinckney.com/pytorch/2018/11/15/feature-scaling.html","mainEntityOfPage":{"@type":"WebPage","@id":"https://donaldpinckney.com/pytorch/2018/11/15/feature-scaling.html"},"author":{"@type":"Person","name":"Donald Pinckney"},"description":"1st year programming languages PhD student at UMass Amherst.","@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

</head>


  <body class="theme-base-0b">

    <div class="sidebar">
  <div class="container">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Donald Pinckney
        </a>
      </h1>
      <p class="lead">1st year programming languages PhD student at UMass Amherst.</p>
    </div>

    <nav class="sidebar-nav">
      <div class="horizontal-block">
        <div class="horizontal-item">
        </div>
        
        

        <a class="horizontal-item sidebar-nav-item" href="/">Home</a>

        

        
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
          
        
          
            
              
              <a class="horizontal-item sidebar-nav-item " href="/publications.html">Publications</a>
            
          
        
          
            
          
        
          
            
          
        
          
        
          
            
          
        
          
            
          
        
          
        
          
        

        <a class="horizontal-item sidebar-nav-item active" href="/blog/">Blog Posts</a>

        <a class="horizontal-item sidebar-nav-item" href="/books/pytorch/book/ch1-setup/intro.html">PyTorch Guide</a>

        <div class="horizontal-item">
        </div>
      </div>

      <hr />

      <div class="horizontal-block">
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        


        <a href="https://github.com/donald-pinckney" class="sidebar-nav-item horizontal-item social" target="_blank">
          <i class="fa fa-github"></i><span class="only-desktop"> donald-pinckney</span>
        </a>
        <a href="https://twitter.com/donald_pinckney" class="sidebar-nav-item horizontal-item social" target="_blank">
          <i class="fa fa-twitter"></i><span class="only-desktop">@donald_pinckney</span>
        </a>
        <a href="mailto:donald_pinckney@icloud.com" class="sidebar-nav-item horizontal-item social" target="_blank">
          <i class="fa fa-envelope"></i>&nbsp;<span class="only-desktop">donald_pinckney@icloud.com</span>
        </a>
        <a href="/public/files/documents/resume.pdf" class="sidebar-nav-item horizontal-item social" target="_blank">
            <i class="fa fa-file"></i>&nbsp;<span>Curriculum vitae</span>
        </a>

      </div>
    </nav>

    <div class="sidebar-footnote">
      <p class="sidebar-footnote">
        &copy; 2021 Donald Pinckney. All rights reserved.
        <!-- <br />
        The views and opinions expressed here are my own, and do not reflect the views of Apple, Inc. -->
      </p>
    </div>
  </div>
</div>


    <div class="content container">
      <div class="post">

  
    <h1 class="post-title titular floating-box-left">Feature Scaling</h1>
  
  
    <div class="floating-box-right">
      
        
          <a href="https://github.com/donald-pinckney/donald-pinckney.github.io/blob/src/books/pytorch/src/ch2-linreg/2018-11-15-feature-scaling.md" target="_blank">
        
        <i class="fa fa-github"></i>Edit this page</a><br />
      

      
        <a href="https://github.com/donald-pinckney/donald-pinckney.github.io/issues/21" target="_blank">
          <i class="fa fa-bullhorn"></i>Subscribe to new posts
        </a>
      
    </div> 

  

  <span class="post-date">15 Nov 2018

  <div class="post-categories">
  
  
  <a href="/books/pytorch/book">Read this post in book format instead</a>
  
  
</div>

</span>


<div id="toc"></div>
<script type="text/javascript">
  $(document).ready(function() {
    // alert("READY");
      $('#toc').toc();
  });
</script>


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<h1 id="feature-scaling">Feature Scaling</h1>
<p>In chapters <a href="/books/pytorch/book/ch2-linreg/2017-12-03-single-variable.html">2.1</a>, <a href="/books/pytorch/book/ch2-linreg/2017-12-27-optimization.html">2.2</a>, <a href="/books/pytorch/book/ch2-linreg/2018-03-21-multi-variable.html">2.3</a> we used the gradient descent algorithm (or variants of) to minimize a loss function, and thus achieve a line of best fit. However, it turns out that the optimization in chapter 2.3 was much, much slower than it needed to be. While this isn’t a big problem for these fairly simple linear regression models that we can train in seconds anyways, this inefficiency becomes a much more drastic problem when dealing with large data sets and models.</p>
<h2 id="example-of-the-problem">Example of the Problem</h2>
<p>First, let’s look at a concrete example of the problem, by again considering a synthetic data set. Like in chapter 2.3 I generated another simple <a href="/books/pytorch/book/ch2-linreg/code/linreg-scaling-synthetic.csv">synthetic data set</a> consisting of 2 independent variables \(x_1\) and \(x_2\), and one dependent variable \(y = 2x_1 + 0.013x_2\). However, note that the range for \(x_1\) is 0 to 10, but the range for \(x_2\) is 0 to 1000. Let&#39;s call this data set \(D\). A few sample data points look like this:</p>

<table><thead>
<tr>
<th>\(x_1\)</th>
<th>\(x_2\)</th>
<th>\(y\)</th>
</tr>
</thead><tbody>
<tr>
<td>7.36</td>
<td>1000</td>
<td>34.25</td>
</tr>
<tr>
<td>9.47</td>
<td>0</td>
<td>19.24</td>
</tr>
<tr>
<td>0.52</td>
<td>315.78</td>
<td>3.50</td>
</tr>
<tr>
<td>1.57</td>
<td>315.78</td>
<td>11.02</td>
</tr>
<tr>
<td>6.31</td>
<td>263.15</td>
<td>13.93</td>
</tr>
<tr>
<td>1.57</td>
<td>526.31</td>
<td>10.21</td>
</tr>
<tr>
<td>0.52</td>
<td>105.26</td>
<td>3.41</td>
</tr>
<tr>
<td>4.21</td>
<td>842.10</td>
<td>16.27</td>
</tr>
<tr>
<td>2.63</td>
<td>894.73</td>
<td>19.04</td>
</tr>
<tr>
<td>3.68</td>
<td>210.52</td>
<td>4.60</td>
</tr>
<tr>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
</tbody></table>

<p>For simplification I have excluded a constant term from this synthetic data set, and when training models I &quot;cheated&quot; by not training the constant term \(b\) and just setting it to 0, so as to simplify visualization. The model that I used was simply:</p>
<div class="highlight"><pre class="highlight python"><code><span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">x_input</span><span class="p">):</span>
    <span class="n">y_predicted</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">x_input</span><span class="p">)</span> <span class="c1"># A is a 1x2 matrix
</span></code></pre></div>
<p>Since this is just a simple application of the ideas from chapter 2.3, one would expect this to work fairly easily. However, when using an <code>optim.SGD</code> optimizer, training goes rather poorly. Here is a plot showing the training progress of <code>A[0]</code> and the loss function side-by-side:</p>

<p><img src="/books/pytorch/book/ch2-linreg/assets/scaling_plot1.png" alt="Training progress without scaling"></p>

<p>This optimization was performed with a learning rate of 0.0000025, which is about the largest before it would diverge. The loss function quickly decreases at first, but then quickly stalls, and decreases quite slowly. Meanwhile, \(A_0\) is increasing towards the expected value of approximately 2.0, but very slowly. Within 5000 iterations this model fails to finish training.</p>

<p>Now let&#39;s do something rather strange: take the data set \(D\), and create a new data set \(D&#39;\) by dividing all the \(x_2\) values by 100. The resulting data set \(D&#39;\) looks roughly like this:</p>

<table><thead>
<tr>
<th>\(x_1&#39;\)</th>
<th>\(x_2&#39;\)</th>
<th>\(y&#39;\)</th>
</tr>
</thead><tbody>
<tr>
<td>7.36</td>
<td>10</td>
<td>34.25</td>
</tr>
<tr>
<td>9.47</td>
<td>0</td>
<td>19.24</td>
</tr>
<tr>
<td>0.52</td>
<td>3.1578</td>
<td>3.50</td>
</tr>
<tr>
<td>1.57</td>
<td>3.1578</td>
<td>11.02</td>
</tr>
<tr>
<td>6.31</td>
<td>2.6315</td>
<td>13.93</td>
</tr>
<tr>
<td>1.57</td>
<td>5.2631</td>
<td>10.21</td>
</tr>
<tr>
<td>0.52</td>
<td>1.0526</td>
<td>3.41</td>
</tr>
<tr>
<td>4.21</td>
<td>8.4210</td>
<td>16.27</td>
</tr>
<tr>
<td>2.63</td>
<td>8.9473</td>
<td>19.04</td>
</tr>
<tr>
<td>3.68</td>
<td>2.1052</td>
<td>4.60</td>
</tr>
<tr>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
</tbody></table>

<p>A crucial note is that while \(D&#39;\) is technically different from \(D\), it contains exactly the same information: one can convert between them freely, by dividing or multiplying the second column by 100. In fact, since this transformation is linear, and we are using a linear model, we can train our model on \(D&#39;\) instead. We would just expect to obtain a value of 1.3 for <code>A[1]</code> rather than 0.013. So let&#39;s give it a try!</p>

<p>The first interesting observation is that we can use a much larger learning rate. The largest learning rate we could use with \(D\) was 0.0000025, but with \(D&#39;\) we can use a learning rate of 0.01. And when we plot <code>A[0]</code> and the loss function for both \(D\) and \(D&#39;\) we see something pretty crazy:</p>

<p><img src="/books/pytorch/book/ch2-linreg/assets/scaling_plot2.png" alt="Training progress with scaling"></p>

<p>While training on \(D\) wasn&#39;t even close to done after 5000 iterations, training on \(D&#39;\) seems to have completed nearly instantly. If we zoom in on the first 60 iterations, we can see the training more clearly:</p>

<p><img src="/books/pytorch/book/ch2-linreg/assets/scaling_plot3.png" alt="Training progress with scaling zoom"></p>

<p>So this incredibly simple data set transformation has changed a problem that was untrainable within 5000 iterations to one that can be trained practically instantly with 50 iterations. What is this black magic, and how does it work?</p>
<h2 id="visualizing-gradient-descent-with-level-sets">Visualizing Gradient Descent with Level Sets</h2>
<p>One of the best ways to gain insight in machine learning is by visualization. As seen in chapter 2.1 we can visualize loss functions using plots. Since we have 2 parameters <code>A[0]</code> and <code>A[1]</code> of the loss function, it would be a 3D plot. We used this for visualization in chapter 2.1, but frankly it&#39;s a bit messy looking. Instead, we will use <a href="https://en.wikipedia.org/wiki/Level_set">level sets</a> (also called contour plots), which use lines to indicate where the loss function has a constant value. An example is easiest, so here is the contour plot for the loss function for \(D&#39;\), the one that converges quickly:</p>

<p><img src="/books/pytorch/book/ch2-linreg/assets/contour2.png" alt="Contour plot of D&#39;"></p>

<p>Each ellipse border is where the loss function has a particular constant value (the innermost ones are labeled), and the red X marks the spot of the optimal values of <code>A[0]</code> and <code>A[1]</code>. By convention the particular constant values of the loss function are evenly spaced, which means that contour lines that are closer together indicate a &quot;steeper&quot; slope. In this plot, the center near the X is quite shallow, while far away is pretty steep. In addition, one diagonal axis of the ellipses is steeper than the other diagonal axis.</p>

<p>We can also plot how <code>A[0]</code> and <code>A[1]</code> evolve during training on the contour plot, to get a feel for how gradient descent is working:</p>

<p><img src="/books/pytorch/book/ch2-linreg/assets/contour2_dots.png" alt="Contour plot of D&#39; with training"></p>

<p>Here, we can see that the initial values for <code>A[0]</code> and <code>A[1]</code> start pretty far off, but gradient descent quickly zeros in towards the X. As a side note, the line connecting each pair of dots is perpendicular to the line of the level set: see if you can figure out why.</p>

<p>So if that is what the contour plot looks like for the &quot;good&quot; case of \(D&#39;\), how does it look for \(D\)? Well, it looks rather strange:</p>

<p><img src="/books/pytorch/book/ch2-linreg/assets/contour1.png" alt="Contour plot of D"></p>

<p>Don&#39;t be fooled: like the previous plot the level sets also form ellipses, but they are so stretched that they are nearly straight lines at this scale. This means that vertically (<code>A[0]</code> is constant and we vary <code>A[1]</code>) there is substantial gradient, but horizontally (<code>A[1]</code> is constant and we vary <code>A[0]</code>) there is practically no slope. Put another way, gradient descent only knows to vary <code>A[1]</code>, and (almost) doesn&#39;t vary <code>A[0]</code>.  We can test this hypothesis by plotting how gradient descent updates <code>A[0]</code> and <code>A[1]</code>, like above. Since gradient descent makes such little progress though, we have to zoom in a lot to see what is going on:</p>

<p><img src="/books/pytorch/book/ch2-linreg/assets/contour1_dots.png" alt="Contour plot of D"></p>

<p>We can clearly see that gradient descent applies large updates to <code>A[1]</code> (a bit too large, a smaller learning rate would have been a bit better) due to the large gradient in the <code>A[1]</code> direction. But due to the (comparatively) tiny gradient in the <code>A[0]</code> direction very small updates are done to <code>A[0]</code>. Gradient descent quickly converges on the optimal value of <code>A[1]</code>, but is very very far away from finding the optimal value of <code>A[0]</code>.</p>

<p>Let&#39;s take a quick look at what is going on mathematically to see why this happens. The model we are using is:
\[
    y&#39;(x, A) = Ax = a_1 x_1 + a_2 x_2
\]
Here, \(a_1\) is in the role of <code>A[0]</code> and \(a_2\) is <code>A[1]</code>. We substitute this into the loss function to get:
\[
    L(a_1, a_2) = \sum_{i=1}^m (a_1 x_1^{(i)} + a_2 x_2^{(i)} - y^{(i)})^2
\]
Now if we differentiate \(L\) in the direction of \(a_1\) and \(a_2\) separately, we get:
\[
    \frac{\partial L}{\partial a_1} = \sum_{i=1}^m 2(a_1 x_1^{(i)} + a_2 x_2^{(i)} - y^{(i)})x_1^{(i)} \\
    \frac{\partial L}{\partial a_2} = \sum_{i=1}^m 2(a_1 x_1^{(i)} + a_2 x_2^{(i)} - y^{(i)})x_2^{(i)}
\]
Here we can see the problem: the inner terms of the derivatives are the same, except one is multiplied by \(x_1^{(i)}\) and the other by \(x_2^{(i)}\). If \(x_2\) is on average 100 times bigger than \(x_1\) (which it is in the original data set), then we would expect \(\frac{\partial L}{\partial a_2}\) to be roughly 100 times bigger than \(\frac{\partial L}{\partial a_1}\). It isn&#39;t exactly 100 times larger, but with any reasonable data set it should be close. Since the derivatives in the directions of \(a_1\) and \(a_2\) are scaled completely differently, gradient descent fails to update both of them adequately.</p>

<p>The solution is simple: we need to rescale the input features before training. This is exactly what happened when we mysteriously divided by 100: we rescaled \(x_2\) to be comparable to \(x_1\). But we should work out a more methodical way of rescaling, rather than randomly dividing by 100.</p>
<h2 id="rescaling-features">Rescaling Features</h2>
<p>This wasn&#39;t explored above, but there are really two ways that we potentially need to rescale features. Consider an example where \(x_1\) ranges between -1 and 1, but \(x_2\) ranges between 99 and 101: both of these features have (at least approximately) the same <a href="https://en.wikipedia.org/wiki/Standard_deviation">standard deviation</a>, but \(x_2\) has a much larger <a href="https://en.wikipedia.org/wiki/Expected_value">mean</a>. On the other hand, consider an example where \(x_1\) still ranges between -1 and 1, but \(x_2\) ranges between -100 and 100. This time, they have the same mean, but \(x_2\) has a much larger standard deviation. Both of these situations can make gradient descent and related algorithms slower and less reliable. So, our goal is to ensure that all features have the same mean and standard deviation.</p>

<blockquote>
<p><strong>Note:</strong> There are other methods to measure how different features are, and then subsequently rescale them. For a look at more of them, feel free to consult the <a href="https://en.wikipedia.org/wiki/Feature_scaling">Wikipedia article</a>. However, after learning the technique presented here, any other technique is fairly similar and easy to implement if needed.</p>
</blockquote>

<p>Without digressing too far into statistics, let&#39;s quickly review how to calculate the mean \(\mu\) and standard deviation \(\sigma\). Suppose we have already read in our data set in Python into a Numpy vector / matrix, and have all the values for the feature \(x_j\), for each \(j\). Mathematically, the mean for feature \(x_j\) is just the average:
\[
    \mu_j = \frac{\sum_{i=1}^m x_j^{(i)}}{m}
\]
In PyTorch there is a convenient <code>.mean()</code> method we will use.</p>

<p>The standard deviation \(\sigma\) is a bit more tricky. We measure how far each point \(x_j^{(i)}\) is from the mean \(\mu_j\), square this, then take the mean of all of this, and finally square root it:
\[
    \sigma_j = \sqrt{\frac{\sum_{i=1}^m (x_j^{(i)} - \mu_j)^2 }{m}}
\]
Again, PyTorch provides a convenient <code>.std()</code> method.</p>

<blockquote>
<p><strong>Note:</strong> Statistics nerds might point out that in the above equation we should divide by \(m - 1\) instead of \(m\) to obtain an unbiased estimate of the standard deviation. This might well be true, but in this usage it does not matter since we are only using this to rescale features relative to each other, and not make a rigorous statistical inference. To be more precise, doing so would involve multiplying each scaled feature by only a constant factor, and will not change any of their standard deviations or means relative to each other.</p>
</blockquote>

<p>Once we have every mean \(\mu_j\) and standard deviation \(\sigma_j\), rescaling is easy: we simply rescale every feature like so:</p>

<p>\[
\begin{equation} \label{eq:scaling}
    x_j&#39; = \frac{x_j - \mu_j}{\sigma_j}
\end{equation}
\]</p>

<p>This will force every feature to have a mean of 0 and a standard deviation of 1, and thus be scaled well relative to each other.</p>

<blockquote>
<p><strong>Note:</strong> Be careful to make sure to perform the rescaling at both training time and prediction time. That is, we first have to perform the rescaling on the whole training data set, and then train the model so as to achieve good training performance. Once the model is trained and we have a new, never before seen input \(x\), we also need to rescale its features to \(x&#39;\) because the trained model only understands inputs that have already been rescaled (because we trained it that way).</p>
</blockquote>
<h2 id="implementation-and-experiments">Implementation and Experiments</h2>
<p>Feature scaling can be applied to just about any multi-variable model. Since the only multi-variable model we have seen so far is multi-variable linear regression, we&#39;ll look at implementing feature scaling in that context, but the ideas and the code are pretty much the same for other models. First let&#39;s just copy-paste the original multi-variable linear regression code, and modify it to load the <a href="/books/pytorch/book/ch2-linreg/code/linreg-scaling-synthetic.csv">new synthetic data set</a>:</p>
<div class="highlight"><pre class="highlight python"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>

<span class="c1">### Load the data
</span>
<span class="c1"># First we load the entire CSV file into an m x 3
</span><span class="n">D</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"linreg-scaling-synthetic.csv"</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="nb">float</span><span class="p">)</span>

<span class="c1"># We extract all rows and the first 2 columns, and then transpose it
</span><span class="n">x_dataset</span> <span class="o">=</span> <span class="n">D</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>

<span class="c1"># We extract all rows and the last column, and transpose it
</span><span class="n">y_dataset</span> <span class="o">=</span> <span class="n">D</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>

<span class="c1"># And make a convenient variable to remember the number of input columns
</span><span class="n">n</span> <span class="o">=</span> <span class="mi">2</span>


<span class="c1">### Model definition ###
</span>
<span class="c1"># First we define the trainable parameters A and b 
</span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Then we define the prediction model
</span><span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">x_input</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">A</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">x_input</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>


<span class="c1">### Loss function definition ###
</span>
<span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">y_predicted</span><span class="p">,</span> <span class="n">y_target</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">((</span><span class="n">y_predicted</span> <span class="o">-</span> <span class="n">y_target</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>

<span class="c1">### Training the model ###
</span>
<span class="c1"># Setup the optimizer object, so it optimizes a and b.
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># Main optimization loop
</span><span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2000</span><span class="p">):</span>
    <span class="c1"># Set the gradients to 0.
</span>    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="c1"># Compute the current predicted y's from x_dataset
</span>    <span class="n">y_predicted</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_dataset</span><span class="p">)</span>
    <span class="c1"># See how far off the prediction is
</span>    <span class="n">current_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">y_predicted</span><span class="p">,</span> <span class="n">y_dataset</span><span class="p">)</span>
    <span class="c1"># Compute the gradient of the loss with respect to A and b.
</span>    <span class="n">current_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="c1"># Update A and b accordingly.
</span>    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">"t = {t}, loss = {current_loss}, A = {A.detach().numpy()}, b = {b.item()}"</span><span class="p">)</span>
</code></pre></div>
<p>If you run this code right now, you might see output like the following:</p>
<div class="highlight"><pre class="highlight plaintext"><code>...
t = 1995, loss = 0.505296, A = [[1.9920335  0.01292674]], b = 0.089939
t = 1996, loss = 0.5042, A = [[1.9920422  0.01292683]], b = 0.0898404
t = 1997, loss = 0.5031, A = [[1.9920509  0.01292691]], b = 0.0897419
t = 1998, loss = 0.501984, A = [[1.9920596  0.01292699]], b = 0.0896435
t = 1999, loss = 0.50089, A = [[1.9920683  0.01292707]], b = 0.0895451
</code></pre></div>
<p>Note that if you run this code multiple times you will get different results each time due to the random initialization. The synthetic data was generated with the equation \(y = 2x_1 + 0.013x_2 + 0\). So after 2000 iterations of training we are getting close-ish to the correct values, but it&#39;s not fully trained. So let&#39;s implement feature scaling to fix this.</p>

<p>The first step is to compute the mean and standard deviation for each feature in the training data set. Add the following after <code>x_dataset</code> is loaded:</p>
<div class="highlight"><pre class="highlight python"><code><span class="n">means</span> <span class="o">=</span> <span class="n">x_dataset</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">deviations</span> <span class="o">=</span> <span class="n">x_dataset</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div>
<p>The argument of <code>1</code> means that we compute the mean for each feature, averaged over all data points. That is, <code>x_dataset</code> is a \(2 \times 400\) matrix, and <code>means</code> is a \(2 \times 1\) matrix (vector). If we had used <code>0</code>, <code>means</code> would be a \(1 \times 400\) matrix, which is not what we want. We also have to use <code>keepdims=True</code> to tell PyTorch to keep means as an actual \(2 \times 1\) matrix rather than squishing it into an array of length 2.</p>

<p>Now that we know the means and standard deviations of each feature, we have to use them to transform the inputs via Equation \((\ref{eq:scaling})\). We have two options: we could implement the transformation directly by transforming <code>x_dataset</code> before training, or we could include the transformation within the PyTorch model computations. But since we need to do the transformation again when we want to predict output given new inputs, including it in the PyTorch computations will be a bit more convenient.</p>

<p>When we define the model, we want to first transform the input according to Equation \((\ref{eq:scaling})\):</p>
<div class="highlight"><pre class="highlight python"><code><span class="c1"># First we define the trainable parameters A and b 
</span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Then we define the prediction model
</span><span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">x_input</span><span class="p">):</span>
    <span class="n">x_transformed</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_input</span> <span class="o">-</span> <span class="n">means</span><span class="p">)</span> <span class="o">/</span> <span class="n">deviations</span>
    <span class="k">return</span> <span class="n">A</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">x_transformed</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
</code></pre></div>
<p>One important concept is that when this line of code runs, <code>means</code> and <code>deviations</code> have already been computed, so to PyTorch they are <strong>constants</strong>. This means that PyTorch will not compute gradients for them.</p>

<p>Also, note that since <code>x_input</code> and <code>means</code> are compatibly sized matrices, the subtraction (and division) will be done separately for each feature, automatically. So while Equation \((\ref{eq:scaling})\) technically says how to scale one feature individually, this single line of code scales all the features.</p>

<p>Note that when we want to use the model, either for training or later prediction, we should feed unscaled values to the model, since the model already performs the scaling.</p>

<p>That&#39;s all that we need to implement for feature scaling, it&#39;s really pretty easy. If we run this code now we should see:</p>
<div class="highlight"><pre class="highlight plaintext"><code>...
t = 1995, loss = 1.17733e-07, A = [[6.0697703 3.9453504]], b = 16.5
t = 1996, loss = 1.17733e-07, A = [[6.0697703 3.9453504]], b = 16.5
t = 1997, loss = 1.17733e-07, A = [[6.0697703 3.9453504]], b = 16.5
t = 1998, loss = 1.17733e-07, A = [[6.0697703 3.9453504]], b = 16.5
t = 1999, loss = 1.17733e-07, A = [[6.0697703 3.9453504]], b = 16.5
</code></pre></div>
<p>The fact that none of the trained weights are updating anymore and that the loss function is very small is a good sign that we have achieved convergence. But the weights are not the values we expect... shouldn&#39;t we get <code>A = [[2, 0.013]], b = 0</code>, since that is the equation that generated the synthetic data? Actually, the weights we got <em>are</em> correct because the model is now being trained on the rescaled data set, so we get different weights at the end. See Challenge Problem 3 below to explore this more.</p>
<h1 id="concluding-remarks-7">Concluding Remarks</h1>
<p>We&#39;ve seen how to implement feature scaling for a simple multi-variable linear regression model.  While this is a fairly simple model, the principles are basically the same for applying feature scaling to more complex models (which we will see very soon). In fact, the code is pretty much identical: just compute the means and standard deviations, apply the formula to <code>x_input</code> to compute <code>x_transformed</code>, and anywhere you use <code>x_input</code> in the model, just use <code>x_transformed</code> instead.</p>

<p>Whether or not feature scaling helps is dependent on the problem and the model. If you have having trouble getting your model to converge, you can always implement feature scaling and see how it affects the training. While it wasn&#39;t used in this chapter, using TensorBoard (see <a href="/books/pytorch/book/ch2-linreg/2017-12-27-optimization.html">chapter 2.2</a>) is a great way to run experiments to compare training with and without feature scaling.</p>
<h1 id="challenge-problems-6">Challenge Problems</h1>
<!-- <ol>
    <li>**Coding:** Take one of the challenge problems from [the previous chapter](/books/tensorflow/book/ch2-linreg/2018-03-21-multi-variable.html#challenge-problems), and implement it with and without feature scaling, and then compare how training performs.</li>
</ol> -->

<ol>
<li><strong>Coding:</strong> Take one of the challenge problems from <a href="/books/pytorch/book/ch2-linreg/2018-03-21-multi-variable.html#challenge-problems">the previous chapter</a>, and implement it with and without feature scaling, and then compare how training performs.</li>
<li><strong>Coding:</strong> Add TensorBoard support to the code from this chapter, and use it to explore for yourself the effect feature scaling has. Also, you can compare with different optimization algorithms.</li>
<li><strong>Theory:</strong> One problem with feature scaling is that we learn different model parameters. In this chapter, the original data set was generated with \(y = 2x_1 + 0.013x_2 + 0\), but the parameters that the model learned were \(a_1&#39; = 6.0697703, a_2&#39; = 3.9453504, b&#39; = 16.5\). Note that I have called these learned parameters \(a_1&#39;\) rather than \(a_1\) to indicate that these learned parameters are for the <em>rescaled</em> data.<br/><br />One important use of linear regression is to explore and understand a data set, not just predict outputs. After doing a regression, one can understand through the learned weights how severely each feature affects the output. For example, if we have two features \(A\) and \(B\) which are used to predict rates of cancer, and the learned weight for \(A\) is much larger than the learned weight for \(B\), this indicates that occurrence of \(A\) is more correlated with cancer than occurrence of \(B\) is, which is interesting in its own right. Unfortunately, performing feature scaling destroys this: since we have rescaled the training data, the weight for \(A\) (\(a_1&#39;\)) has lost its relevance to the values of \(A\) in the real world. But all is not lost, since we can actually recover the &quot;not-rescaled&quot; parameters from the learned parameters, which we will derive now.<br/><br/>First, suppose that we have learned the &quot;rescaled&quot; parameters \(a_1&#39;, a_2&#39;, b&#39;\). That is, we predict the output \(y\) to be: \[\begin{equation}\label{eq:start}y = a_1&#39; x_1&#39; + a_2&#39; x_2&#39; + b&#39;\end{equation}\]where \(x_1&#39;, x_2&#39;\) are the rescaled features. Now, we <em>want</em> a model that uses some (yet unknown) weights \(a_1, a_2, b\) to predict the same output \(y\), but using the not-rescaled features \(x_1, x_2\). That is, we want to find \(a_1, a_2, b\) such that this is true:\[\begin{equation}\label{eq:goal}y = a_1 x_1 + a_2 x_2 + b\end{equation}\]To go about doing this, substitute for \(x_1&#39;\) and \(x_2&#39;\) using Equation \((\ref{eq:scaling})\) into Equation \((\ref{eq:start})\), and then compare it to Equation \((\ref{eq:goal})\) to obtain 3 equations: one to relate each scaled and not-scaled parameter. Finally, you can use these equations to be able to compute the &quot;not-scaled&quot; parameters in terms of the learned parameters. You can check your work by computing the not-scaled parameters in terms of the learned parameters for the synthetic data set, and verify that they match the expected values.</li>
</ol>
<h1 id="complete-code-6">Complete Code</h1>
<p>The <a href="https://github.com/donald-pinckney/donald-pinckney.github.io/blob/src/books/pytorch/src/ch2-linreg/code/scaling/feature_scaling.py">complete example code is available on GitHub</a>, as well as directly here:</p>
<div class="highlight"><pre class="highlight python"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>

<span class="c1">### Load the data
</span>
<span class="c1"># First we load the entire CSV file into an m x 3
</span><span class="n">D</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"linreg-scaling-synthetic.csv"</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="nb">float</span><span class="p">)</span>

<span class="c1"># We extract all rows and the first 2 columns, and then transpose it
</span><span class="n">x_dataset</span> <span class="o">=</span> <span class="n">D</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>

<span class="c1"># We extract all rows and the last column, and transpose it
</span><span class="n">y_dataset</span> <span class="o">=</span> <span class="n">D</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>

<span class="c1"># And make a convenient variable to remember the number of input columns
</span><span class="n">n</span> <span class="o">=</span> <span class="mi">2</span>


<span class="c1">### Feature Scaling computations
</span>
<span class="c1"># Pre-compute the means and standard deviations of independent variables
</span><span class="n">means</span> <span class="o">=</span> <span class="n">x_dataset</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">deviations</span> <span class="o">=</span> <span class="n">x_dataset</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>


<span class="c1">### Model definition ###
</span>
<span class="c1"># First we define the trainable parameters A and b 
</span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Then we define the prediction model
</span><span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">x_input</span><span class="p">):</span>
    <span class="n">x_transformed</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_input</span> <span class="o">-</span> <span class="n">means</span><span class="p">)</span> <span class="o">/</span> <span class="n">deviations</span>
    <span class="k">return</span> <span class="n">A</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">x_transformed</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>


<span class="c1">### Loss function definition ###
</span>
<span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">y_predicted</span><span class="p">,</span> <span class="n">y_target</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">((</span><span class="n">y_predicted</span> <span class="o">-</span> <span class="n">y_target</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>

<span class="c1">### Training the model ###
</span>
<span class="c1"># Setup the optimizer object, so it optimizes a and b.
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># Main optimization loop
</span><span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2000</span><span class="p">):</span>
    <span class="c1"># Set the gradients to 0.
</span>    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="c1"># Compute the current predicted y's from x_dataset
</span>    <span class="n">y_predicted</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_dataset</span><span class="p">)</span>
    <span class="c1"># See how far off the prediction is
</span>    <span class="n">current_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">y_predicted</span><span class="p">,</span> <span class="n">y_dataset</span><span class="p">)</span>
    <span class="c1"># Compute the gradient of the loss with respect to A and b.
</span>    <span class="n">current_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="c1"># Update A and b accordingly.
</span>    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">"t = {t}, loss = {current_loss}, A = {A.detach().numpy()}, b = {b.item()}"</span><span class="p">)</span>
</code></pre></div>
</div>


<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/idris/2019/06/25/idris-intro.html" class="titular">
            An Interactive Introduction to Dependent Types with Idris
            <small>25 Jun 2019</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2019/06/03/wwdc2019.html" class="titular">
            Top questions and thoughts from WWDC 2019
            <small>03 Jun 2019</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/machine%20learning/2019/05/02/tda.html" class="titular">
            Topological Data Analysis and Persistent Homology
            <small>02 May 2019</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>

    </div>

    <script type="text/javascript">
      window.is_book_mode = false;
    </script>
    
  </body>
</html>
