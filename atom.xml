<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="https://www.w3.org/2005/Atom">

 <title>Donald Pinckney</title>
 <link href="https://donaldpinckney.com/atom.xml" rel="self"/>
 <link href="https://donaldpinckney.com/"/>
 <updated>2019-05-23T16:45:47-07:00</updated>
 <id>https://donaldpinckney.com</id>
 <author>
   <name>Donald Pinckney</name>
   <email>donald_pinckney@icloud.com</email>
 </author>

 
 <entry>
   <title>Topological Data Analysis and Persistent Homology</title>
   <link href="https://donaldpinckney.com/machine%20learning/2019/05/02/tda.html"/>
   <updated>2019-05-02T00:00:00-07:00</updated>
   <id>https://donaldpinckney.com/machine%20learning/2019/05/02/tda</id>
   <content type="html">&lt;script src=&quot;/public/post_assets/tda/d3.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;/public/post_assets/tda/tsne.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;/public/post_assets/tda/demo-configs.js&quot;&gt;&lt;/script&gt;

&lt;!-- &lt;script src=&quot;figure-configs.js&quot;&gt;&lt;/script&gt; --&gt;

&lt;script src=&quot;/public/post_assets/tda/visualize.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;/public/post_assets/tda/complexes.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;/public/post_assets/tda/figures.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;&lt;link href=&quot;/public/post_assets/tda/material-icons.css&quot; rel=&quot;stylesheet&quot;&gt;
&lt;link rel=&quot;stylesheet&quot; href=&quot;/public/post_assets/tda/playground.css&quot;&gt;
&lt;link rel=&quot;stylesheet&quot; href=&quot;/public/post_assets/tda/post.css&quot;&gt;&lt;/p&gt;

&lt;p&gt;The overall goal of Topological Data Analysis (TDA) is to be able to analyze topological features of data sets, often through computations of topological properties such as homology or via visualization. Here I will focus on the former technique, known as &lt;strong&gt;persistent homology&lt;/strong&gt;, but I will briefly touch on the visualization aspect. Before jumping into the mathematical aspects I&amp;#39;ll first give an overview of some motivations for TDA by both offering it as an alternative to typical statistical tools as well as showing some of the unique capabilities of TDA.&lt;/p&gt;

&lt;div&gt;
\(
   \def\R{\mathbb R}
   \def\N{\mathbb N}
   \def\X{\mathbb X}
   \def\homt{\simeq}
   \def\C{\check{C}}
   \def\e{\varepsilon}
   \def\P{\mathcal{P}}
   \require{AMScd}
   \require{extpfeil}
   \Newextarrow{\vxmapsto}{5,10}{0x21A7}
   \require{HTML}
   \DeclareMathOperator{\Ima}{Im}
\)
&lt;/div&gt;
&lt;h1 id=&quot;motivations-for-understanding-topological-properties-of-data&quot;&gt;Motivations for Understanding Topological Properties of Data&lt;/h1&gt;
&lt;p&gt;In many sciences and other fields involving data collection and analysis, one can often consider two types of data analysis: qualitative and quantitative. Given a large and potentially complex data set \(D\), the task of qualitative data analysis is to investigate if \(D\) contains information you are looking for, and this is often performed via some type of visualization. If you then feel that \(D\) does contain desired information, then you can often proceed to applying a more quantitative technique for extracting that information, guided by your intuition from the qualitative analysis. I claim that common statistical techniques are very useful for both tasks, but on more complex data sets currently emerging in today&amp;#39;s sciences can be inadequate, and that a topological approach may help. Let&amp;#39;s look at some specific examples of both qualitative and quantitative data analysis, and motivate why topological techniques might help.&lt;/p&gt;
&lt;h2 id=&quot;clustering-and-dimensionality-reduction&quot;&gt;Clustering and Dimensionality Reduction&lt;/h2&gt;
&lt;p&gt;Consider an experiment in which we have a population consisting of multiple bacteria species&lt;sup id=&quot;fnref1&quot;&gt;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;, and we would like to determine how many different species are present. To do so we can use a technique called &lt;a href=&quot;https://en.wikipedia.org/wiki/Raman_spectroscopy&quot;&gt;Raman spectroscopy&lt;/a&gt;, which in summary shines a laser into your sample and records a emission spectra of the sample. In this paper 4 species with 1000 bacteria each were tested with Raman spectroscopy (giving 4000 spectra) with each spectra consisting of 1500 intensity measurements for wavelengths between 500 and 3500 \(cm^{-1}\). Mathematically this gives us 4000 points living in \(\R^{1500}\). Now we want to see if the spectra can actually be used to differentiate the 4 bacteria species. Unfortunately, visualization of \(\R^{1500}\) directly is not helpful, as the following plot of overlapping spectra shows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/post_assets/tda/spectra.png&quot; alt=&quot;raman spectra&quot; class=&quot;center&quot;/&gt;&lt;/p&gt;

&lt;p&gt;A very standard technique for reducing this high dimensional data down to a smaller dimension is &lt;a href=&quot;https://en.wikipedia.org/wiki/Principal_component_analysis&quot;&gt;PCA&lt;/a&gt; (Principle Components Analysis) which finds the linear projection which maximizes the variance. Using PCA to project \(\R^{1500}\) to \(\R^2\) gives the following plot, with colors indicating the previously known species:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/post_assets/tda/raman_pca.png&quot; alt=&quot;raman pca&quot; class=&quot;center&quot;/&gt;&lt;/p&gt;

&lt;p&gt;PCA has provided some useful information, but not so much as we would like. This linear projection shows that some differentiation of species using the spectra is possible, but unfortunately PCA can not be used to differentiate the blue and light blue samples as they strongly overlap. Any typical algorithm used to identify clusters (such as &lt;a href=&quot;https://en.wikipedia.org/wiki/K-means_clustering#Standard_algorithm&quot;&gt;k-means clustering&lt;/a&gt;) would fail to isolate 4 clusters. The original data in \(\R^{1500}\) probably contains the information necessary to differentiate all 4 species, but we can&amp;#39;t visualize it well directly. On the other hand linearly projecting to a low dimension destroys the useful information we want. Projecting to a higher dimensional space like \(\R^3\) or using non-linear dimensionality reductions such as t-SNE &lt;sup id=&quot;fnref2&quot;&gt;&lt;a href=&quot;#fn2&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; or diffusion maps &lt;sup id=&quot;fnref3&quot;&gt;&lt;a href=&quot;#fn3&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; might or might not work better, but at the end of the day can run into the same limitations. Using topology we can take a different approach: &lt;em&gt;imagine we can regard the data as a manifold \(X \subseteq \R^{1500}\)&lt;/em&gt;; then detecting the number of different species (clusters) corresponds to determining the number of connected components of \(X\). Using TDA techniques we can determine the number of connected components in the original data set, &lt;em&gt;without having to project to lower-dimensions&lt;/em&gt;. This type of clustering analysis is applicable to a huge number of problems, not just in the biological domain.&lt;/p&gt;
&lt;h2 id=&quot;understanding-higher-dimensional-topological-features-of-primary-visual-cortex-of-monkeys&quot;&gt;Understanding Higher-Dimensional Topological Features of Primary Visual Cortex of Monkeys&lt;/h2&gt;
&lt;p&gt;However, TDA can be used for more than just inspecting connected components. One particularly interesting application of TDA to neuroscience is discussed in section 2.5 of Carlsson &lt;sup id=&quot;fnref4&quot;&gt;&lt;a href=&quot;#fn4&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;. In an experiment a 10x10 array of electrodes were implanted in the Primary Visual Cortex of Macaque monkeys, while the monkeys viewed either a blank screen or some movie clips. A bunch of signal processing techniques were applied to the voltage sequences, eventually resulting in a collection of data sets, each consisting of 200 data points lying in \(\R^5\). Each such data set \(D_i \subseteq \R^5\) corresponds to the monkey looking at either a blank screen or movie clips, for a 10 second window. Each point \(p \in D_i \subseteq \R^5\) are the 5 voltages during a 50ms window of the top 5 activated neurons in the 10 second window associated to \(D_i\). So we have many such \(D_i \subseteq \R^5\), of which some correspond to watching blank screens and others correspond to watching movie clips.&lt;/p&gt;

&lt;p&gt;The task now is to differentiate the \(D_i\)&amp;#39;s based on blank screen vs. movie clips. Given some \(D_i\), &lt;em&gt;if we can view \(D_i\) as a topological space&lt;/em&gt;, then we can apply the algebraic topology tool of homology to compute the Betti numbers of \(D_i\). In this study the first 3 Betti numbers \((\beta_0, \beta_1, \beta_2)\) were computed for each \(D_i\). By a vast margin the most common Betti numbers were \(a = (1, 1, 0)\) and \(b = (1, 0, 1)\), corresponding to the topology of a circle and a sphere. Finally Betti numbers were computed for data sets generated by random firings according a Poisson model (i.e. the null hypothesis). The distribution of Betti numbers is easily able to distinguish all three modes (blank screen, movie clips, random Poisson model) from each other. In addition since \(a\) and \(b\) were the most common Betti numbers for both blank screen and movie clip modes, this suggests that the primary visual cortex seems to naturally operate using the topology of circles and spheres, but the reason for this topological phenomenon is not yet known.&lt;/p&gt;
&lt;h1 id=&quot;point-clouds-and-simplicial-complexes&quot;&gt;Point Clouds and Simplicial Complexes&lt;/h1&gt;
&lt;p&gt;In both of the examples above and in nearly all data analysis settings one has a discrete finite data set \(D\). Often \(D \subseteq \R^n\), but more generally we consider \(D\) to be some discrete finite metric space (for example in comparing DNA sequences one can define a distance metric between sequences which is non-Euclidean). \(D\) is often called a &lt;strong&gt;point cloud&lt;/strong&gt;. One way to look at this is that the point cloud \(D\) is drawn from a probability distribution with support some metric space \(X\). For example in the bacteria species classification experiment \(X\) would be the space of possible emission spectra for the 4 different species, and we would expect \(X\) to have 4 connected components. Thus the goal of TDA is to infer the topological properties of \(X\) given a point cloud \(D\) sampled from \(X\).&lt;/p&gt;

&lt;p&gt;Since \(D\) is a discrete space, it is useless topologically as is. The main strategy of TDA is to build a simplicial complex from \(D\) such that the topological properties of the simplicial complex are similar to those of \(X\). Of course this isn&amp;#39;t always possible as it is contingent upon a sufficient number of samples.&lt;/p&gt;
&lt;h2 id=&quot;nerves-of-open-covers-and-the-268ech-complex&quot;&gt;Nerves of Open Covers and the &amp;#268;ech Complex&lt;/h2&gt;
&lt;p&gt;The first and most natural technique for building a simplicial complex based on some topological space \(X\) is based on an open cover \(\mathcal{U}\) of \(X\). If \(\mathcal{U} = \{U_i\}, i \in I\) is an open cover of \(X\) then the simplicial complex called the &lt;strong&gt;nerve of \(\mathcal{U}\)&lt;/strong&gt;, \(N(\mathcal{U})\), is defined to have vertex set \(I\), with a \(k-1\) simplex \(\{i_1, \cdots, i_k\} \in N(\mathcal{U})\) if and only if:
\[
   U_{i_1} \cap U_{i_2} \cap \cdots \cap U_{i_k} \neq \emptyset
\]&lt;/p&gt;

&lt;p&gt;The following picture from Wikipedia illustrates this well:&lt;/p&gt;

&lt;p&gt;&lt;a title=&quot;ProboscideaRubber15 [CC BY-SA 4.0 (https://creativecommons.org/licenses/by-sa/4.0)], via Wikimedia Commons&quot; href=&quot;https://commons.wikimedia.org/wiki/File:Constructing_nerve.png&quot; class=&quot;center&quot;&gt;&lt;img width=&quot;512&quot; alt=&quot;Constructing nerve&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/f/fc/Constructing_nerve.png/512px-Constructing_nerve.png&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Note that if at most \(n\) open sets intersect simultaneously then \(N(\mathcal{U})\) has dimension \(n-1\). A basic result of nerves is the Nerve Lemma:
&lt;div class=&quot;lemma&quot; text=&quot;Nerve Lemma&quot;&gt;
   Let (X) be a topological space, and let (\mathcal{U}) be an open cover of (X). If every non-empty intersection of open sets in (\mathcal{U}) is contractible, then (N(\mathcal{U}) \homt X).
&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;We can now define the &lt;strong&gt;&amp;#268;ech Complex&lt;/strong&gt; of a finite point cloud \(D \subseteq X\). Let \(\varepsilon &amp;gt; 0\), and define the open cover \(\mathcal{U} = \{ B_\varepsilon (x) \mid x \in D \} \).  Then the &amp;#268;ech Complex of radius \(\varepsilon\) is defined as \(\check{C}_\varepsilon(D) = N(\mathcal{U})\).&lt;/p&gt;
&lt;h2 id=&quot;homotopy-type-of-the-268ech-complex&quot;&gt;Homotopy Type of the &amp;#268;ech Complex&lt;/h2&gt;
&lt;p&gt;To better understand the topological properties of the &amp;#268;ech Complex we can apply the Nerve Lemma. By the Nerve Lemma, \(\check{C}_\varepsilon(D) \homt \bigcup_{x \in D} B_\varepsilon(x)\). However, what we would like is to relate the topology of \(\C_\e(D)\) to the topology of \(X\), where \(D\) has been sampled from \(X\). The following theorem&lt;sup id=&quot;fnref4&quot;&gt;&lt;a href=&quot;#fn4&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; is a partial result in this direction:
&lt;div class=&quot;theorem&quot;&gt;
   Let (X) be a Riemannian manifold. Then there exists an (e) such that for all (\e \leq e), there is a finite point cloud (D \subseteq X) such that (\C_\e(D) \homt X).
&lt;/div&gt;
This theorem is a nice theoretic result, since it hints that &amp;#268;ech Complexes of point clouds are morally the correct objects to study as they do have a topological relationship with the underlying topological space. However in terms of practicalities this theorem buys us little. The problems are the following:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Since we only have access to \(D\) and not \(X\), how can we know that \(X\) is a Riemannian manifold?&lt;/li&gt;
&lt;li&gt;The existence of such an \(e\) is good, but the theorem offers no way to find or search for \(e\).&lt;/li&gt;
&lt;li&gt;The largest issue is that we only have one given point cloud \(D\), so the existence of another \(D&amp;#39;\) guaranteeing homotopy equivalence is not useful.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There have been further results that partially address some of these issues, of which Chazal&lt;sup id=&quot;fnref5&quot;&gt;&lt;a href=&quot;#fn5&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; surveys a few. An additional problem with &amp;#268;ech Complexes is that computationally they are quite slow to compute, which leads us to consider alternate simplicial complex constructions such as the &lt;a href=&quot;https://en.wikipedia.org/wiki/Vietoris%E2%80%93Rips_complex&quot;&gt;Vietoris-Rips Complex&lt;/a&gt; and many others which are surveyed by Carlsson&lt;sup id=&quot;fnref4&quot;&gt;&lt;a href=&quot;#fn4&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;. However, the simplicial complex constructions are all similar in that they all attempt to reconstruct the homotopy type of \(X\) and can do so under strict assumptions, but in practical data analysis situations will almost always fail to be homotopy equivalent.&lt;/p&gt;
&lt;h2 id=&quot;exploring-some-268ech-complexes&quot;&gt;Exploring Some &amp;#268;ech Complexes&lt;/h2&gt;
&lt;p&gt;To get a better intuition for why &amp;#268;ech Complexes of point clouds \(D\) sampled from a topological space \(X\) fail to be homotopy equivalent to \(X\), we can look at visualizations of &amp;#268;ech Complexes on sample data sets. Since &amp;#268;ech Complexes can be quite high dimensional, we only visualize the 2-skeleton of the &amp;#268;ech Complex. Below you can interact with a variety of sample data sets, and vary the parameter \(\varepsilon\) to see how it affects \(\C_\e(D)^{(2)}\).&lt;/p&gt;

&lt;div id=&quot;playground&quot;&gt;&lt;div id=&quot;playground-canvas&quot;&gt;&lt;/div&gt;&lt;div id=&quot;data-menu&quot;&gt;&lt;/div&gt;&lt;div id=&quot;data-details&quot;&gt;&lt;div id=&quot;data-controls&quot;&gt;&lt;div id=&quot;data-options&quot;&gt;&lt;/div&gt;&lt;div id=&quot;tsne-options&quot;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div id=&quot;data-description&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;

&lt;script src=&quot;/public/post_assets/tda/playground.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;The above code was based on Wattenberg, et al.&lt;sup id=&quot;fnref6&quot;&gt;&lt;a href=&quot;#fn6&quot; rel=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;. Play around with the &amp;#268;ech Complex visualization as long as you want. But once you&amp;#39;re done, here are some of the key takeaways from it.&lt;/p&gt;

&lt;p&gt;First, due to random sampling noise and non-homogenous density it can be tricky to chose a value of \(\e\) such that \(\C_\e(D) \homt X\). For example with the annulus you have to pick some \(\e\) that is large enough to fill in some holes from noise, but if you pick an \(\e\) that is way too big then it will fill in the hole of the annulus itself. For these toy data sets we can have an intuition for what choice of \(\e\) &amp;quot;looks right&amp;quot;, but for high dimensional data it is hard to have an understanding of a &amp;quot;good&amp;quot; choice of \(\e\).&lt;/p&gt;

&lt;p&gt;Second, there can be multiple &amp;quot;good&amp;quot; choices of \(\e\). For example, consider the 4th data set above with 2 circles of vastly different sizes. For \(\e \approx 0.07\) The tiny circle obtains the correct topology, but the big circle is still disconnected. But if you try to increase \(\e\) so as to make the big circle connected, the tiny circle actually becomes simply connected. Both the big and tiny circles have the homotopy type of a circle, but the homotopy type of each is only visible at distinct values of \(\e\). In a sense \(\e\) is like the amount of zoom in a microscope: you can make \(\e\) very small to zoom in closely and inspect the topology of the tiny circle, or you can make \(\e\) large to zoom out and observe the topology of the big circle, at which point the tiny circle is negligible. &lt;/p&gt;

&lt;p&gt;The second point really provides the key insight of TDA: &lt;em&gt;There is no one best value of \(\e\), but rather the spectrum of topologies as \(\e\) varies provides a comprehensive view of the topology of the point cloud.&lt;/em&gt; This will also address the first point, as the holes due to noisy sampling only last for a short range of \(\e\) values. Thus, in TDA we study families of simplicial complexes such as \(\P = \{\C_\e(D) \mid \e \in T \subseteq \R \}\). The crucial property of these &amp;#268;ech Complex families is that for \(\e \leq \e&amp;#39;\) we have that \(\C_\e(D)\) is a subcomplex of \(\C_{\e&amp;#39;}(D)\) (this is a trivial result of the definition of a &amp;#268;ech Complex). Also note that since point clouds are finite, only finitely many \(\e\) will produce distinct &amp;#268;ech Complexes, so we can consider \(T\) to be finite. Other simplicial complex constructions such as Vietoris-Rips complexes also share these properties.&lt;/p&gt;
&lt;h1 id=&quot;persistence&quot;&gt;Persistence&lt;/h1&gt;&lt;h2 id=&quot;persistence-objects&quot;&gt;Persistence Objects&lt;/h2&gt;
&lt;p&gt;We briefly set aside the specific construction of &amp;#268;ech Complexes and work from an algebraic perspective. Let \((T, \leq)\) be any partially ordered set and let \(\underline{C}\) be any category. We say that a &lt;strong&gt;\(T\)-persistence object&lt;/strong&gt; is a family \( \{c_t\}_{t \in T} \subseteq \underline{C} \) together with a morphism \(\phi^{t,u} : c_t \to c_u \) whenever \(t \leq u\) such that if \(t \leq u \leq v\) then \(\phi^{u,v} \circ \phi^{t,u} = \phi^{t,v}\). Alternatively a \(T\)-persistence object can be viewed as a functor \(\Phi : T \to \underline{C}\). Diagrammatically this looks something like:&lt;/p&gt;

&lt;div&gt;
\[
\begin{array}{ccc}
t &amp; \xmapsto{\Phi} &amp; c_t \\
\style{display: inline-block; transform: rotate(90deg)}{\leq} &amp; &amp; \big\downarrow \; \phi^{t,u} \\
u &amp; \xmapsto{\Phi} &amp; c_u \\
\style{display: inline-block; transform: rotate(90deg)}{\leq} &amp; &amp; \big\downarrow \; \phi^{u,v} \\
v &amp; \xmapsto{\Phi} &amp; c_v 
\end{array}
\]
&lt;/div&gt;

&lt;p&gt;Suppose we have a \(T\)-persistence object \( \{c_t, \phi^{t,u} \} \) and some partial order preserving map \(f : T&amp;#39; \to T\) for some other partially ordered set \(T&amp;#39;\). Then we have an induced \(T&amp;#39;\)-persistence object \( \{c_{f(t&amp;#39;)} \}_{t&amp;#39; \in T&amp;#39;} \) with morphisms \(\psi^{t&amp;#39;,u&amp;#39;} : c_{f(t&amp;#39;)} \to c_{f(u&amp;#39;)}\) given by \(\psi^{t&amp;#39;,u&amp;#39;} = \phi^{f(t&amp;#39;),f(u&amp;#39;)}\). It sounds more complicated than it is, so hopefully a diagram clarifies:&lt;/p&gt;

&lt;div&gt;
\[
\begin{array}{ccccc}
t' &amp; \xmapsto{f} &amp; t &amp; \xmapsto{\Phi} &amp; c_t \\
\style{display: inline-block; transform: rotate(90deg)}{\leq} &amp; &amp; \style{display: inline-block; transform: rotate(90deg)}{\leq} &amp; &amp; \big\downarrow \; \phi^{t,u} \\
u' &amp; \xmapsto{f} &amp; u &amp; \xmapsto{\Phi} &amp; c_u \\
\style{display: inline-block; transform: rotate(90deg)}{\leq} &amp; &amp; \style{display: inline-block; transform: rotate(90deg)}{\leq} &amp; &amp; \big\downarrow \; \phi^{u,v} \\
v' &amp; \xmapsto{f} &amp; v &amp; \xmapsto{\Phi} &amp; c_v 
\end{array}
\]
&lt;/div&gt;
&lt;h2 id=&quot;ech-complex-families-as-persistence-objects&quot;&gt;&amp;#268;ech Complex families as Persistence Objects&lt;/h2&gt;
&lt;p&gt;We can easily see that &amp;#268;ech Complex families are \(\R\)-persistence simplicial complexes since for all \(\e \leq \e&amp;#39;\) we have the inclusion map \(i^{\e,\e&amp;#39;}: \C_\e(D) \hookrightarrow \C_{\e&amp;#39;}(D)\) as discussed above (the superscript notation denotes &amp;#268;ech Complex radius, and is unrelated to cohomology). In addition since there are only finitely many values of \(\e\), say \(E = \{\e_1, \cdots, \e_n\} \subseteq \R\), which produce distinct &amp;#268;ech Complexes, we can easily construct an order preserving map \(f: \N \to E\), and thus we have that a &amp;#268;ech Complex family is an \(\N\)-persistence simplicial complex. For now we will work with \(\R\)-persistence, but will return to using \(\N\)-persistence later.&lt;/p&gt;

&lt;p&gt;The inclusion maps \(i^{\e,\e&amp;#39;}: \C_\e(D) \hookrightarrow \C_{\e&amp;#39;}(D)\) for an \(\R\)-persistence simplicial complex then induce chain maps for the simplicial chain complex \( i_k^{\e,\e&amp;#39;}: C_k(\C_\e(D); G) \rightarrow C_k(\C_{\e&amp;#39;}(D); G) \), and likewise for the simplicial homology \( i_k^{\e,\e&amp;#39;}: H_k(\C_\e(D); G) \rightarrow H_k(\C_{\e&amp;#39;}(D); G) \) for any coefficient group \(G\).  We can see this in a diagram, for \(\e \leq \e+p\):&lt;/p&gt;

&lt;!-- Thus, we have that the homology groups \\(H_\*(\C_\e(D); R)\\) are an \\(\R\\)-persistence \\(R\\)-module. --&gt;

&lt;div&gt;
\[
\begin{array}{ccccc}
                   &amp; \vdots \;\;\;\;\;\;  &amp;                          &amp; \vdots \;\;\;\;\;\;      &amp; \\
                   &amp; \big\downarrow \;\;\;\;\;\;   &amp;                          &amp; \big\downarrow \;\;\;\;\;\;      &amp; \\
\cdots \rightarrow &amp; H_k(\C_\e(D); G) &amp; \xrightarrow{\partial_*} &amp; H_{k-1}(\C_\e(D); G) &amp; \rightarrow \cdots \\
                   &amp; \big\downarrow \; i_k^{\e,\e+p}   &amp;                          &amp; \big\downarrow \; i_{k-1}^{\e,\e+p}       &amp; \\
\cdots \rightarrow &amp; H_k(\C_{\e+p}(D); G) &amp; \xrightarrow{\partial_*} &amp; H_{k-1}(\C_{\e+p}(D); G) &amp; \rightarrow \cdots \\
                   &amp; \big\downarrow \;\;\;\;\;\;   &amp;                          &amp; \big\downarrow \;\;\;\;\;\;      &amp; \\
                   &amp; \vdots \;\;\;\;\;\;  &amp;                          &amp; \vdots \;\;\;\;\;\;      &amp; 
\end{array}
\]
&lt;/div&gt;

&lt;p&gt;To better understand what these inclusion induced maps are doing, we can look at a specific example from the visualization above. Consider two choices of \(\e\) for the annulus:&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align: center&quot;&gt;\(\C_\e(D)\)&lt;/th&gt;
&lt;th style=&quot;text-align: center&quot;&gt;\(\C_{\e+p}(D)\)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/public/post_assets/tda/annulus1.png&quot; alt=&quot;annulus1&quot;&gt;&lt;/td&gt;
&lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/public/post_assets/tda/annulus2.png&quot; alt=&quot;annulus2&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;p&gt;In the left &amp;#268;ech Complex there are 3 generators for \(H_1\), shown in red, purple and orange. If we slightly increase \(\e\) to \(\e+p\) in the right &amp;#268;ech Complex we kill the purple and orange generators, but a new green generator is born. But if we look at the image of the homomorphism induced by the inclusion we see only that purple and orange are killed, leaving only the red generator in the image. In this case the inclusion induced homomorphism for \(\e\) to \(\e+p\) killed the topological features introduced by sampling noise, but whether or not this happens depends on the values of \(\e\) and \(p\). We define a new object called the &lt;strong&gt;\(p\)-persistent homology&lt;/strong&gt; in order to capture the elements of \(H_k\) that survive an increase of the &amp;#268;ech Complex radius by \(p\). Formally, we define the level-\(k\) \(p\)-persistent homology group starting at radius \(\e\) to be:
\[
   H_k^{\e,\e+p}(\C_\e(D); G) := \Ima (\;i^{\e,\e+p} : H_k(\C_\e(D); G) \to H_k(\C_{\e+p}(D); G)\;)
\]
This is precisely the elements of the level-\(k\) homology of the &amp;#268;ech Complex with radius \(\e\) that are not killed by increasing the radius by \(p\). Note that this generalizes normal homology, as \(H_k^{\e,\e+0}(\C_\e(D); G) = H_k(\C_\e(D); G)\).&lt;/p&gt;
&lt;h2 id=&quot;graded-modules-correspondence&quot;&gt;Graded Modules Correspondence&lt;/h2&gt;
&lt;p&gt;The \(p\)-persistent homology groups are what we would like to compute, but unfortunately the definition above does not offer a scheme for carrying out the computation. To remedy this we will develop a structure theorem for persistence. Let \(\mathcal{M}\) be an \(\N\)-persistence module over \(R\). Concretely we have a family of \(R\)-modules \(\{M^i\}_{i \in \N}\) with linear maps \(\phi^i : M^i \to M^{i+1}\) (this is sufficient to generate maps from \(M^i\) to \(M^j\) for \(i \leq j\)). Let \(R[t]\) with degree of \(t\) being 1 be the standard graded polynomial ring. We map \(\mathcal{M}\) to a graded module over \(R[t]\) as follows:&lt;/p&gt;

&lt;p&gt;\[
   \alpha(\mathcal{M}) := \bigoplus_{i=0}^\infty M^i
\]&lt;/p&gt;

&lt;p&gt;We say that the \(n\)-th graded part of \(\alpha(\mathcal{M})\) is \(M^i\). Lastly we need to describe scalar multiplication by the polynomial generator \(t\) on a vector \((m^0, m^1, m^2, \cdots)\) for \(m^i \in M^i\). We define this as:
\[
   t (m^0, m^1, m^2, \cdots) := (0, \phi^0(m^0), \phi^1(m^1), \phi^2(m^2), \cdots)
\]
The action of \(t\) is simply to shift each element up one gradation level by means of using the linear maps \(\phi^i\) provided by the persistence module. It is easy to check that \(\alpha\) defines a functor from the category of persistence modules over \(R\) to the category of graded &lt;/p&gt;

&lt;div class=&quot;theorem&quot;&gt;
   \(\alpha\) defines an equivalence of categories between the category of persistence modules over \(R\) and the category of graded modules over \(R[t]\).
&lt;/div&gt;

&lt;div class=&quot;proof&quot;&gt;
   &lt;p&gt;
   First we show that \(\alpha\) is a functor from the category of persistence modules over \(R\) to the category of graded modules over \(R[t]\). Let \(\mathcal{M}\) and \(\mathcal{N}\) be persistence modules as describe above with linear maps \(\phi^i\) and \(\psi^i\) respectively. Let \(f : \mathcal{M} \to \mathcal{N}\) be a persistence module homomorphism, that is a family of module homomorphisms \(f^i : M^i \to N^i\). From the definition of \(\alpha\) above it is clear that \(\alpha(\mathcal{M})\) is indeed a module with a gradation, but we need to check that the \(i\) gradation of \(R[t]\) multiplied with the \(j\) gradation of the module, \(M^j\), is contained in \(M^{i+j}\). This holds because \(t^i (0, \cdots, 0, m^j, 0, \cdots) = (0, \cdots, 0, (\phi^{i+j-1}\circ\cdots\circ\phi^{j+1}\circ\phi^j)(m^j), 0, \cdots) \in M^{i+j}\). Next we need to check that the persistence module homomorphism \(f\) is mapped to a graded module homomorphism. We define \(\alpha(f) : \alpha(\mathcal{M}) \to \alpha(\mathcal{N})\) to be given by mapping \( (m^0, m^1, \cdots) \mapsto (f^0(m^0), f^1(m^1), \cdots) \). Since each \(f^i\) is a module homomorphism and gradation is respected, \(\alpha(f)\) is a graded module homomorphism. Thus \(\alpha\) is a functor from the category of persistence modules over \(R\) to the category of graded modules over \(R[t]\).
   &lt;/p&gt;

   &lt;!-- &lt;p&gt; --&gt;
   Now we show the reverse. Let \(M = \bigoplus_i M^i\) be a graded module over \(R[t]\). We define:
   \[
      \alpha^{-1}(M) := \{M^i\}
   \]
   with the linear map \(\phi^i : M^i \to M^{i+1}\) given by multiplication by \(t\). This is clearly a linear map, and thus \(\alpha^{-1}\) is well-defined. If \(g : M \to N\) is a graded module homomorphism, then \(\alpha^{-1}(g)\) is defined as simply applying \(g\) to each \(M^i\). Clearly \(\alpha^{-1}(g)\) is then a persistence module homomorphism. Finally note that by construction it is trivial to check that \(\alpha\) and \(\alpha^{-1}\) are inverses.
   &lt;!-- &lt;/p&gt; --&gt;
&lt;/div&gt;

&lt;p&gt;The intuition is that we use multiplication by \(t\) to keep track of the number of times an element of a module is mapped through the linear persistence maps.&lt;/p&gt;
&lt;h2 id=&quot;classification-theorem-for-mathbbn-persistence-modules&quot;&gt;Classification Theorem for \(\mathbb{N}\)-Persistence Modules&lt;/h2&gt;
&lt;p&gt;Showing that persistence modules are in correspondence with graded modules over \(R[t]\) opens the door to applying well-know classification theorems of graded modules. Unfortunately there is no simple classification when \(R\) is not a field, but for \(R = F\) a field, there exists a nice result:&lt;/p&gt;

&lt;div class=&quot;theorem&quot; text='Graded Module Classification'&gt;
   If \(M_*\) is a finitely generated graded \(F[t]\) module, then there exist integers \(\{i_1, \cdots, i_m\}\) and \(\{j_1, l_1, \cdots, j_n, l_n\}\) such that:
   \[
      M_* \cong \left( \bigoplus_{s=1}^m t^{i_s} \cdot F[t] \right) \oplus \left( \bigoplus_{r=1}^n t^{j_r} \cdot(F[t] / (t^{l_r} \cdot F[t])) \right)
   \]
   This classification is unique up to permutations of factors.
&lt;/div&gt;

&lt;p&gt;There are two components to the classification. On the left side of \(\oplus\), we have the free part which corresponds to elements of the persistence module that appear at index \(i_s\) and never disappear. On the right side we have the torsion part which corresponds to elements of the persistence module that appear at index \(j_r\) and die at index \(j_r + l_r\). This classification is the main result we are looking for, since this gives us a concrete computational tool with which to calculate \(p\)-persistence homology groups. In particular the torsion part above corresponds precisely to the \(p\)-persistence homology groups discussed previously. Note that using \(R = F\) a field means that rather than having persistence modules, we have persistence vector spaces. But to be able to apply this theorem we need to know that our graded modules over \(F[t]\) are finitely generated. The following theorem provides an exact condition:&lt;/p&gt;

&lt;div class=&quot;theorem&quot;&gt;
   Let \(\mathcal{V} = \{V^i, \phi^i\}\) be an \(\N\)-persistence vector space over a field \(F\). Then \(\alpha(\mathcal{V})\) (which is a graded module over \(F[t]\)) is finitely generated if and only if every \(V^n\) has finite dimension and there exists an \(n\) such that for all \(m \geq n, \phi^m : V^m \cong V^{m+1}\). In other words \(\mathcal{V}\) needs to be finite in both the interior dimension of each vector space and finite in the persistence. Such a \(\N\)-persistence vector space is called &lt;b&gt;tame.&lt;/b&gt;
&lt;/div&gt;

&lt;p&gt;In light of this condition we can give our final conclusion on the classification of \(\N\)-persistence vector spaces by incorporating the intuition discussed above. For a field \(F\) and any \(0 \leq m \leq n\) we define an \(\N\)-persistence vector space \(\mathcal{U}(m, n) := \{U_i(m, n), \phi^i\}\) where \(U_i(m, n) = F\) for \(m \leq i \leq n\) and 0 otherwise, and \(phi^i\) is defined to be the identity function for \(m \leq i \leq n-1\) and the 0 function otherwise. In addition we allow \(n = \infty\). If \(\mathcal{V}\) is an \(\N\) persistent vector that satisfies the conditions in the above theorem, then we have the following classification result:&lt;/p&gt;

&lt;div class=&quot;theorem&quot; text='Persistence Vector Space Classification'&gt;
   If \(\mathcal{V}\) is a tame \(\N\)-persistence vector space over a field \(F\), then it can be decomposed as:
   \[
      \mathcal{V} \cong \bigoplus_{i=0}^N \mathcal{U}(m_i, n_i)
   \]
   with \(\mathcal{U}(m, n)\) defined as above. This classification is unique up to permutations of factors.
&lt;/div&gt;

&lt;p&gt;There is a quite nice and practical way to visualize this decomposition, called a &lt;strong&gt;persistence barcode&lt;/strong&gt;. Each \(U(m, n)\) is drawn as a bar from index \(m\) to \(n\) with indices along the x-axis and different bars along the y-axis:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/post_assets/tda/barcode.png&quot; alt=&quot;sample barcode&quot; class=&quot;center&quot;/&gt;&lt;/p&gt;

&lt;p&gt;The start of a bar at \(m\) corresponds to the birth of a new \(U(m, n)\), until the bar disappears at index \(n\).&lt;/p&gt;
&lt;h2 id=&quot;applying-persistence-module-theory-to-268ech-complexes&quot;&gt;Applying Persistence Module Theory to &amp;#268;ech Complexes&lt;/h2&gt;
&lt;p&gt;Let&amp;#39;s leave the area of abstract algebra and conclude by applying the results of persistence module classification to &amp;#268;ech Complexes. Recall that varying values of radii gives inclusion maps between &amp;#268;ech Complexes of increasing radius. These inclusion maps induce homomorphsims at the simplicial chain level and at the simplicial homology level, thus making \(H_k(\C_\e(D); R)\) an \(\R\)-persistence module over \(R\) with linear maps the homomorphisms induced by inclusion. In addition recall that since \(D\) is a finite point cloud only finitely many \(\e\) will produce distinct complexes, so we can enumerate these \(\e_i\)&amp;#39;s to consider \(H_k(\C_\e(D); R)\) an \(\N\)-persistence module over \(R\) with linear maps \(\phi^i = i_k^{\e_i,\e_{i+1}}\).&lt;/p&gt;

&lt;p&gt;In order to apply the classification results we must choose \(R = F\) to be a field, such as \(\mathbb{Z}_2\), and then we have \(H_k(\C_\e(D); R)\) is an \(\N\)-persistence vector space over \(F\). In addition since \(D\) is finite each &amp;#268;ech Complex built from \(D\) will be finite and thus each \(H_k(\C_\e(D); F)\) will be finitely generated. Lastly, since we have only finitely many \(\e\), \(H_k(\C_\e(D); F)\) is tame. Therefore we can use the classification theorem above to compute persistent homology barcodes!&lt;/p&gt;
&lt;h1 id=&quot;future-directions-of-persistent-homology&quot;&gt;Future Directions of Persistent Homology&lt;/h1&gt;
&lt;p&gt;I&amp;#39;ve only outlined the most direct path for computing persistent homology from a point cloud. However, there are many other fascinating and important aspects to consider. In particular, studying the &lt;strong&gt;stability&lt;/strong&gt; of persistent homology computations involves seeing if slightly altering the point cloud \(D\) will drastically affect the barcode result, while statistical approaches to persistent homology consider more carefully the underlying probility distribution from which a point cloud is sampled, thus turning a barcode into a probabilistic object. Chazal&lt;sup id=&quot;fnref5&quot;&gt;&lt;a href=&quot;#fn5&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; gives a survey of both of these directions of current research.&lt;/p&gt;
&lt;h1 id=&quot;using-the-gudhi-python-library-to-compute-persistent-homology&quot;&gt;Using the GUDHI Python Library to Compute Persistent Homology&lt;/h1&gt;
&lt;p&gt;I would like to conclude on a matter of practicality. While understanding the underlying math is insightful and fun, I believe that TDA can have a significant impact on a wide number of fields and applications. Fortunately, algorithms to compute persistent homology have been &lt;a href=&quot;http://gudhi.gforge.inria.fr&quot;&gt;implemented for Python in the GUDHI Library&lt;/a&gt;. Using computing persistent homology via GUDHI is quite easy, and I believe can offer interesting and new insights for fields with data analysis. I won&amp;#39;t include a full tutorial, but a simple example of GUDHI in action should be sufficient to show how easy it is:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight python&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;gudhi&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;circle_sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;angles&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;angles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;angles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# First we sample 150 points from the unit circle and the radius 2 circle&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xs1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ys1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;circle_sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;150&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xs2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ys2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;circle_sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;150&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ys&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concatenate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xs2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concatenate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ys1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ys2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Then we build a Vietoris-Rips complex (2 skeleton)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RipsComplex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;points&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_edge_length&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;4.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Then we build a simplex tree, which is a fast data structure encoding the simplex&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# This is just an intermediate step&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create_simplex_tree&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_dimension&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Finally we compute the persistence, which is a list of the form [(k, (b, d))]&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# where k is the homology level, b is the birth time and d is the death time.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;diag&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;persistence&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Then we plot the persistence barcode and the original data side by side.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;gd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot_persistence_barcode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;gd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot_persistence_diagram&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Original data set&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;ro&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Running this code produces the following plots:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/post_assets/tda/circles.png&quot; alt=&quot;circles example&quot; class=&quot;center-full&quot;/&gt;&lt;/p&gt;

&lt;p&gt;In particular it shows the correct computation of the Betti numbers \(\beta_0 = 2, \beta_1 = 2\).&lt;/p&gt;
&lt;h1 id=&quot;references-1&quot;&gt;References&lt;/h1&gt;
&lt;!-- Hello!! --&gt;

&lt;div class=&quot;footnotes&quot;&gt;
&lt;hr&gt;
&lt;ol&gt;

&lt;li id=&quot;fn1&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0003267016300137&quot; class=&quot;dont-break-out&quot;&gt;https://www.sciencedirect.com/science/article/pii/S0003267016300137&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref1&quot; rev=&quot;footnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;

&lt;li id=&quot;fn2&quot;&gt;
&lt;p&gt;&lt;a href=&quot;http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf&quot; class=&quot;dont-break-out&quot;&gt;http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref2&quot; rev=&quot;footnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;

&lt;li id=&quot;fn3&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S1063520306000546&quot; class=&quot;dont-break-out&quot;&gt;https://www.sciencedirect.com/science/article/pii/S1063520306000546&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref3&quot; rev=&quot;footnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;

&lt;li id=&quot;fn4&quot;&gt;
&lt;p&gt;&lt;a href=&quot;http://www.ams.org/journals/bull/2009-46-02/S0273-0979-09-01249-X/S0273-0979-09-01249-X.pdf&quot; class=&quot;dont-break-out&quot;&gt;http://www.ams.org/journals/bull/2009-46-02/S0273-0979-09-01249-X/S0273-0979-09-01249-X.pdf&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref4&quot; rev=&quot;footnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;

&lt;li id=&quot;fn5&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1710.04019&quot; class=&quot;dont-break-out&quot;&gt;https://arxiv.org/abs/1710.04019&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref5&quot; rev=&quot;footnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;

&lt;li id=&quot;fn6&quot;&gt;
&lt;p&gt;Wattenberg, et al., &amp;quot;How to Use t-SNE Effectively&amp;quot;, Distill, 2016. &lt;a href=&quot;http://doi.org/10.23915/distill.00002&quot; class=&quot;dont-break-out&quot;&gt;http://doi.org/10.23915/distill.00002&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref6&quot; rev=&quot;footnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;

&lt;/ol&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Coding Serverless Functions in Idris</title>
   <link href="https://donaldpinckney.com/idris/2019/03/26/idris-serverless.html"/>
   <updated>2019-03-26T00:00:00-07:00</updated>
   <id>https://donaldpinckney.com/idris/2019/03/26/idris-serverless</id>
   <content type="html">&lt;h1 id=&quot;brief-intro-to-serverless-computing&quot;&gt;Brief Intro to Serverless Computing&lt;/h1&gt;
&lt;p&gt;Serverless computing platforms such as &lt;a href=&quot;https://aws.amazon.com/lambda/&quot;&gt;AWS Lambda&lt;/a&gt; and &lt;a href=&quot;https://cloud.google.com/functions/&quot;&gt;Google Cloud Functions&lt;/a&gt; have recently become a hot trend for writing backend code. With serverless computing we don&amp;#39;t need to setup, configure and maintain servers ourselves as it is handled automatically by the platform. Likewise, the platform will also automatically scale our code to more instances as needed, saving us both money and development time.&lt;/p&gt;

&lt;p&gt;The fundamental abstraction of serverless computing are &lt;em&gt;serverless functions&lt;/em&gt;, which is a piece of code that receives as input an HTTP request, and produces as output an HTTP response. If this is new to you, &lt;a href=&quot;https://medium.com/@BoweiHan/an-introduction-to-serverless-and-faas-functions-as-a-service-fb5cec0417b2&quot;&gt;check out this post&lt;/a&gt;. Most commonly these serverless functions are written in JavaScript or Python, the architecture of serverless functions make functional programming languages such as &lt;a href=&quot;https://www.haskell.org&quot;&gt;Haskell&lt;/a&gt; a really nice fit. With Haskell we can get some good guarantees about our code due to its quite powerful type system. However, since we often house critical business logic inside these serverless functions, it would be great if we can get absolute assurance that our code is correct. For this reason I think that dependently-typed programming languages, in particular &lt;a href=&quot;https://www.idris-lang.org&quot;&gt;Idris&lt;/a&gt;, are really interesting and promising languages for writing serverless functions.&lt;/p&gt;
&lt;h1 id=&quot;writing-serverless-functions-in-idris&quot;&gt;Writing Serverless Functions in Idris&lt;/h1&gt;
&lt;p&gt;In this post I want to walk through how we can write serverless functions in Idris and deploy them to Google Cloud. To do so, you need to make sure you have the necessary toolchains installed.&lt;/p&gt;
&lt;h2 id=&quot;installing-stuff&quot;&gt;Installing Stuff&lt;/h2&gt;
&lt;p&gt;First, Idris needs to be installed (see &lt;a href=&quot;https://www.idris-lang.org/download/&quot;&gt;here&lt;/a&gt; for more info). Assuming you already have Haskell and &lt;code&gt;cabal&lt;/code&gt; installed, it should be as easy as:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight shell&quot;&gt;&lt;code&gt;cabal update
cabal install idris
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In addition, for writing Idris code I highly recommend using the &lt;a href=&quot;https://atom.io&quot;&gt;Atom editor&lt;/a&gt; since Atom has a great Idris package &lt;code&gt;language-idris&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Since this tutorial specifically shows how to deploy to Google Cloud Functions, you need to make sure to have the Google Cloud SDK command line tools installed if you want to follow along directly. On the other hand, the techniques shown below will probably work just as well with AWS Lambda if you would rather use that, I just won&amp;#39;t give specific instructions since I haven&amp;#39;t tried it yet. If you want to setup Google Cloud Functions SDK, you can &lt;a href=&quot;https://cloud.google.com/functions/&quot;&gt;get started here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;writing-a-function-in-idris&quot;&gt;Writing a Function in Idris&lt;/h2&gt;
&lt;p&gt;In this post we won&amp;#39;t be looking at using Idris to prove correctness of code, just how to get any code at all to run on Google Cloud Functions. We can start out by writing a simple hello world function in Idris by making a new file &lt;code&gt;function.idr&lt;/code&gt; and putting in the following:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight plaintext&quot;&gt;&lt;code&gt;module MyFunction

hello : String -&amp;gt; String
hello req = &quot;Hello: &quot; ++ req
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;All that this function does is take as input a string (&lt;code&gt;req&lt;/code&gt;), and concatenate at the beginning &lt;code&gt;&amp;quot;Hello: &amp;quot;&lt;/code&gt;. You can test that this works by opening the Idris REPL with this file in your terminal, and calling the function:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight shell&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;idris &lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;.idr
     ____    __     _
    /  _/___/ /____&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;_&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;____
    / // __  / ___/ / ___/     Version 1.3.1-git:268db5dc2
  _/ // /_/ / /  / &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;__  &lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;      http://www.idris-lang.org/
 /___/&lt;span class=&quot;se&quot;&gt;\_&lt;/span&gt;_,_/_/  /_/____/       Type :? &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;help

&lt;/span&gt;Idris is free software with ABSOLUTELY NO WARRANTY.
For details &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt; :warranty.
&lt;span class=&quot;k&quot;&gt;*function&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; hello &lt;span class=&quot;s2&quot;&gt;&quot;Nancy&quot;&lt;/span&gt;
&lt;span class=&quot;s2&quot;&gt;&quot;Hello: Nancy&quot;&lt;/span&gt; : String
&lt;span class=&quot;k&quot;&gt;*function&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&quot;exporting-the-function-to-javascript&quot;&gt;Exporting the Function to JavaScript&lt;/h2&gt;
&lt;p&gt;This is great, but we need a way to run this code on Google Cloud Functions. Fortunately, Idris provides a built-in compiler to JavaScript, so we can compile our Idris function above into JavaScript code, which Google Cloud Functions directly supports. However, we still need a way to access the incoming HTTP request object, and write a string to the response. Since both the request and response are actual JavaScript objects, the most convenient thing to do is write some wrapper JavaScript which calls our &lt;code&gt;hello&lt;/code&gt; function.&lt;/p&gt;

&lt;p&gt;But to do this we need to make sure that JavaScript code can call our &lt;code&gt;hello&lt;/code&gt; function. To do this we just need to add an &lt;code&gt;FFI_Export&lt;/code&gt; declaration to the Idris code:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight plaintext&quot;&gt;&lt;code&gt;module MyFunction

export -- This is new
hello : String -&amp;gt; String
hello req = &quot;Hello: &quot; ++ req

-- This is all new
lib : FFI_Export FFI_JS &quot;&quot; []
lib =
    Fun hello &quot;hello&quot; $
    End
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;At this point it&amp;#39;s a good idea to test that the compilation and exporting to JavaScript actually works. In your terminal first compile the function to JavaScript:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight shell&quot;&gt;&lt;code&gt;idris &lt;span class=&quot;nt&quot;&gt;--codegen&lt;/span&gt; node &lt;span class=&quot;nt&quot;&gt;--interface&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;.idr &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;.js
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The &lt;code&gt;--codegen node&lt;/code&gt; options tells Idris to compile it to JavaScript rather than a binary, and the &lt;code&gt;--interface&lt;/code&gt; options tells Idris to create a node module with exports rather than a standalone executable script. If this compiles without problems, you can try loading the result in node and calling your &lt;code&gt;hello&lt;/code&gt; function from JavaScript. Assuming you have &lt;code&gt;node&lt;/code&gt; installed, you can try:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight shell&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;node
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; f &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; require&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'./function.js'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt; hello: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Function: MyFunction__hello] &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; f.hello&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Larry&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;s1&quot;&gt;'Hello: Larry'&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&quot;putting-the-pieces-together&quot;&gt;Putting the Pieces Together&lt;/h2&gt;
&lt;p&gt;At this point we have a function written in Idris, which we can call from JavaScript. All that we need to do is write a simple JavaScript wrapper which will be the main entry point of the serverless function, and which will call the Idris &lt;code&gt;hello&lt;/code&gt; function. Make a new file &lt;code&gt;index.js&lt;/code&gt; with this code:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight javascript&quot;&gt;&lt;code&gt;&lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;require&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'./function.js'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;nx&quot;&gt;exports&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;gcf_main&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;gcf_main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;req&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;send&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;hello&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;req&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;body&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This code first loads &lt;code&gt;function.js&lt;/code&gt; which is the compiled version of our Idris code &lt;code&gt;function.idr&lt;/code&gt;, and then defines &lt;code&gt;gcf_main&lt;/code&gt; which is the main entry point of our serverless function. All this does is extract the HTTP request body as a string, call the &lt;code&gt;hello&lt;/code&gt; function, and send the result in the HTTP response body. There should probably be some checking for existence and content type of the body, but this suffices for a demo example.&lt;/p&gt;

&lt;p&gt;This is all the code we need to write! You can deploy this to Google Cloud Functions with the command:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight shell&quot;&gt;&lt;code&gt;gcloud functions deploy my-function-name &lt;span class=&quot;nt&quot;&gt;--entry-point&lt;/span&gt; gcf_main &lt;span class=&quot;nt&quot;&gt;--runtime&lt;/span&gt; nodejs6 &lt;span class=&quot;nt&quot;&gt;--trigger-http&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You can put whatever you want for &lt;code&gt;my-function-name&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Once it finishes deploying it should report an &lt;code&gt;httpsTrigger&lt;/code&gt; URL, something like:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight plaintext&quot;&gt;&lt;code&gt;httpsTrigger:
  url: https://MY-DOMAIN.cloudfunctions.net/my-function-name
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;At this point you can send an actual HTTP request and get a response by using &lt;code&gt;curl&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight shell&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;curl &lt;span class=&quot;nt&quot;&gt;-X&lt;/span&gt; POST https://MY-DOMAIN.cloudfunctions.net/my-function-name &lt;span class=&quot;nt&quot;&gt;-H&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Content-Type:text/plain&quot;&lt;/span&gt;  &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'Suzie'&lt;/span&gt;
Hello: Suzie
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This shows that our compiled Idris code was actually successfully executed on Google&amp;#39;s servers. Pretty cool!&lt;/p&gt;
&lt;h1 id=&quot;what39s-next&quot;&gt;What&amp;#39;s Next&lt;/h1&gt;
&lt;p&gt;While easy to setup, the approach here does have a few problems. Most importantly, the Idris function doesn&amp;#39;t have very much control over accessing the HTTP request and response. For example, the Idris function has no way to return an HTTP status code other than 200, nor does it have a way to return content types other than plain text. In addition, the Idris function can&amp;#39;t branch on different request content types or other HTTP headers.&lt;/p&gt;

&lt;p&gt;To improve this we can try and put more logic into the JavaScript code wrapper. However, the purpose of using Idris is to be able to verify correctness of our code, so the more code we move into the JavaScript, the more code we aren&amp;#39;t able to prove correctness of. A different approach is to write everything in Idris, discard the JavaScript wrapper, and use Idris&amp;#39;s JavaScript FFI to access the request and response objects directly. This approach is a lot more involved to setup, and unfortunately &lt;a href=&quot;https://github.com/idris-lang/Idris-dev/issues/4656&quot;&gt;currently triggers a bug in the compiler&lt;/a&gt;. However, as this bug is worked out and libraries are developed to make accessing the request and response objects more convenient, I think this approach will become more promising.&lt;/p&gt;

&lt;p&gt;At the present, Idris provides a convenient way to write serverless functions in a dependently-typed language, giving you the ability to prove correctness of critical business logic. If you need to do a lot of manipulation of HTTP request and response objects then Idris might not be quite ready yet, but hopefully will be down the road. If you are a lighter user of HTTP APIs, then I suggest giving Idris a try, it&amp;#39;s quite fun!&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Feature Scaling</title>
   <link href="https://donaldpinckney.com/tensorflow/2018/11/15/feature-scaling.html"/>
   <updated>2018-11-15T00:00:00-08:00</updated>
   <id>https://donaldpinckney.com/tensorflow/2018/11/15/feature-scaling</id>
   <content type="html">&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: &quot;AMS&quot; } }
});
&lt;/script&gt;
&lt;h1 id=&quot;feature-scaling&quot;&gt;Feature Scaling&lt;/h1&gt;
&lt;p&gt;In chapters &lt;a href=&quot;/books/tensorflow/book/ch2-linreg/2017-12-03-single-variable.html&quot;&gt;2.1&lt;/a&gt;, &lt;a href=&quot;/books/tensorflow/book/ch2-linreg/2017-12-27-optimization.html&quot;&gt;2.2&lt;/a&gt;, &lt;a href=&quot;/books/tensorflow/book/ch2-linreg/2018-03-21-multi-variable.html&quot;&gt;2.3&lt;/a&gt; we used the gradient descent algorithm (or variants of) to minimize a loss function, and thus achieve a line of best fit. However, it turns out that the optimization in chapter 2.3 was much, much slower than it needed to be. While this isn’t a big problem for these fairly simple linear regression models that we can train in seconds anyways, this inefficiency becomes a much more drastic problem when dealing with large data sets and models.&lt;/p&gt;
&lt;h2 id=&quot;example-of-the-problem&quot;&gt;Example of the Problem&lt;/h2&gt;
&lt;p&gt;First, let’s look at a concrete example of the problem, by again considering a synthetic data set. Like in chapter 2.3 I generated a simple &lt;a href=&quot;/books/tensorflow/book/ch2-linreg/code/linreg-scaling-synthetic.csv&quot;&gt;synthetic data set&lt;/a&gt; consisting of 2 independent variables \(x_1\) and \(x_2\), and one dependent variable \(y = 2x_1 + 0.013x_2\). However, note that the range for \(x_1\) is 0 to 10, but the range for \(x_2\) is 0 to 1000. Let&amp;#39;s call this data set \(D\). A few sample data points look like this:&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;\(x_1\)&lt;/th&gt;
&lt;th&gt;\(x_2\)&lt;/th&gt;
&lt;th&gt;\(y\)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;7.36&lt;/td&gt;
&lt;td&gt;1000&lt;/td&gt;
&lt;td&gt;34.25&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;9.47&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;19.24&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.52&lt;/td&gt;
&lt;td&gt;315.78&lt;/td&gt;
&lt;td&gt;3.50&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.57&lt;/td&gt;
&lt;td&gt;315.78&lt;/td&gt;
&lt;td&gt;11.02&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;6.31&lt;/td&gt;
&lt;td&gt;263.15&lt;/td&gt;
&lt;td&gt;13.93&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.57&lt;/td&gt;
&lt;td&gt;526.31&lt;/td&gt;
&lt;td&gt;10.21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.52&lt;/td&gt;
&lt;td&gt;105.26&lt;/td&gt;
&lt;td&gt;3.41&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4.21&lt;/td&gt;
&lt;td&gt;842.10&lt;/td&gt;
&lt;td&gt;16.27&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2.63&lt;/td&gt;
&lt;td&gt;894.73&lt;/td&gt;
&lt;td&gt;19.04&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3.68&lt;/td&gt;
&lt;td&gt;210.52&lt;/td&gt;
&lt;td&gt;4.60&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;p&gt;For simplification I have excluded a constant term from this synthetic data set, and when training models I &amp;quot;cheated&amp;quot; by not training the constant term \(b\) and just setting it to 0, so as to simplify visualization. The model that I used was simply:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight python&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;y_predicted&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# A is a 1x2 matrix&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Since this is just a simple application of the ideas from chapter 2.3, one would expect this to work fairly easily. However, when using a &lt;code&gt;GradientDescentOptimizer&lt;/code&gt;, training goes rather poorly. Here is a plot showing the training progress of &lt;code&gt;A[0]&lt;/code&gt; and the loss function side-by-side:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/books/tensorflow/book/ch2-linreg/assets/scaling_plot1.png&quot; alt=&quot;Training progress without scaling&quot;&gt;&lt;/p&gt;

&lt;p&gt;This optimization was performed with a learning rate of 0.0000025, which is about the largest before it would diverge. The loss function quickly decreases at first, but then quickly stalls, and decreases quite slowly. Meanwhile, \(A_0\) is increasing towards the expected value of approximately 2.0, but very slowly. Within 5000 iterations this model fails to finish training.&lt;/p&gt;

&lt;p&gt;Now let&amp;#39;s do something rather strange: take the data set \(D\), and create a new data set \(D&amp;#39;\) by dividing all the \(x_2\) values by 100. The resulting data set \(D&amp;#39;\) looks roughly like this:&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;\(x_1&amp;#39;\)&lt;/th&gt;
&lt;th&gt;\(x_2&amp;#39;\)&lt;/th&gt;
&lt;th&gt;\(y&amp;#39;\)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;7.36&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;34.25&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;9.47&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;19.24&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.52&lt;/td&gt;
&lt;td&gt;3.1578&lt;/td&gt;
&lt;td&gt;3.50&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.57&lt;/td&gt;
&lt;td&gt;3.1578&lt;/td&gt;
&lt;td&gt;11.02&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;6.31&lt;/td&gt;
&lt;td&gt;2.6315&lt;/td&gt;
&lt;td&gt;13.93&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.57&lt;/td&gt;
&lt;td&gt;5.2631&lt;/td&gt;
&lt;td&gt;10.21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.52&lt;/td&gt;
&lt;td&gt;1.0526&lt;/td&gt;
&lt;td&gt;3.41&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4.21&lt;/td&gt;
&lt;td&gt;8.4210&lt;/td&gt;
&lt;td&gt;16.27&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2.63&lt;/td&gt;
&lt;td&gt;8.9473&lt;/td&gt;
&lt;td&gt;19.04&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3.68&lt;/td&gt;
&lt;td&gt;2.1052&lt;/td&gt;
&lt;td&gt;4.60&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;p&gt;A crucial note is that while \(D&amp;#39;\) is technically different from \(D\), it contains exactly the same information: one can convert between them freely, by dividing or multiplying the second column by 100. In fact, since this transformation is linear, and we are using a linear model, we can train our model on \(D&amp;#39;\) instead. We would just expect to obtain a value of 1.3 for &lt;code&gt;A[1]&lt;/code&gt; rather than 0.013. So let&amp;#39;s give it a try!&lt;/p&gt;

&lt;p&gt;The first interesting observation is that we can use a much larger learning rate. The largest learning rate we could use with \(D\) was 0.0000025, but with \(D&amp;#39;\) we can use a learning rate of 0.01. And when we plot &lt;code&gt;A[0]&lt;/code&gt; and the loss function for both \(D\) and \(D&amp;#39;\) we see something pretty crazy:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/books/tensorflow/book/ch2-linreg/assets/scaling_plot2.png&quot; alt=&quot;Training progress with scaling&quot;&gt;&lt;/p&gt;

&lt;p&gt;While training on \(D\) wasn&amp;#39;t even close to done after 5000 iterations, training on \(D&amp;#39;\) seems to have completed nearly instantly. If we zoom in on the first 60 iterations, we can see the training more clearly:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/books/tensorflow/book/ch2-linreg/assets/scaling_plot3.png&quot; alt=&quot;Training progress with scaling zoom&quot;&gt;&lt;/p&gt;

&lt;p&gt;So this incredibly simple data set transformation has changed a problem that was untrainable within 5000 iterations to one that can be trained practically instantly with 50 iterations. What is this black magic, and how does it work?&lt;/p&gt;
&lt;h2 id=&quot;visualizing-gradient-descent-with-level-sets&quot;&gt;Visualizing Gradient Descent with Level Sets&lt;/h2&gt;
&lt;p&gt;One of the best ways to gain insight in machine learning is by visualization. As seen in chapter 2.1 we can visualize loss functions using plots. Since we have 2 parameters &lt;code&gt;A[0]&lt;/code&gt; and &lt;code&gt;A[1]&lt;/code&gt; of the loss function, it would be a 3D plot. We used this for visualization in chapter 2.1, but frankly it&amp;#39;s a bit messy looking. Instead, we will use &lt;a href=&quot;https://en.wikipedia.org/wiki/Level_set&quot;&gt;level sets&lt;/a&gt; (also called contour plots), which use lines to indicate where the loss function has a constant value. An example is easiest, so here is the contour plot for the loss function for \(D&amp;#39;\), the one that converges quickly:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/books/tensorflow/book/ch2-linreg/assets/contour2.png&quot; alt=&quot;Contour plot of D&amp;#39;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Each ellipse border is where the loss function has a particular constant value (the innermost ones are labeled), and the red X marks the spot of the optimal values of &lt;code&gt;A[0]&lt;/code&gt; and &lt;code&gt;A[1]&lt;/code&gt;. By convention the particular constant values of the loss function are evenly spaced, which means that contour lines that are closer together indicate a &amp;quot;steeper&amp;quot; slope. In this plot, the center near the X is quite shallow, while far away is pretty steep. In addition, one diagonal axis of the ellipses is steeper than the other diagonal axis.&lt;/p&gt;

&lt;p&gt;We can also plot how &lt;code&gt;A[0]&lt;/code&gt; and &lt;code&gt;A[1]&lt;/code&gt; evolve during training on the contour plot, to get a feel for how gradient descent is working:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/books/tensorflow/book/ch2-linreg/assets/contour2_dots.png&quot; alt=&quot;Contour plot of D&amp;#39; with training&quot;&gt;&lt;/p&gt;

&lt;p&gt;Here, we can see that the initial values for &lt;code&gt;A[0]&lt;/code&gt; and &lt;code&gt;A[1]&lt;/code&gt; start pretty far off, but gradient descent quickly zeros in towards the X. As a side note, the line connecting each pair of dots is perpendicular to the line of the level set: see if you can figure out why.&lt;/p&gt;

&lt;p&gt;So if that is what the contour plot looks like for the &amp;quot;good&amp;quot; case of \(D&amp;#39;\), how does it look for \(D\)? Well, it looks rather strange:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/books/tensorflow/book/ch2-linreg/assets/contour1.png&quot; alt=&quot;Contour plot of D&quot;&gt;&lt;/p&gt;

&lt;p&gt;Don&amp;#39;t be fooled: like the previous plot the level sets also form ellipses, but they are so stretched that they are nearly straight lines at this scale. This means that vertically (&lt;code&gt;A[0]&lt;/code&gt; is constant and we vary &lt;code&gt;A[1]&lt;/code&gt;) there is substantial gradient, but horizontally (&lt;code&gt;A[1]&lt;/code&gt; is constant and we vary &lt;code&gt;A[0]&lt;/code&gt;) there is practically no slope. Put another way, gradient descent only knows to vary &lt;code&gt;A[1]&lt;/code&gt;, and (almost) doesn&amp;#39;t vary &lt;code&gt;A[0]&lt;/code&gt;.  We can test this hypothesis by plotting how gradient descent updates &lt;code&gt;A[0]&lt;/code&gt; and &lt;code&gt;A[1]&lt;/code&gt;, like above. Since gradient descent makes such little progress though, we have to zoom in a lot to see what is going on:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/books/tensorflow/book/ch2-linreg/assets/contour1_dots.png&quot; alt=&quot;Contour plot of D&quot;&gt;&lt;/p&gt;

&lt;p&gt;We can clearly see that gradient descent applies large updates to &lt;code&gt;A[1]&lt;/code&gt; (a bit too large, a smaller learning rate would have been a bit better) due to the large gradient in the &lt;code&gt;A[1]&lt;/code&gt; direction. But due to the (comparatively) tiny gradient in the &lt;code&gt;A[0]&lt;/code&gt; direction very small updates are done to &lt;code&gt;A[0]&lt;/code&gt;. Gradient descent quickly converges on the optimal value of &lt;code&gt;A[1]&lt;/code&gt;, but is very very far away from finding the optimal value of &lt;code&gt;A[0]&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Let&amp;#39;s take a quick look at what is going on mathematically to see why this happens. The model we are using is:
\[
    y&amp;#39;(x, A) = Ax = a_1 x_1 + a_2 x_2
\]
Here, \(a_1\) is in the role of &lt;code&gt;A[0]&lt;/code&gt; and \(a_2\) is &lt;code&gt;A[1]&lt;/code&gt;. We substitute this into the loss function to get:
\[
    L(a_1, a_2) = \sum_{i=1}^m (a_1 x_1^{(i)} + a_2 x_2^{(i)} - y^{(i)})^2
\]
Now if we differentiate \(L\) in the direction of \(a_1\) and \(a_2\) separately, we get:
\[
    \frac{\partial L}{\partial a_1} = \sum_{i=1}^m 2(a_1 x_1^{(i)} + a_2 x_2^{(i)} - y^{(i)})x_1^{(i)} \\
    \frac{\partial L}{\partial a_2} = \sum_{i=1}^m 2(a_1 x_1^{(i)} + a_2 x_2^{(i)} - y^{(i)})x_2^{(i)}
\]
Here we can see the problem: the inner terms of the derivatives are the same, except one is multiplied by \(x_1^{(i)}\) and the other by \(x_2^{(i)}\). If \(x_2\) is on average 100 times bigger than \(x_1\) (which it is in the original data set), then we would expect \(\frac{\partial L}{\partial a_2}\) to be roughly 100 times bigger than \(\frac{\partial L}{\partial a_1}\). It isn&amp;#39;t exactly 100 times larger, but with any reasonable data set it should be close. Since the derivatives in the directions of \(a_1\) and \(a_2\) are scaled completely differently, gradient descent fails to update both of them adequately.&lt;/p&gt;

&lt;p&gt;The solution is simple: we need to rescale the input features before training. This is exactly what happened when we mysteriously divided by 100: we rescaled \(x_2\) to be comparable to \(x_1\). But we should work out a more methodical way of rescaling, rather than randomly dividing by 100.&lt;/p&gt;
&lt;h2 id=&quot;rescaling-features&quot;&gt;Rescaling Features&lt;/h2&gt;
&lt;p&gt;This wasn&amp;#39;t explored above, but there are really two ways that we potentially need to rescale features. Consider an example where \(x_1\) ranges between -1 and 1, but \(x_2\) ranges between 99 and 101: both of these features have (at least approximately) the same &lt;a href=&quot;https://en.wikipedia.org/wiki/Standard_deviation&quot;&gt;standard deviation&lt;/a&gt;, but \(x_2\) has a much larger &lt;a href=&quot;https://en.wikipedia.org/wiki/Expected_value&quot;&gt;mean&lt;/a&gt;. On the other hand, consider an example where \(x_1\) still ranges between -1 and 1, but \(x_2\) ranges between -100 and 100. This time, they have the same mean, but \(x_2\) has a much larger standard deviation. Both of these situations can make gradient descent and related algorithms slower and less reliable. So, our goal is to ensure that all features have the same mean and standard deviation.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; There are other methods to measure how different features are, and then subsequently rescale them. For a look at more of them, feel free to consult the &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_scaling&quot;&gt;Wikipedia article&lt;/a&gt;. However, after implementing the technique presented here, any other technique is fairly similar and easy to implement if needed.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Without digressing too far into statistics, let&amp;#39;s quickly review how to calculate the mean \(\mu\) and standard deviation \(\sigma\). Suppose we have already read in our data set in Python into a Numpy vector / matrix, and have all the values for the feature \(x_j\), for each \(j\). Mathematically, the mean for feature \(x_j\) is just the average:
\[
    \mu_j = \frac{\sum_{i=1}^m x_j^{(i)}}{m}
\]
In Numpy there is a convenient &lt;code&gt;np.mean()&lt;/code&gt; function we will use.&lt;/p&gt;

&lt;p&gt;The standard deviation \(\sigma\) is a bit more tricky. We measure how far each point \(x_j^{(i)}\) is from the mean \(\mu_j\), square this, then take the mean of all of this, and finally square root it:
\[
    \sigma_j = \sqrt{\frac{\sum_{i=1}^m (x_j^{(i)} - \mu_j)^2 }{m}}
\]
Again, Numpy provides a convenient &lt;code&gt;np.std()&lt;/code&gt; function.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Statistics nerds might point out that in the above equation we should divide by \(m - 1\) instead of \(m\) to obtain an unbiased estimate of the standard deviation. This might well be true, but in this usage it does not matter since we are only using this to rescale features relative to each other, and not make a rigorous statistical inference. To be more precise, doing so would involve multiplying each scaled feature by only a constant factor, and will not change any of their standard deviations or means relative to each other.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Once we have every mean \(\mu_j\) and standard deviation \(\sigma_j\), rescaling is easy: we simply rescale every feature like so:&lt;/p&gt;

&lt;p&gt;\[
\begin{equation} \label{eq:scaling}
    x_j&amp;#39; = \frac{x_j - \mu_j}{\sigma_j}
\end{equation}
\]&lt;/p&gt;

&lt;p&gt;This will force every feature to have a mean of 0 and a standard deviation of 1, and thus be scaled well relative to each other.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Be careful to make sure to perform the rescaling at both training time and prediction time. That is, we first have to perform the rescaling on the whole training data set, and then train the model so as to achieve good training performance. Once the model is trained and we have a new, never before seen input \(x\), we also need to rescale its features to \(x&amp;#39;\) because the trained model only understands inputs that have already been rescaled (because we trained it that way).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;implementation-and-experiments&quot;&gt;Implementation and Experiments&lt;/h2&gt;
&lt;p&gt;Feature scaling can be applied to just about any multi-variable model. Since the only multi-variable model we have seen so far is multi-variable linear regression, we&amp;#39;ll look at implementing feature scaling in that context, but the ideas and the code are pretty much the same for other models. First let&amp;#39;s just copy-paste the original multi-variable linear regression code, and modify it to load the &lt;a href=&quot;/books/tensorflow/book/ch2-linreg/code/linreg-scaling-synthetic.csv&quot;&gt;new synthetic data set&lt;/a&gt;:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight python&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# First we load the entire CSV file into an m x n matrix&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;linreg-scaling-synthetic.csv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;header&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Make a convenient variable to remember the number of input columns&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# We extract all rows and the first n columns into X_data&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Then we flip it&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# We extract all rows and the last column into y_data&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Then we flip it&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;



&lt;span class=&quot;c&quot;&gt;# Define data placeholders&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define trainable variables&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;A&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;b&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define model output&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_predicted&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define the loss function&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduce_sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_predicted&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define optimizer object&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AdamOptimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minimize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Create a session and initialize variables&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;session&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Session&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;session&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;global_variables_initializer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Main optimization loop&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;session&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feed_dict&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_data&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;t = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;g, loss = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;g, A = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s, b = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;g&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you run this code right now, you might see output like the following:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight plaintext&quot;&gt;&lt;code&gt;...
t = 1995, loss = 0.505296, A = [[1.9920335  0.01292674]], b = 0.089939
t = 1996, loss = 0.5042, A = [[1.9920422  0.01292683]], b = 0.0898404
t = 1997, loss = 0.5031, A = [[1.9920509  0.01292691]], b = 0.0897419
t = 1998, loss = 0.501984, A = [[1.9920596  0.01292699]], b = 0.0896435
t = 1999, loss = 0.50089, A = [[1.9920683  0.01292707]], b = 0.0895451
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that if you run this code multiple times you will get different results each time due to the random initialization. The synthetic data was generated with the equation \(y = 2x_1 + 0.013x_2 + 0\). So after 2000 iterations of training we are getting close-ish to the correct values, but it&amp;#39;s not fully trained. So let&amp;#39;s implement feature scaling to fix this.&lt;/p&gt;

&lt;p&gt;The first step is to compute the mean and standard deviation for each feature in the training data set. Add the following after &lt;code&gt;X_data&lt;/code&gt; is loaded:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight python&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;means&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;deviations&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Using &lt;code&gt;axis=1&lt;/code&gt; means that we compute the mean for each feature, averaged over all data points. That is, &lt;code&gt;X_data&lt;/code&gt; is a \(2 \times 400\) matrix, &lt;code&gt;means&lt;/code&gt; is a \(2 \times 1\) matrix (vector). If we had used &lt;code&gt;axis=0&lt;/code&gt;, &lt;code&gt;means&lt;/code&gt; would be a \(1 \times 400\) matrix, which is not what we want.&lt;/p&gt;

&lt;p&gt;Now that we know the means and standard deviations of each feature, we have to use them to transform the inputs via Equation \((\ref{eq:scaling})\). We have two options: we could implement the transformation directly using &lt;code&gt;numpy&lt;/code&gt; to transform &lt;code&gt;X_data&lt;/code&gt; before training, or we could include the transformation within the TensorFlow computations. But since we need to do the transformation again when we want to predict output given new inputs, including it in the TensorFlow computations will be a bit more convenient.&lt;/p&gt;

&lt;p&gt;Just like before we setup &lt;code&gt;x&lt;/code&gt; as a placeholder, so this code is identical:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight python&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Define data placeholders&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now we want to transform &lt;code&gt;x&lt;/code&gt; according to Equation \((\ref{eq:scaling})\). We can define a new TensorFlow value &lt;code&gt;x_scaled&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight python&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Apply the rescaling&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x_scaled&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;means&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;deviations&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;One important concept is that when this line of code runs, &lt;code&gt;means&lt;/code&gt; and &lt;code&gt;deviations&lt;/code&gt; have already been computed: they are actual matrices. So to TensorFlow they are &lt;strong&gt;constants&lt;/strong&gt;: they are neither trainable variables that will be updated during optimization, nor are they placeholders that need to be fed values later. They have already been computed, and TensorFlow just uses the already computed values directly.&lt;/p&gt;

&lt;p&gt;Also, note that since &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;means&lt;/code&gt; are compatibly sized matrices, the subtraction (and division) will be done separately for each feature, automatically. So while Equation \((\ref{eq:scaling})\) technically says how to scale one feature individually, this single line of code scales all the features.&lt;/p&gt;

&lt;p&gt;Now, everywhere that we use &lt;code&gt;x&lt;/code&gt; in the model we should now use &lt;code&gt;x_scaled&lt;/code&gt; instead. For us the only code that needs to change is in defining the model&amp;#39;s prediction:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight python&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Define model output, using the scaled features&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_predicted&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_scaled&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that when we run the session we do &lt;em&gt;not&lt;/em&gt; feed values to &lt;code&gt;x_scaled&lt;/code&gt;, because we want to feed unscaled values to &lt;code&gt;x&lt;/code&gt; which will then automatically get scaled when &lt;code&gt;x_scaled&lt;/code&gt; is computed based on what is fed to &lt;code&gt;x&lt;/code&gt;. We instead continue to feed values to &lt;code&gt;x&lt;/code&gt; as before.&lt;/p&gt;

&lt;p&gt;That&amp;#39;s all that we need to implement for feature scaling, it&amp;#39;s really pretty easy. If we run this code now we should see:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight plaintext&quot;&gt;&lt;code&gt;...
t = 1995, loss = 1.17733e-07, A = [[6.0697703 3.9453504]], b = 16.5
t = 1996, loss = 1.17733e-07, A = [[6.0697703 3.9453504]], b = 16.5
t = 1997, loss = 1.17733e-07, A = [[6.0697703 3.9453504]], b = 16.5
t = 1998, loss = 1.17733e-07, A = [[6.0697703 3.9453504]], b = 16.5
t = 1999, loss = 1.17733e-07, A = [[6.0697703 3.9453504]], b = 16.5
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The fact that none of the trained weights are updating anymore and that the loss function is very small is a good sign that we have achieved convergence. But the weights are not the values we expect... shouldn&amp;#39;t we get &lt;code&gt;A = [[2, 0.013]], b = 0&lt;/code&gt;, since that is the equation that generated the synthetic data? Actually, the weights we got &lt;em&gt;are&lt;/em&gt; correct because the model is now being trained on the rescaled data set, so we get different weights at the end. See Challenge Problem 3 below to explore this more.&lt;/p&gt;
&lt;h1 id=&quot;concluding-remarks-4&quot;&gt;Concluding Remarks&lt;/h1&gt;
&lt;p&gt;We&amp;#39;ve seen how to implement feature scaling for a simple multi-variable linear regression model.  While this is a fairly simple model, the principles are basically the same for applying feature scaling to more complex models (which we will see very soon). In fact, the code is pretty much identical: just compute the means and standard deviations, apply the formula to &lt;code&gt;x&lt;/code&gt; to compute &lt;code&gt;x_scaled&lt;/code&gt;, and anywhere you use &lt;code&gt;x&lt;/code&gt; in the model, just use &lt;code&gt;x_scaled&lt;/code&gt; instead.&lt;/p&gt;

&lt;p&gt;Whether or not feature scaling helps is dependent on the problem and the model. If you have having trouble getting your model to converge, you can always implement feature scaling and see how it affects the training. While it wasn&amp;#39;t used in this chapter, using TensorBoard (see &lt;a href=&quot;/books/tensorflow/book/ch2-linreg/2017-12-27-optimization.html&quot;&gt;chapter 2.2&lt;/a&gt;) is a great way to run experiments to compare training with and without feature scaling.&lt;/p&gt;
&lt;h1 id=&quot;challenge-problems-3&quot;&gt;Challenge Problems&lt;/h1&gt;
&lt;!-- &lt;ol&gt;
    &lt;li&gt;**Coding:** Take one of the challenge problems from [the previous chapter](/books/tensorflow/book/ch2-linreg/2018-03-21-multi-variable.html#challenge-problems), and implement it with and without feature scaling, and then compare how training performs.&lt;/li&gt;
&lt;/ol&gt; --&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Coding:&lt;/strong&gt; Take one of the challenge problems from &lt;a href=&quot;/books/tensorflow/book/ch2-linreg/2018-03-21-multi-variable.html#challenge-problems&quot;&gt;the previous chapter&lt;/a&gt;, and implement it with and without feature scaling, and then compare how training performs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Coding:&lt;/strong&gt; Add TensorBoard support to the code from this chapter, and use it to explore for yourself the effect feature scaling has. Also, you can compare with different optimization algorithms.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Theory:&lt;/strong&gt; One problem with feature scaling is that we learn different model parameters. In this chapter, the original data set was generated with \(y = 2x_1 + 0.013x_2 + 0\), but the parameters that the model learned were \(a_1&amp;#39; = 6.0697703, a_2&amp;#39; = 3.9453504, b&amp;#39; = 16.5\). Note that I have called these learned parameters \(a_1&amp;#39;\) rather than \(a_1\) to indicate that these learned parameters are for the &lt;em&gt;rescaled&lt;/em&gt; data.&lt;br/&gt;&lt;br /&gt;One important use of linear regression is to explore and understand a data set, not just predict outputs. After doing a regression, one can understand through the learned weights how severely each feature affects the output. For example, if we have two features \(A\) and \(B\) which are used to predict rates of cancer, and the learned weight for \(A\) is much larger than the learned weight for \(B\), this indicates that occurrence of \(A\) is more correlated with cancer than occurrence of \(B\) is, which is interesting in its own right. Unfortunately, performing feature scaling destroys this: since we have rescaled the training data, the weight for \(A\) (\(a_1&amp;#39;\)) has lost its relevance to the values of \(A\) in the real world. But all is not lost, since we can actually recover the &amp;quot;not-rescaled&amp;quot; parameters from the learned parameters, which we will derive now.&lt;br/&gt;&lt;br/&gt;First, suppose that we have learned the &amp;quot;rescaled&amp;quot; parameters \(a_1&amp;#39;, a_2&amp;#39;, b&amp;#39;\). That is, we predict the output \(y\) to be: \[\begin{equation}\label{eq:start}y = a_1&amp;#39; x_1&amp;#39; + a_2&amp;#39; x_2&amp;#39; + b&amp;#39;\end{equation}\]where \(x_1&amp;#39;, x_2&amp;#39;\) are the rescaled features. Now, we &lt;em&gt;want&lt;/em&gt; a model that uses some (yet unknown) weights \(a_1, a_2, b\) to predict the same output \(y\), but using the not-rescaled features \(x_1, x_2\). That is, we want to find \(a_1, a_2, b\) such that this is true:\[\begin{equation}\label{eq:goal}y = a_1 x_1 + a_2 x_2 + b\end{equation}\]To go about doing this, substitute for \(x_1&amp;#39;\) and \(x_2&amp;#39;\) using Equation \((\ref{eq:scaling})\) into Equation \((\ref{eq:start})\), and then compare it to Equation \((\ref{eq:goal})\) to obtain 3 equations: one to relate each scaled and not-scaled parameter. Finally, you can use these equations to be able to compute the &amp;quot;not-scaled&amp;quot; parameters in terms of the learned parameters. You can check your work by computing the not-scaled parameters in terms of the learned parameters for the synthetic data set, and verify that they match the expected values.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&quot;complete-code-3&quot;&gt;Complete Code&lt;/h1&gt;
&lt;p&gt;The &lt;a href=&quot;https://github.com/donald-pinckney/donald-pinckney.github.io/blob/src/books/tensorflow/src/ch2-linreg/code/feature_scaling.py&quot;&gt;complete example code is available on GitHub&lt;/a&gt;, as well as directly here:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight python&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# First we load the entire CSV file into an m x n matrix&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;linreg-scaling-synthetic.csv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;header&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Make a convenient variable to remember the number of input columns&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# We extract all rows and the first n columns into X_data&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Then we flip it&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# We extract all rows and the last column into y_data&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Then we flip it&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# We compute the mean and standard deviation of each feature&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;means&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;deviations&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;means&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;deviations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define data placeholders&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Apply the rescaling&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x_scaled&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;means&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;deviations&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define trainable variables&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;A&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;b&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define model output, using the scaled features&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_predicted&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_scaled&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define the loss function&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduce_sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_predicted&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define optimizer object&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AdamOptimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minimize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Create a session and initialize variables&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;session&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Session&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;session&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;global_variables_initializer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Main optimization loop&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;session&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feed_dict&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_data&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;t = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;g, loss = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;g, A = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s, b = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;g&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content>
 </entry>
 
 <entry>
   <title>Metal 3D Graphics Part 2: Animated Uniform Data with Synchronization</title>
   <link href="https://donaldpinckney.com/metal/2018/07/27/metal-intro-2.html"/>
   <updated>2018-07-27T00:00:00-07:00</updated>
   <id>https://donaldpinckney.com/metal/2018/07/27/metal-intro-2</id>
   <content type="html">&lt;h1 id=&quot;recap&quot;&gt;Recap&lt;/h1&gt;
&lt;p&gt;In the &lt;a href=&quot;/metal/2018/07/05/metal-intro-1.html&quot;&gt;previous post&lt;/a&gt; we setup the necessary basic code to render a multi-colored triangle using &lt;a href=&quot;https://developer.apple.com/metal/&quot;&gt;Metal&lt;/a&gt;. By doing so we learned about the fundamental concepts of the Metal API:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A &lt;a href=&quot;https://developer.apple.com/documentation/metal/mtldevice&quot;&gt;MTLDevice&lt;/a&gt; represents the actual GPU.&lt;/li&gt;
&lt;li&gt;A &lt;a href=&quot;https://developer.apple.com/documentation/metal/MTLRenderPipelineDescriptor&quot;&gt;MTLRenderPipelineDescriptor&lt;/a&gt; / &lt;a href=&quot;https://developer.apple.com/documentation/metal/MTLRenderPipelineState&quot;&gt;MTLRenderPipelineState&lt;/a&gt; describes what the render pipeline consists of. In particular, this is where we specify what vertex and fragment shaders to use. A &lt;code&gt;MTLRenderPipelineState&lt;/code&gt; is just a compiled version of a &lt;code&gt;MTLRenderPipelineDescriptor&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;A &lt;a href=&quot;https://developer.apple.com/documentation/metal/MTLCommandQueue&quot;&gt;MTLCommandQueue&lt;/a&gt; keeps track of many &lt;code&gt;MTLCommandBuffer&lt;/code&gt;s that are waiting in line to be executed.&lt;/li&gt;
&lt;li&gt;A &lt;a href=&quot;https://developer.apple.com/documentation/metal/MTLCommandBuffer&quot;&gt;MTLCommandBuffer&lt;/a&gt; represents the entire set of information the GPU needs to execute this pipeline: it contains the pipeline info itself, as well as vertex data and drawing commands that will be fed into the pipeline by the GPU. &lt;/li&gt;
&lt;li&gt;A &lt;a href=&quot;https://developer.apple.com/documentation/metal/MTLRenderPassDescriptor&quot;&gt;MTLRenderPassDescriptor&lt;/a&gt; is used to configure the interface of the pipeline, but not the interior of the pipeline. It is like the 2 openings of the pipe. For example, a &lt;code&gt;MTLRenderPassDescriptor&lt;/code&gt; contains information such as the destination resolution to render to. This information is included in each &lt;code&gt;MTLCommandBuffer&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;A &lt;a href=&quot;https://developer.apple.com/documentation/metal/MTLRenderCommandEncoder&quot;&gt;MTLRenderCommandEncoder&lt;/a&gt; is used to prepare the vertex data and drawing commands that will be fed into the pipeline.&lt;/li&gt;
&lt;li&gt;A &lt;a href=&quot;https://developer.apple.com/documentation/metal/MTLBuffer&quot;&gt;MTLBuffer&lt;/a&gt; represents a chunk of data which can be accessed by either the CPU and GPU. We used a &lt;code&gt;MTLBuffer&lt;/code&gt; to create the vertex data on the CPU, and let the GPU access it in the shader code.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We managed to render a triangle, which looks like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/post_assets/metal/metal-intro-1/screen1.png&quot; alt=&quot;Colored Triangle&quot;&gt;&lt;/p&gt;

&lt;p&gt;This post will build on top of the last post by adding what is called &lt;strong&gt;uniform data&lt;/strong&gt; (or just &lt;strong&gt;uniforms&lt;/strong&gt;) to the rendering process. Use your code from the previous post, or feel free to &lt;a href=&quot;/public/post_assets/metal/metal-intro-1/MetalIntro1.zip&quot;&gt;download my code&lt;/a&gt; and use it.&lt;/p&gt;
&lt;h1 id=&quot;what-is-a-uniform-and-what-is-it-good-for&quot;&gt;What is a uniform, and what is it good for?&lt;/h1&gt;
&lt;p&gt;Right now we can render a triangle, but the output of the rendering is always identical. Having the rendering output be different each frame is necessary for almost any real-time graphics application: for example, every 3D video game needs to move the camera around. In future posts we will actually look at moving the camera around in a 3D scene, but for now we will settle for something more modest; a triangle that changes brightness overtime:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/post_assets/metal/metal-intro-2/screen1.gif&quot; alt=&quot;Colored Triangle with Changing Brightness&quot;&gt;&lt;/p&gt;

&lt;p&gt;Let&amp;#39;s brainstorm how we could accomplish this effect.  We currently have a &lt;code&gt;MTLBuffer&lt;/code&gt; that contains the vertex data for drawing the triangle. Each vertex data contains the position and color. The vertex data we currently have in it is:&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Color&lt;/th&gt;
&lt;th&gt;Position&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;(1, 0, 0, 1)&lt;/td&gt;
&lt;td&gt;(-1, -1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;(0, 1, 0, 1)&lt;/td&gt;
&lt;td&gt;(0, 1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;(0, 0, 1, 1)&lt;/td&gt;
&lt;td&gt;(1, -1)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;p&gt;Since we are describing the color of each vertex in this buffer, what we could do is modify the color of each vertex during every new frame. For example, during the second frame we could change the vertex buffer data to:&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Color&lt;/th&gt;
&lt;th&gt;Position&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;(0.9, 0, 0, 1)&lt;/td&gt;
&lt;td&gt;(-1, -1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;(0, 0.9, 0, 1)&lt;/td&gt;
&lt;td&gt;(0, 1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;(0, 0, 0.9, 1)&lt;/td&gt;
&lt;td&gt;(1, -1)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;p&gt;to make the triangle a bit darker, and so on. Now, this strategy can work, but it&amp;#39;s a poor way to go about it. Consider what happens if we have a ton of vertices to render complex shapes. To adjust the brightness of the complex shapes we would need to change the color data for all of the vertices using the CPU. The amount of work the CPU has to do grows linearly with the number of vertices, which is bad news when you want to have a complex scene with lots of vertices.&lt;/p&gt;

&lt;p&gt;We can instead achieve this effect by modifying our shader code to make it a bit more general. How can we mathematically make a color darker or brighter? Since bright red is &lt;code&gt;(1, 0, 0, 1)&lt;/code&gt; and the darkest red (that is, pure black) is &lt;code&gt;(0, 0, 0, 1)&lt;/code&gt;, we can simply multiply our color by a number between 0 and 1:
\[
    \tilde{c} = c \cdot brightness, \;\; brightness \in [0, 1] 
\]&lt;/p&gt;

&lt;p&gt;For example, if \(c = (1, 0, 0)\), and \(brightness = 0.9\), then the final color output would be the almost fully-bright red \(\tilde{c} = (0.9, 0, 0)\).&lt;/p&gt;

&lt;p&gt;Now, this \(brightness\) variable is what we call a &lt;strong&gt;uniform&lt;/strong&gt;. Like vertex data we use it in our shaders to affect the final pixel color, but it is &lt;em&gt;uniform&lt;/em&gt; across all the vertices. While there are 3 vertices, each with different position and color data, there is only 1 \(brightness\) value. I like to think of it as a knob on the control panel of our shader: our shader will take in the vertex data and accordingly render a triangle, and the \(brightness\) know provides a way to adjust the brightness of the rendering output. And who is responsible for turning the knob? The CPU is.&lt;/p&gt;

&lt;p&gt;Each frame the CPU can update the brightness, a single floating point number, to affect the rendering of the entire triangle. Much more efficient than modifying every vertex&amp;#39;s color data!&lt;/p&gt;
&lt;h1 id=&quot;adding-uniform-parameters-to-the-shader-code&quot;&gt;Adding uniform parameters to the shader code&lt;/h1&gt;
&lt;p&gt;Enough talk, let&amp;#39;s get on with the code! We will start by first adding the code for our brightness uniform to the shaders. In pseduo-Metal-code, we want to write something like this for our fragment shader:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight cpp&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;fragment&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;float4&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fragmentShader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;VertexOut&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;interpolated&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stage_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;brightness&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;interpolated&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;However, this isn&amp;#39;t quite right for 2 reasons: the first is obvious, we need to define the variable &lt;code&gt;brightness&lt;/code&gt;. The second reason will be discussed soon. So how do we define &lt;code&gt;brightness&lt;/code&gt;? That is, how does the CPU pass data (the value of &lt;code&gt;brightness&lt;/code&gt;) to the GPU? It works in essentially the same way as vertex data: we use a &lt;code&gt;MTLBuffer&lt;/code&gt; on the CPU side, and a &lt;code&gt;[[buffer(0)]]&lt;/code&gt; on the GPU side, just like the vertex buffer data that is passed to the vertex shader. First, crack open &lt;code&gt;ShaderDefinitions.h&lt;/code&gt; and add the following &lt;code&gt;struct&lt;/code&gt; definition:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight cpp&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FragmentUniforms&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;brightness&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is a &lt;code&gt;struct&lt;/code&gt; that will hold all of the uniforms that will be passed to our fragment shader. Right now it only contains one uniform, &lt;code&gt;brightness&lt;/code&gt;. Then we can add it as a buffer parameter to the fragment shader, and use the &lt;code&gt;brightness&lt;/code&gt; member of the struct to do the multiplication:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight cpp&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;fragment&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;float4&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fragmentShader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;VertexOut&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;interpolated&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stage_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;constant&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FragmentUniforms&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uniforms&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;buffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]])&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;float4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uniforms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;brightness&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;interpolated&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;interpolated&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;While this code isn&amp;#39;t entirely foreign looking since it is similar to the vertex buffer parameter, there are some differences to walk through. Most importantly, we declare the buffer parameter to be &lt;code&gt;constant&lt;/code&gt; instead of &lt;code&gt;device&lt;/code&gt; because each pixel that is rendered will access the same memory location of the buffer (just the since &lt;code&gt;brightness&lt;/code&gt; member). We also have the parameter be a reference (&lt;code&gt;&amp;amp;&lt;/code&gt;) instead of a pointer (&lt;code&gt;*&lt;/code&gt;). They are quite similar and both perform the role of referencing a location in memory, but references can&amp;#39;t be used to reference an &lt;em&gt;array&lt;/em&gt; of structs, just a single struct. We will not have an array of &lt;code&gt;FragmentUniforms&lt;/code&gt;, unlike for the vertex buffer where we did have an array of 3 &lt;code&gt;Vertex&lt;/code&gt; structs. Since we don&amp;#39;t have an array here, we are free to use either a pointer or a reference or a pointer.&lt;/p&gt;

&lt;p&gt;Finally, the multiplication is a bit more convoluted than just doing &lt;code&gt;uniforms.brightness * interpolated.color&lt;/code&gt;. This is because the colors are in &lt;code&gt;(r, g, b, a)&lt;/code&gt; format, and we want to make the color darker by decreasing &lt;code&gt;r&lt;/code&gt;, &lt;code&gt;g&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;, but we want to leave &lt;code&gt;a&lt;/code&gt; (the alpha value) alone. So we grab just the &lt;code&gt;rgb&lt;/code&gt; components of the interpolated color, multiply those by &lt;code&gt;uniforms.brightness&lt;/code&gt;, and repack the result into a &lt;code&gt;float4&lt;/code&gt; with the original alpha value.&lt;/p&gt;

&lt;p&gt;This is all the shader code we need to write. If we run this now, we get the following crash:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight plaintext&quot;&gt;&lt;code&gt;validateFunctionArguments:3332: failed assertion `Fragment Function(fragmentShader): missing buffer binding at index 0 for uniforms[0].'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is perfect, Metal tells us directly that the fragment shader is expecting a buffer to be bound at index 0, and we didn&amp;#39;t do that yet. Let&amp;#39;s get to it!&lt;/p&gt;
&lt;h1 id=&quot;creating-and-binding-a-uniform-buffer&quot;&gt;Creating and binding a uniform buffer&lt;/h1&gt;
&lt;p&gt;Open up &lt;code&gt;Renderer.swift&lt;/code&gt;, and first define a new buffer in the class:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight swift&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;fragmentUniformsBuffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;MTLBuffer&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now in the initializer we need to setup this buffer. The buffer will contain the data for only one &lt;code&gt;FragmentUniforms&lt;/code&gt; struct, so we don&amp;#39;t use an array but just a direct pointer. Add this below the initialization of the vertex data buffer:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight swift&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// Create our uniform buffer, and fill it with an initial brightness of 1.0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;initialFragmentUniforms&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;FragmentUniforms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;brightness&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fragmentUniformsBuffer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;makeBuffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;bytes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;initialFragmentUniforms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;MemoryLayout&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;FragmentUniforms&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;options&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It&amp;#39;s important to note that &lt;code&gt;initialFragmentUniforms&lt;/code&gt; is just a temporary instance of &lt;code&gt;FragmentUniforms&lt;/code&gt;: we create it and then copy the data of it into the &lt;code&gt;fragmentUniformsBuffer&lt;/code&gt;, but once the initializer finishes, the actual memory of &lt;code&gt;initialFragmentUniforms&lt;/code&gt; will be gone forever. &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Side remark about mutability in Swift: we need &lt;code&gt;initialFragmentUniforms&lt;/code&gt; to be mutable (use &lt;code&gt;var&lt;/code&gt; instead of &lt;code&gt;let&lt;/code&gt;) since a pointer of it is passed to &lt;code&gt;makeBuffer(...)&lt;/code&gt;. Metal &lt;em&gt;could&lt;/em&gt; modify &lt;code&gt;initialFragmentUniforms&lt;/code&gt; via this pointer, so we have to mark &lt;code&gt;initialFragmentUniforms&lt;/code&gt; as mutable. Of course Metal doesn&amp;#39;t do this, it just benignly copies the data somewhere else, but Swift doesn&amp;#39;t know that.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now we need to bind our &lt;code&gt;fragmentUniformsBuffer&lt;/code&gt; to the fragment shader. Just like the vertex data, we perform this binding every frame using the &lt;code&gt;renderEncoder&lt;/code&gt;. Inside &lt;code&gt;draw(in view: MTKView)&lt;/code&gt; we currently have the code:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight swift&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// What vertex buffer data to use&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;renderEncoder&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;setVertexBuffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vertexBuffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;offset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// And what to draw&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;renderEncoder&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;drawPrimitives&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;triangle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;vertexStart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;vertexCount&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is where we bind the vertex buffer to the buffer at index 0 in the vertex shader. We do the same thing, but with the fragment shader and fragment uniforms by adding the following line before the &lt;code&gt;drawPrimitives&lt;/code&gt; call:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight swift&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// Bind the fragment uniforms&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;renderEncoder&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;setFragmentBuffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fragmentUniformsBuffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;offset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And now, if we run the app, we get a triangle that looks identical to before. It is the same as before since we used an initial brightness of &lt;code&gt;1.0&lt;/code&gt;. We can quickly test that &lt;code&gt;brightness&lt;/code&gt; actually works by manually changing the initial value from &lt;code&gt;1.0&lt;/code&gt; to &lt;code&gt;0.5&lt;/code&gt;, which should give a darker looking triangle:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/post_assets/metal/metal-intro-2/screen2.png&quot; alt=&quot;Darker Triangle&quot;&gt;&lt;/p&gt;

&lt;p&gt;Once you are done experiment with it, set the initial value back to &lt;code&gt;1.0&lt;/code&gt;.&lt;/p&gt;
&lt;h1 id=&quot;animating-the-brightness&quot;&gt;Animating the brightness&lt;/h1&gt;&lt;h2 id=&quot;keeping-track-of-time&quot;&gt;Keeping track of time&lt;/h2&gt;
&lt;p&gt;Right now we have uniform data being initialized on the CPU, and passed to the GPU. But what we really want to do is animate the brightness of the triangle, which means we want to modify the brightness uniform on each frame. The first thing we need to do is start keeping track of time, so that we can base the brightness off of the current time. Our strategy is:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Keep track of our app&amp;#39;s time with a variable &lt;code&gt;currentTime&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;On each frame compute the difference in time between the current frame and the previous frame. To do so we will have to store the system time of the previous frame in a variable &lt;code&gt;lastRenderTime&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Use the time difference to update &lt;code&gt;currentTime&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We start by adding these instance variables to the &lt;code&gt;Renderer&lt;/code&gt; class:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight swift&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// This keeps track of the system time of the last render&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;lastRenderTime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;CFTimeInterval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;?&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;nil&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;// This is the current time in our app, starting at 0, in units of seconds&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;currentTime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Double&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now at the top of &lt;code&gt;draw(in view: MTKView)&lt;/code&gt; we compute the time difference, save the system time to &lt;code&gt;lastRenderTime&lt;/code&gt;, and then use the time difference to update the state:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight swift&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// Compute dt&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;systemTime&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;CACurrentMediaTime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;timeDifference&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lastRenderTime&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;?&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;systemTime&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lastRenderTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;// Save this system time&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;lastRenderTime&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;systemTime&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// Update state&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;update&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;timeDifference&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// TODO: We need to implement this function!&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&quot;updating-state&quot;&gt;Updating state&lt;/h2&gt;
&lt;p&gt;Now we just need to implement this &lt;code&gt;update&lt;/code&gt; function to update the &lt;code&gt;currentTime&lt;/code&gt;, and change the brightness:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight swift&quot;&gt;&lt;code&gt;&lt;span class=&quot;kd&quot;&gt;func&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;update&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;CFTimeInterval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;ptr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fragmentUniformsBuffer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;contents&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;bindMemory&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;FragmentUniforms&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;capacity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ptr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pointee&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;brightness&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;cos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;currentTime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;currentTime&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;First, we access the memory location of the uniform buffer via &lt;code&gt;.contents()&lt;/code&gt;, and then tell Swift to interpret the memory location as a pointer to a &lt;code&gt;FragmentUniforms&lt;/code&gt; struct (which it is). Then, we write to the &lt;code&gt;brightness&lt;/code&gt; field, setting the brightness to be a cosine wave based on the current time. Using a cosine wave makes the brightness start at 1, and then smoothly animate between 1 and 0:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/post_assets/metal/metal-intro-2/cosine.png&quot; alt=&quot;cosine wave&quot;&gt;&lt;/p&gt;

&lt;p&gt;Finally, we update the &lt;code&gt;currentTime&lt;/code&gt; by adding the time difference. If you run this code now, you should see exactly what we were hoping for, a triangle with smoothly animating brightness:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/post_assets/metal/metal-intro-2/screen1.gif&quot; alt=&quot;animating triangle&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;synchronizing-the-cpu-and-gpu&quot;&gt;Synchronizing the CPU and GPU&lt;/h2&gt;
&lt;p&gt;Unfortunately, the code we currently have is subtly bugged. The output looks fine, but there is actually a race condition that may or may not manifest itself in the future. It&amp;#39;s actually an easy fix, but we need to understand what is happening in terms of CPU and GPU coordination. &lt;/p&gt;

&lt;p&gt;Each time &lt;code&gt;draw(in view: MTKView)&lt;/code&gt; is called (60 times per second), we use the CPU to update the uniform data, and then tell the GPU to go and render using that uniform data &lt;em&gt;asynchronously&lt;/em&gt;. Let&amp;#39;s lay the rendering of the first 3 frames in a timeline:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/post_assets/metal/metal-intro-2/timeline1.png&quot; alt=&quot;timeline1&quot;&gt;&lt;/p&gt;

&lt;p&gt;This is fine, and there are not really any problems with this. But what happens if our rendering starts to take too long? Something like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/post_assets/metal/metal-intro-2/timeline2.png&quot; alt=&quot;timeline2&quot;&gt;&lt;/p&gt;

&lt;p&gt;In the red highlighted areas, there is a big problem. The GPU is reading from the uniforms buffer while the CPU is simultaneously writing to it (remember, they both share the same physical memory). What exactly will happen here is anyone&amp;#39;s guess: it could crash (though this would be far too forgiving), subtly introduce incorrectly rendered pixels, or have no visible effect. It&amp;#39;s hard to predict, and hard to track down later. Ignoring this bug that isn&amp;#39;t immediately affecting us is the embodiment of this meme:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/post_assets/metal/metal-intro-2/fine.jpg&quot; alt=&quot;dog in burning house&quot;&gt;&lt;/p&gt;

&lt;p&gt;(Image credit: KC Green, The Nib). The solution is to synchronize the CPU and GPU. What we want to ensure is that the writing to the uniforms buffer by the CPU can not overlap with the GPU&amp;#39;s rendering. We accomplish this by using a &lt;a href=&quot;https://en.wikipedia.org/wiki/Semaphore_(programming)&quot;&gt;semaphore&lt;/a&gt;. Let&amp;#39;s code it really fast, and then explain how the code works. Add this instance variable to the &lt;code&gt;Renderer&lt;/code&gt; class:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight swift&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;gpuLock&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;DispatchSemaphore&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This creates the semaphore we will use. Then at the very top of &lt;code&gt;draw(in view: MTKView)&lt;/code&gt; add:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight swift&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;gpuLock&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;wait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Finally, just before &lt;code&gt;commandBuffer.commit()&lt;/code&gt;, add:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight swift&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;commandBuffer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;addCompletedHandler&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gpuLock&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;signal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;That&amp;#39;s all the code we need to add. Let&amp;#39;s walk through how this exactly works. We first initialize the semaphore with a value of &lt;code&gt;1&lt;/code&gt;. Then, when we call &lt;code&gt;wait&lt;/code&gt; one of two things happen:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;If the semaphore value is greater than 0, it is decremented by 1, and execution continues.&lt;/li&gt;
&lt;li&gt;If the semaphore value is 0, then the CPU waits until the value is greater than 0, after which it will decrement it and continue executing.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So when &lt;code&gt;draw(in view: MTKView)&lt;/code&gt; is called the first time, the value is 1, so &lt;code&gt;wait&lt;/code&gt; changes the value to 0 and moves on. We then update uniforms and encode the drawing. Before the GPU starts drawing (&lt;code&gt;commandBuffer.commit()&lt;/code&gt;), we add a completion handler that will call &lt;code&gt;signal&lt;/code&gt; when the GPU is done drawing. All &lt;code&gt;signal&lt;/code&gt; does is increment the semaphore&amp;#39;s value by 1.&lt;/p&gt;

&lt;p&gt;This means that the next time &lt;code&gt;draw(in view: MTKView)&lt;/code&gt; is called the CPU will wait to start modifying the uniforms until the previous rendering is complete, which ensures that we do not read and write to the uniforms simultaneously.&lt;/p&gt;

&lt;p&gt;The problem of enforcing synchronization between the CPU and GPU is not unique to uniform data: anytime that one of them is reading and the other one is writing (or they are both writing), then you must enforce synchronization. This is almost always the case with uniform data, but is not necessarily the case with vertex data. As always, the complete source code is &lt;a href=&quot;/public/post_assets/metal/metal-intro-2/MetalFinal.zip&quot;&gt;available for download&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&quot;challenges-1&quot;&gt;Challenges&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;We are currently computing the &lt;code&gt;brightness&lt;/code&gt; using &lt;code&gt;cos()&lt;/code&gt; on the CPU, and passing this to the GPU. Instead, try to pass the &lt;code&gt;currentTime&lt;/code&gt; to the GPU, and compute the &lt;code&gt;brightness&lt;/code&gt; on the GPU.&lt;/li&gt;
&lt;li&gt;I wrote this post as passing the &lt;code&gt;brightness&lt;/code&gt; uniform to the fragment shader. But we can accomplish the exact same thing by passing the &lt;code&gt;brightness&lt;/code&gt; uniform to the vertex shader instead, and in the vertex shader modify the vertex colors before interpolation. Implement it this way instead.&lt;/li&gt;
&lt;li&gt;In challenge #2 you achieved the exact same effect by modifying the vertex colors in the vertex shader instead of the fragment shader. Will this always be the case? As an example, try taking the &lt;code&gt;sqrt()&lt;/code&gt; of each RGB color component in the vertex shader and compare to doing it in the fragment shader. Are they identical?&lt;/li&gt;
&lt;li&gt;Use a time uniform to cause the triangle to grow and shrink. (Hint: what happens if you multiply the vertex positions by a number less than 1 or greater than 1?)&lt;/li&gt;
&lt;/ol&gt;
</content>
 </entry>
 
 <entry>
   <title>Metal 3D Graphics Part 1: Basic Rendering</title>
   <link href="https://donaldpinckney.com/metal/2018/07/05/metal-intro-1.html"/>
   <updated>2018-07-05T00:00:00-07:00</updated>
   <id>https://donaldpinckney.com/metal/2018/07/05/metal-intro-1</id>
   <content type="html">&lt;h1 id=&quot;what-is-metal-and-why-use-it&quot;&gt;What is Metal, and why use it?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://developer.apple.com/metal/&quot;&gt;Metal&lt;/a&gt; is a powerful new GPU programming API designed by Apple. Originally it was announced in 2014 for iOS and claimed significant performance benefits over &lt;a href=&quot;https://www.opengl.org&quot;&gt;OpenGL&lt;/a&gt;, the standard 3D graphics API, and since 2014 it has gained the ability for general purpose GPU computation, cross-platform support between iOS and macOS, and other features. But what exactly is it, and how does it compare to OpenGL?&lt;/p&gt;

&lt;p&gt;Both Metal and OpenGL are low-level APIs that provide programmable access to GPU hardware for 3D graphics. Both allow you to write code that will execute on the GPU to customize how 3D objects are rendered. However, OpenGL tends to hide the communication between the CPU and the GPU, whereas Metal requires you to explicitly program this communication.&lt;/p&gt;

&lt;p&gt;This gives us two advantages: first, it allows for greater efficiency in terms of CPU and GPU communication; and second it provides an excellent learning opportunity to understand how this low-level communication works. But don&amp;#39;t let this scare you: while this might sound horribly complicated, it&amp;#39;s actually quite elegant and enjoyable code to write.&lt;/p&gt;

&lt;p&gt;Finally, a quick note about what this tutorial is and what it isn&amp;#39;t: it is meant to give a brief introduction to the very basics of rendering with Metal, with little or no prior knowledge required about Swift or 3D graphics. Later tutorials will build on this by looking at programming interesting 3D effects in Metal. However, if your goal is to get started quickly with game development, learning Metal is probably not the fastest way to make a game; instead you should look at higher-level game engines, such as &lt;a href=&quot;https://unity3d.com&quot;&gt;Unity&lt;/a&gt; or others.&lt;/p&gt;

&lt;p&gt;Rendering a 2D multi-colored triangle is the hello world program for graphics, and is the goal for this post. We will develop specifically for macOS since it is most convenient, but the code will actually be cross-platform with iOS and tvOS. To visualize the end goal, a screenshot of it looks like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/post_assets/metal/metal-intro-1/screen1.png&quot; alt=&quot;End Goal Screenshot&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;rough-sketch-of-graphics-pipelines&quot;&gt;Rough Sketch of Graphics Pipelines&lt;/h1&gt;
&lt;p&gt;3D graphics are generally described by geometry, which is usually specified by the &lt;strong&gt;vertices&lt;/strong&gt; of triangles. These vertices are generated by code on the CPU, and then need to be sent over to the GPU for rendering. The vertex data will then be fed through a GPU pipeline, eventually resulting in a final image being rendered, which can then be displayed on screen:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/post_assets/metal/metal-intro-1/basic_pipeline.png&quot; alt=&quot;Basic GPU Pipeline&quot;&gt;&lt;/p&gt;

&lt;p&gt;In short, a &lt;strong&gt;pipeline&lt;/strong&gt; is a series of pre-configured steps that the GPU hardware takes to turn a bunch of vertex data into a final rendered image. Modern 3D graphics requires being able to program exactly what happens inside of the pipeline, and describing the pipeline via code is the central concept in Metal programming. Programming a simple pipeline that can render a single 2D triangle is the goal of this tutorial, and as we go through the code we will break this abstract pipeline into detailed individual components.&lt;/p&gt;
&lt;h1 id=&quot;basic-setup-and-clearing-the-screen&quot;&gt;Basic Setup and Clearing the Screen&lt;/h1&gt;&lt;h2 id=&quot;creating-a-one-window-macos-app&quot;&gt;Creating a one window macOS app&lt;/h2&gt;
&lt;p&gt;Open Xcode (make sure to install the latest version of Xcode from the Mac App Store), make a new Xcode project, choose &amp;quot;Cocoa App&amp;quot; under &amp;quot;macOS&amp;quot;, and hit next. Then, fill in whatever you want for the Product Name (I&amp;#39;ll use &lt;code&gt;MetalIntro1&lt;/code&gt;), choose Swift for the language, make sure Use Storyboards is checked, and Create Document-Based Application is &lt;em&gt;not&lt;/em&gt; checked. My settings look like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/post_assets/metal/metal-intro-1/xcode1.png&quot; alt=&quot;Project Settings&quot;&gt;&lt;/p&gt;

&lt;p&gt;Then, hit Next, and save it somewhere. If you run the app (&amp;#8984;R) then a single blank window should appear. This blank window is where we want to display our 3D graphics. Before we can write actual Metal code, we need a way for the 3D graphics to even appear in our window.&lt;/p&gt;
&lt;h2 id=&quot;setting-up-a-metalkit-view&quot;&gt;Setting Up a MetalKit View&lt;/h2&gt;
&lt;p&gt;Everything visual in &lt;code&gt;macOS&lt;/code&gt; is represented by a &lt;em&gt;view&lt;/em&gt;, which concretely is a subclass of &lt;code&gt;NSView&lt;/code&gt;. We need a view in our window in which we can display the results of the Metal graphics rendering. Apple provides a prebuilt view just for this purpose, &lt;code&gt;MTKView&lt;/code&gt;, which we will take advantage of.&lt;/p&gt;

&lt;p&gt;First, we need to add a &lt;code&gt;MTKView&lt;/code&gt; to the window by modifying the Storyboard file, and then we need to configure it via code. Open &lt;code&gt;Main.storyboard&lt;/code&gt;, select the root view of the view controller, and in the inspector panel change the class from &lt;code&gt;NSView&lt;/code&gt; to &lt;code&gt;MTKView&lt;/code&gt;, as shown below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/post_assets/metal/metal-intro-1/xcode2.png&quot; alt=&quot;Modifying the Storyboard&quot;&gt;&lt;/p&gt;

&lt;p&gt;Now to configure the &lt;code&gt;MTKView&lt;/code&gt; we need to write some initialization code in the view controller, so open &lt;code&gt;ViewController.swift&lt;/code&gt;. We will need access to the Metal framework, and the auxiliary MetalKit framework, so add these imports at the top:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight swift&quot;&gt;&lt;code&gt;&lt;span class=&quot;kd&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Metal&lt;/span&gt;
&lt;span class=&quot;kd&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;MetalKit&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;First we want to save the &lt;code&gt;MTKView&lt;/code&gt; in a convenient variable, so add the following instance variable to the view controller class:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight swift&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;mtkView&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;MTKView&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To initialize this variable in the &lt;code&gt;viewDidLoad&lt;/code&gt; function add this code to the &lt;code&gt;viewDidLoad&lt;/code&gt; function, after the &lt;code&gt;super.viewDidLoad()&lt;/code&gt; call:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight swift&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;guard&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;mtkViewTemp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as?&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;MTKView&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;View attached to ViewController is not an MTKView!&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;mtkView&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mtkViewTemp&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now we can configure &lt;code&gt;mtkView&lt;/code&gt;. The two necessary properties to configure are the &lt;code&gt;device&lt;/code&gt; and the &lt;code&gt;delegate&lt;/code&gt;. The device is easiest, so we will do it first. The &lt;code&gt;device&lt;/code&gt; (of type &lt;code&gt;MTLDevice&lt;/code&gt;) represents the actual GPU hardware. We can retrieve the default GPU and save it to the &lt;code&gt;mtkView&lt;/code&gt; like so:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight swift&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;guard&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;defaultDevice&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;MTLCreateSystemDefaultDevice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Metal is not supported on this device&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;My GPU is: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;defaultDevice&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;mtkView&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;defaultDevice&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you run the app now, it should print in the Xcode console what your GPU is, but nothing will be rendered inside the window, since we haven’t told Metal to do any rendering. To set this up we need to configure the &lt;code&gt;delegate&lt;/code&gt; property of the &lt;code&gt;mtkView&lt;/code&gt;. The &lt;code&gt;delegate&lt;/code&gt;is an independent object which is responsible for performing our custom rendering, whenever the &lt;code&gt;mtkView&lt;/code&gt; asks it to. The great thing about this approach is that it is easy to write platform independent code: our view controller code is specific to macOS, but it is very short; the meat of the code will live inside the object we assign to the &lt;code&gt;delegate&lt;/code&gt;property.&lt;/p&gt;

&lt;p&gt;So we need to make a new &lt;code&gt;class&lt;/code&gt; who’s responsibility is to render our custom graphics whenever &lt;code&gt;mtkView&lt;/code&gt; asks it to. To do so it just needs to implement the &lt;code&gt;MTKViewDelegate&lt;/code&gt; protocol. Let’s make a new Swift file called &lt;code&gt;Renderer.swift&lt;/code&gt;, and add the code below to declare a new &lt;code&gt;Renderer&lt;/code&gt; class (and import the needed frameworks):&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight swift&quot;&gt;&lt;code&gt;&lt;span class=&quot;kd&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Metal&lt;/span&gt;
&lt;span class=&quot;kd&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;MetalKit&lt;/span&gt;

&lt;span class=&quot;kd&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Renderer&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;NSObject&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;MTKViewDelegate&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;// This is the initializer for the Renderer class.&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// We will need access to the mtkView later, so we add it as a parameter here.&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;?(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;mtkView&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;MTKView&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;

    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;// mtkView will automatically call this function&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// whenever it wants new content to be rendered.&lt;/span&gt;
    &lt;span class=&quot;kd&quot;&gt;func&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;draw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;MTKView&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;

    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;// mtkView will automatically call this function&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// whenever the size of the view changes (such as resizing the window).&lt;/span&gt;
    &lt;span class=&quot;kd&quot;&gt;func&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;mtkView&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;MTKView&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;drawableSizeWillChange&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;CGSize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;

    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This class includes an initializer (with an &lt;code&gt;MTKView&lt;/code&gt; as a parameter), and the two required functions for implementing the &lt;code&gt;MTKViewDelegate&lt;/code&gt; protocol. We will come back to filling in the &lt;code&gt;draw(in view: MTKView)&lt;/code&gt; function later, but first we will finish writing all of the code for &lt;code&gt;ViewController.swift&lt;/code&gt;. &lt;/p&gt;

&lt;p&gt;Now that we have this skeleton class, we can create an instance in the &lt;code&gt;viewDidLoad&lt;/code&gt; function and configure &lt;code&gt;mtkView&lt;/code&gt; with it. First, we add an instance variable for it to the &lt;code&gt;ViewController&lt;/code&gt; class:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight swift&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;renderer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Renderer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;At the end of the &lt;code&gt;viewDidLoad&lt;/code&gt; function we create an instance of the &lt;code&gt;Renderer&lt;/code&gt; class and configure the &lt;code&gt;delegate&lt;/code&gt; property:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight swift&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;guard&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;tempRenderer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Renderer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;mtkView&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mtkView&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Renderer failed to initialize&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;renderer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tempRenderer&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;mtkView&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;delegate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;renderer&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;At this point we are completely done with the setup code, and now we can move on to actual Metal code. For reference, here is my final version of &lt;code&gt;ViewController.swift&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight swift&quot;&gt;&lt;code&gt;&lt;span class=&quot;kd&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Cocoa&lt;/span&gt;
&lt;span class=&quot;kd&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Metal&lt;/span&gt;
&lt;span class=&quot;kd&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;MetalKit&lt;/span&gt;

&lt;span class=&quot;kd&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;ViewController&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;NSViewController&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;mtkView&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;MTKView&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;renderer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Renderer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;override&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;func&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;viewDidLoad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;viewDidLoad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;// First we save the MTKView to a convenient instance variable&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;guard&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;mtkViewTemp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as?&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;MTKView&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;View attached to ViewController is not an MTKView!&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;mtkView&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mtkViewTemp&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;// Then we create the default device, and configure mtkView with it&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;guard&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;defaultDevice&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;MTLCreateSystemDefaultDevice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Metal is not supported on this device&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

        &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;My GPU is: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;defaultDevice&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;mtkView&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;defaultDevice&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;// Lastly we create an instance of our Renderer object, &lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;// and set it as the delegate of mtkView&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;guard&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;tempRenderer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Renderer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;mtkView&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mtkView&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Renderer failed to initialize&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;renderer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tempRenderer&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;mtkView&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;delegate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;renderer&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&quot;clearing-the-screen-by-issuing-gpu-commands&quot;&gt;Clearing the Screen by Issuing GPU Commands&lt;/h2&gt;
&lt;p&gt;We have everything setup for us to start writing graphics code in &lt;code&gt;Renderer.swift&lt;/code&gt;. Before we draw a triangle, we will start with clearing the screen, which will involve issuing commands to GPU, and transferring the results back to the &lt;code&gt;mtkView&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Metal requires us to keep track of a queue (essentially a list) of commands that are waiting to be executed on the GPU. A Metal command queue is represented by the class &lt;code&gt;MTLCommandQueue&lt;/code&gt;, and commands are represented by the class &lt;code&gt;MTLCommandBuffer&lt;/code&gt;. This is an outline of what we need to do to have a full render pass working:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;At initialization time, create one &lt;code&gt;MTLCommandQueue&lt;/code&gt; (call it &lt;code&gt;commandQueue&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;At each draw cycle, create a &lt;code&gt;MTLCommandBuffer&lt;/code&gt;, configure it to include the draw commands we want, and then add it to &lt;code&gt;commandQueue&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Once the &lt;code&gt;MTLCommandBuffer&lt;/code&gt; finishes executing on the GPU, we need to display the results in the &lt;code&gt;mtkView&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let&amp;#39;s start with the first task. We need to keep track of one &lt;code&gt;MTLCommandQueue&lt;/code&gt; for the whole &lt;code&gt;Renderer&lt;/code&gt; class. For convenience we also want to keep track of the &lt;code&gt;MTLDevice&lt;/code&gt;, since we will use it a lot later. So, add the following instance variables to the &lt;code&gt;Renderer&lt;/code&gt; class:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight swift&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;MTLDevice&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;commandQueue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;MTLCommandQueue&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, inside the initializer (the &lt;code&gt;init?&lt;/code&gt; function) add the following code to setup these variables:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight swift&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mtkView&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;commandQueue&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;makeCommandQueue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that we have to use the &lt;code&gt;device&lt;/code&gt; to create a new command queue: this means that a command queue is associated with a specific device, and can only be used with that device.&lt;/p&gt;

&lt;p&gt;We have now initialized a command queue, so now in the &lt;code&gt;draw(in view: MTKView)&lt;/code&gt; we can create a &lt;code&gt;MTLCommandBuffer&lt;/code&gt;, configure it, and add it to the queue. In the &lt;code&gt;draw(in view: MTKView)&lt;/code&gt; function the first thing we do is create a new &lt;code&gt;MTLCommandBuffer&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight swift&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// Get an available command buffer&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;guard&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;commandBuffer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;commandQueue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;makeCommandBuffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, we need to configure &lt;code&gt;commandBuffer&lt;/code&gt; to perform drawing commands that we want. First, we use a &lt;code&gt;MTLRenderPassDescriptor&lt;/code&gt; to configure some options about input and output. &lt;em&gt;When that is finalized&lt;/em&gt;, we then use a &lt;code&gt;MTLRenderCommandEncoder&lt;/code&gt; to configure what drawing operations the GPU will perform.&lt;/p&gt;

&lt;p&gt;There are a lot of options to configure for the &lt;code&gt;MTLRenderPassDescriptor&lt;/code&gt;. Fortunately, the &lt;code&gt;MTKView&lt;/code&gt; provides us with a pre-configured &lt;code&gt;MTLRenderPassDescriptor&lt;/code&gt;, so we can just grab that and then change the default options:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight swift&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// Get the default MTLRenderPassDescriptor from the MTKView argument&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;guard&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;renderPassDescriptor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;currentRenderPassDescriptor&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// Change default settings. For example, we change the clear color from black to red.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;renderPassDescriptor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;colorAttachments&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clearColor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;MTLClearColorMake&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;More configure options on a &lt;code&gt;MTLRenderPassDescriptor&lt;/code&gt; will be discussed in future posts, but for now the most important option is the &lt;code&gt;colorAttachments&lt;/code&gt; array. The first (and for now only) item in the &lt;code&gt;colorAttachments&lt;/code&gt; array describes the output destination of the rendering. In this case, setting the &lt;code&gt;clearColor&lt;/code&gt; property to &lt;code&gt;(1, 0, 0, 1)&lt;/code&gt; (these are RGBA values) tells Metal to clear the color attachment to a value of &lt;code&gt;(1, 0, 0, 1)&lt;/code&gt; before rendering. Other notable properties that the &lt;code&gt;MTKView&lt;/code&gt; set for us include &lt;code&gt;renderTargetWidth&lt;/code&gt; and &lt;code&gt;renderTargetHeight&lt;/code&gt;, which were automatically set to the size of the &lt;code&gt;MTKView&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;This is all that we need to do to configure &lt;code&gt;renderPassDescriptor&lt;/code&gt;. We now finalize it by converting it into a &lt;code&gt;MTLRenderCommandEncoder&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight swift&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// We compile renderPassDescriptor to a MTLRenderCommandEncoder.&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;guard&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;renderEncoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;commandBuffer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;makeRenderCommandEncoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;descriptor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;renderPassDescriptor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;At this point we would use &lt;code&gt;renderEncoder&lt;/code&gt; to encode various drawing commands to tell the GPU to draw triangles based on vertex data. For now we just want to clear the screen, so we don&amp;#39;t need to encode any drawing commands.&lt;/p&gt;

&lt;p&gt;Since we are done encoding our drawing commands (none of them), we finish the encoding process:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight swift&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// This finalizes the encoding of drawing commands.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;renderEncoder&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;endEncoding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;At this point, &lt;code&gt;commandBuffer&lt;/code&gt; as been configured via &lt;code&gt;renderPassDescriptor&lt;/code&gt; and &lt;code&gt;renderEncoder&lt;/code&gt; to describe a bunch of GPU commands for an entire render pass. The &lt;code&gt;commandBuffer&lt;/code&gt; includes rendering output information, such as output width and height, the clear color, and other properties, as well as a (currently empty) list of encoded drawing commands. However, &lt;em&gt;calling &lt;code&gt;endEncoding()&lt;/code&gt; does NOT send this information to the GPU yet!&lt;/em&gt; This gives us control about when to actually trigger expensive drawing commands on the GPU, vs. just finish encoding them.&lt;/p&gt;

&lt;p&gt;We are now ready to send the encoded commands to the GPU. However, it is crucial to understand that the CPU and GPU work asynchronously: so when we send the encoded commands to the GPU, it will work on completing them while the CPU code continues to run, and then finish the commands and obtain the color image result at some indeterminate point in the future. But, we need a way to know when the rendering finishes and at that time place the result into the &lt;code&gt;MTKView&lt;/code&gt;. If we don&amp;#39;t do this, the GPU will finish rendering, and then the result will simply disappear, since the CPU doesn&amp;#39;t even know about the result.&lt;/p&gt;

&lt;p&gt;Setting up the callback to place it into the &lt;code&gt;MTKView&lt;/code&gt; is very easy, since &lt;code&gt;MTKView&lt;/code&gt; actually provides most of the implementation for us. We just need to tell the &lt;code&gt;commandBuffer&lt;/code&gt; to present into the &lt;code&gt;MTKView&lt;/code&gt;&amp;#39;s drawable:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight swift&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// Tell Metal to send the rendering result to the MTKView when rendering completes&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;commandBuffer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;present&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;currentDrawable&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;A drawable is simply a resource managed by the &lt;code&gt;MTKView&lt;/code&gt; which Metal can write the result into.&lt;/p&gt;

&lt;p&gt;Now, we are finally ready to send the encoded command buffer to the GPU. This is just one of line code:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight swift&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// Finally, send the encoded command buffer to the GPU.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;commandBuffer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;commit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that &lt;code&gt;commandBuffer.commit()&lt;/code&gt; is a bit more nuanced than just sending the command buffer to the GPU. Since &lt;code&gt;commandBuffer&lt;/code&gt; is actually stored in &lt;code&gt;commandQueue&lt;/code&gt;, writing &lt;code&gt;commandBuffer.commit()&lt;/code&gt; will prepare it for execution on the GPU, at the back of the queue. So other command buffers that are ahead of it in line (that were committed first) will execute first.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Running the code now should look like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/post_assets/metal/metal-intro-1/screen2.png&quot; alt=&quot;Clear Red Screen&quot;&gt;&lt;/p&gt;

&lt;p&gt;And for reference the current code in &lt;code&gt;Renderer.swift&lt;/code&gt; is:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight swift&quot;&gt;&lt;code&gt;&lt;span class=&quot;kd&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Foundation&lt;/span&gt;
&lt;span class=&quot;kd&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Metal&lt;/span&gt;
&lt;span class=&quot;kd&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;MetalKit&lt;/span&gt;

&lt;span class=&quot;kd&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Renderer&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;NSObject&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;MTKViewDelegate&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;MTLDevice&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;commandQueue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;MTLCommandQueue&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;// This is the initializer for the Renderer class.&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// We will need access to the mtkView later, so we add it as a parameter here.&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;?(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;mtkView&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;MTKView&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mtkView&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;commandQueue&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;makeCommandQueue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;// mtkView will automatically call this function&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// whenever it wants new content to be rendered.&lt;/span&gt;
    &lt;span class=&quot;kd&quot;&gt;func&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;draw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;MTKView&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;// Get an available command buffer&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;guard&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;commandBuffer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;commandQueue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;makeCommandBuffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;// Get the default MTLRenderPassDescriptor from the MTKView argument&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;guard&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;renderPassDescriptor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;currentRenderPassDescriptor&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;// Change default settings. For example, we change the clear color from black to red.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;renderPassDescriptor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;colorAttachments&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clearColor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;MTLClearColorMake&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;// We compile renderPassDescriptor to a MTLRenderCommandEncoder.&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;guard&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;renderEncoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;commandBuffer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;makeRenderCommandEncoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;descriptor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;renderPassDescriptor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;// TODO: Here is where we need to encode drawing commands!&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;// This finalizes the encoding of drawing commands.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;renderEncoder&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;endEncoding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;// Tell Metal to send the rendering result to the MTKView when rendering completes&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;commandBuffer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;present&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;currentDrawable&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;// Finally, send the encoded command buffer to the GPU.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;commandBuffer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;commit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;// mtkView will automatically call this function&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// whenever the size of the view changes (such as resizing the window).&lt;/span&gt;
    &lt;span class=&quot;kd&quot;&gt;func&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;mtkView&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;MTKView&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;drawableSizeWillChange&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;CGSize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;

    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&quot;hello-triangle&quot;&gt;Hello, Triangle!&lt;/h1&gt;&lt;h2 id=&quot;a-review-of-the-metal-architecture&quot;&gt;A Review of the Metal Architecture&lt;/h2&gt;
&lt;p&gt;With the code to clear the screen, we have most of the infrastructure that we need for performing rendering passes. Not all, but most. Since there are a lot of different pieces to the Metal API, let&amp;#39;s briefly organize these into a mental picture. The GPU executes pipelines which transform vertex data and encoded drawing commands into a final image result. The different Metal classes fit in as so:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A &lt;code&gt;MTLCommandBuffer&lt;/code&gt; represents the entire set of information the GPU needs to execute this pipeline: it contains the pipeline info itself, as well as vertex data and drawing commands that will be fed into the pipeline by the GPU. &lt;/li&gt;
&lt;li&gt;A &lt;code&gt;MTLRenderPassDescriptor&lt;/code&gt; is used to configure the interface of the pipeline, but not the interior of the pipeline. It is like the 2 openings of the pipe.&lt;/li&gt;
&lt;li&gt;A &lt;code&gt;MTLRenderCommandEncoder&lt;/code&gt; is used to prepare the vertex data and drawing commands that will be fed into the pipeline (we will see code for this in the next section).&lt;/li&gt;
&lt;li&gt;A &lt;code&gt;MTLCommandQueue&lt;/code&gt; keeps track of many &lt;code&gt;MTLCommandBuffer&lt;/code&gt;s that are waiting in line to be executed.&lt;/li&gt;
&lt;li&gt;A &lt;code&gt;MTLDevice&lt;/code&gt; represents the actual GPU.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Sketched as picture, the interactions of these pieces look like:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/post_assets/metal/metal-intro-1/clear_sketch.png&quot; alt=&quot;Metal Sketch&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;details-of-the-pipeline&quot;&gt;Details of the Pipeline&lt;/h2&gt;
&lt;p&gt;In the above diagram, we have written code to setup or configure pretty much all parts of it. The two pieces we have avoided so far are encoding drawing commands / vertex data, and configuring the pipeline itself. We have configured the &lt;code&gt;MTLRenderPassDescriptor&lt;/code&gt;, which is the openings to the pipeline, how it connects to the rest, but we have not configured the internals of the pipeline.&lt;/p&gt;

&lt;p&gt;Configuring the pipeline itself is really the whole point of graphics programming: this is the code which will render vertex data with any given effect we can code. And in Metal configuring a custom pipeline is &lt;em&gt;necessary&lt;/em&gt; for being able to render vertices: otherwise the GPU has no way of knowing how to transform encoded drawing commands into a final image.&lt;/p&gt;

&lt;p&gt;There are many parts to the interior of the pipeline, some of which we can write fully custom code for, and others are a fixed function provided by the GPU. Here is a diagram of the main parts of the pipeline:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/post_assets/metal/metal-intro-1/pipeline.png&quot; alt=&quot;A sketch of a graphics pipeline&quot;&gt;&lt;/p&gt;

&lt;p&gt;The stages marked in green are the ones that we can write fully custom code for, while the others are done mostly automatically by the GPU. Also, note that the first stage, the custom encoding of drawing commands is done on the CPU, and all other steps are done on the GPU.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Encoding Drawing Commands / Vertex Data: The data that the GPU receives, and that must be processed in the pipeline.&lt;/li&gt;
&lt;li&gt;Vertex Shader: Converts the 3D vertex locations into 2D screen coordinates. It also passes vertex data down the pipeline.&lt;/li&gt;
&lt;li&gt;Tessellation: Subdivides triangles into further triangles to provide higher-quality results.&lt;/li&gt;
&lt;li&gt;Rasterization: Discretizes the 2D geometric data into 2D discrete pixels. This will also take data attached to each vertex and interpolate it over the whole shape to every rasterized pixel.&lt;/li&gt;
&lt;li&gt;Fragment Shader: Given the interpolated pixel data from the rasterizer, the fragment shader determines the final color of each pixel.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Tessellation is an advanced technique that will be looked at much later, but the drawing command encoding, vertex shader, and fragment shader are necessary parts for the pipeline, and we will work on setting them up now.&lt;/p&gt;
&lt;h2 id=&quot;creating-vertex-and-fragment-shaders&quot;&gt;Creating Vertex and Fragment Shaders&lt;/h2&gt;
&lt;p&gt;Vertex and fragment shaders are written in a special programming language, the &lt;a href=&quot;https://developer.apple.com/metal/Metal-Shading-Language-Specification.pdf&quot;&gt;Metal Shading Language&lt;/a&gt; (MSL). It is based on C++, with some extra features for describing properties of vertex and fragment shaders.&lt;/p&gt;

&lt;p&gt;We want our MSL code (running on the GPU) to share some &lt;code&gt;struct&lt;/code&gt; definitions with our Swift code (running on the CPU), since both sides need to know how vertex data will be communicated to the GPU. We will write a C &lt;code&gt;struct&lt;/code&gt;, which can be used by the MSL code since it is based on C++, and by the Swift code, since Swift can import C type definitions. To do so we need to setup a bridging header, so Swift knows which C header files to import.&lt;/p&gt;

&lt;p&gt;Probably the easiest (though pretty annoying) way to setup a bridging header is to create a new Objective-C &lt;em&gt;(NOT C)&lt;/em&gt; file in Xcode (you can call it anything), and it should prompt you to create a bridging header. Click &amp;quot;Create Bridging Header&amp;quot;, and then you can immediately delete the Objective-C file you made.&lt;/p&gt;

&lt;p&gt;We can now make a C header file which will be imported by both the MSL code and the Swift code. Create a new file, and choose &amp;quot;Header File&amp;quot;. Name it &lt;code&gt;ShaderDefinitions.h&lt;/code&gt;, and create it. Then, in the bridging header add the following:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight c&quot;&gt;&lt;code&gt;&lt;span class=&quot;cp&quot;&gt;#include &quot;ShaderDefinitions.h&quot;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We are now ready to define a &lt;code&gt;struct&lt;/code&gt; which will determine what data constitutes a vertex. Since our goal is to have a 2D colored triangle, we need at a minimum to describe the position and color of each vertex. The triangle is 2D, so the positions can be described with 2 dimensions, and the colors can be described with 4 dimensions (RGBA). In vertex &lt;code&gt;struct&lt;/code&gt;s we will typically want the &lt;code&gt;struct&lt;/code&gt; members to be vectors, in this case a vector of length 2 for the position, and a vector of length 4 for the color. To conveniently use vector types we use the &lt;code&gt;simd.h&lt;/code&gt;library. In &lt;code&gt;ShaderDefinitions.h&lt;/code&gt; add the following include and &lt;code&gt;struct&lt;/code&gt; definition:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight c&quot;&gt;&lt;code&gt;&lt;span class=&quot;cp&quot;&gt;#include &amp;lt;simd/simd.h&amp;gt;
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Vertex&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vector_float4&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vector_float2&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We have the data structure for vertex data ready, so now we can make our shaders. Create a new file, and this time choose &amp;quot;Metal file&amp;quot;, and name it &lt;code&gt;Shaders.metal&lt;/code&gt;. Now, the vertex shader is a function, which converts input vertex data into final locations of vertices on the screen. It is declared like any C / C++ function, but with a special keyword so Metal knows that it is a vertex shader. Similarly, a fragment shader is a function that converts interpolated data into a final pixel color, and also has a special keyword. Add the following to &lt;code&gt;Shaders.metal&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight cpp&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// TODO: We need to change the parameters and return types of the shaders.
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;vertex&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;vertexShader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;fragment&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fragmentShader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We will come back to fixing the types of the functions shortly, but first we will load the shaders and setup vertex data on the CPU.&lt;/p&gt;
&lt;h2 id=&quot;setting-up-a-pipeline&quot;&gt;Setting up a Pipeline&lt;/h2&gt;
&lt;p&gt;To use our shaders we need to configure our own custom pipeline in &lt;code&gt;Renderer.swift&lt;/code&gt;. To do so we configure a &lt;code&gt;MTLRenderPipelineDescriptor&lt;/code&gt;, and then compile it to a finalized &lt;code&gt;MTLRenderPipelineState&lt;/code&gt;, all at initialization time.&lt;/p&gt;

&lt;p&gt;We will do this in a new function. Add this new class function stub to &lt;code&gt;Renderer&lt;/code&gt; (we will see why we need the parameters soon):&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight swift&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// Create our custom rendering pipeline, which loads shaders using `device`, and outputs to the format of `metalKitView`&lt;/span&gt;
&lt;span class=&quot;kd&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;func&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;buildRenderPipelineWith&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;MTLDevice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;metalKitView&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;MTKView&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;throws&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;MTLRenderPipelineState&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// ...&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, inside this function we need to construct a &lt;code&gt;MTLRenderPipelineDescriptor&lt;/code&gt;, and then configure it. We start by constructing one:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight swift&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// Create a new pipeline descriptor&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipelineDescriptor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;MTLRenderPipelineDescriptor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, there are only 3 properties we need to configure: the vertex shader, the fragment shader, and the format that pixel data is produced as. To load the shader code in &lt;code&gt;Shaders.metal&lt;/code&gt;, we use the &amp;quot;default library&amp;quot;: a collection of all the compiled Metal shader files in the app. We can then access the vertex and fragment shader functions by name:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight swift&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// Setup the shaders in the pipeline&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;library&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;makeDefaultLibrary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pipelineDescriptor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vertexFunction&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;?&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;makeFunction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;vertexShader&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pipelineDescriptor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fragmentFunction&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;?&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;makeFunction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;fragmentShader&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The device is required to make the default library (and is thus a parameter for the function), since when this code is rune the Metal code must be compiled into final machine code that is specific to the device. &lt;/p&gt;

&lt;p&gt;We also need to tell the pipeline in what format to store the pixel data. Options include how many bytes per pixel, and in what order to store red, green, blue, and alpha. But we just need the pipeline&amp;#39;s output format to match the format of the &lt;code&gt;MTKView&lt;/code&gt;, which is why an &lt;code&gt;MTKView&lt;/code&gt; is a parameter. We setup this configuration with one line:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight swift&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// Setup the output pixel format to match the pixel format of the metal kit view&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pipelineDescriptor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;colorAttachments&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pixelFormat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metalKitView&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;colorPixelFormat&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Lastly, we have to compile the pipeline descriptor to a final pipeline, ready to be executed on the GPU. We return the result, and in case of an error we throw the error:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight swift&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// Compile the configured pipeline descriptor to a pipeline state object&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;makeRenderPipelineState&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;descriptor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipelineDescriptor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now we just need to use our &lt;code&gt;buildRenderPipelineWith(device: MTLDevice, metalKitView: MTKView)&lt;/code&gt; function to save the pipeline into an instance variable. Add this instance variable to &lt;code&gt;Renderer&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight swift&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipelineState&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;MTLRenderPipelineState&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And in the initializer add the following:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight swift&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// Create the Render Pipeline&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pipelineState&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Renderer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;buildRenderPipelineWith&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;metalKitView&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mtkView&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;catch&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Unable to compile render pipeline state: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;nil&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&quot;sending-vertex-data-and-drawing-commands-to-the-gpu&quot;&gt;Sending Vertex Data and Drawing Commands to the GPU&lt;/h2&gt;
&lt;p&gt;Now that we have a configured pipeline, we had better use it. That means we need to send vertex data to the GPU, and drawing commands telling it what to do with that data. First, let&amp;#39;s discuss what exactly vertex data is, and then we will implement it. After that, encoding drawing commands is easy.&lt;/p&gt;

&lt;p&gt;As described above, vertex data just stores information about each vertex. In the vertex data itself we do not specify what types of shapes (points, lines, or triangles) the vertices describe, that is done later in the draw call. The information stored in vertex data is entirely up to us: we can include any vector data in it, in any format that is convenient for us. We already decided this format above: a &lt;code&gt;vector_float4&lt;/code&gt; for color (RGBA), and a &lt;code&gt;vector_float2&lt;/code&gt; for XY screen coordinates. However, how should we scale the X coordinate? It could be a real screen coordinate, in the range of 0 to &lt;code&gt;width&lt;/code&gt;, or it could be a normalized screen coordinate, in the range of -1 (left) to 1 (right). Likewise for the Y coordinate. Again, it is entirely up to us. As we will see, it is the job of the vertex shader to translate our arbitrary vertex data to consistent position data for the GPU to understand. For simplicity we will use normalized screen coordinates. First, we create an array of our desired vertex data in the initializer of &lt;code&gt;Renderer&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight swift&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// Create our vertex data&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;vertices&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;Vertex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt;
            &lt;span class=&quot;kt&quot;&gt;Vertex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt;
            &lt;span class=&quot;kt&quot;&gt;Vertex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you are unsure how this vertex data relates to our final goal of drawing a triangle with red, green, and blue vertices, then I suggest getting our a piece of paper and sketching the positions of these vertices, and their colors.&lt;/p&gt;

&lt;p&gt;This vertex data is correct, but it&amp;#39;s not accessible by the GPU, since it is stored in CPU accessible memory, not GPU accessible memory. To make it accessible by the GPU, we must use a &lt;code&gt;MTLBuffer&lt;/code&gt;, which provides access to CPU and GPU shared memory. Add an instance variable to &lt;code&gt;Renderer&lt;/code&gt; for this buffer:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight swift&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;vertexBuffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;MTLBuffer&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And in the initializer you can now create a buffer:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight swift&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// And copy it to a Metal buffer...&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;vertexBuffer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;makeBuffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;bytes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vertices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vertices&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;MemoryLayout&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;Vertex&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;options&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that the &lt;code&gt;makeBuffer&lt;/code&gt; function takes &lt;code&gt;length&lt;/code&gt; bytes stored at the given pointer in CPU memory, and copies the bytes directly into a newly allocated shared CPU / GPU buffer. In this case the buffer looks like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/post_assets/metal/metal-intro-1/buffer.png&quot; alt=&quot;Buffer&quot;&gt;&lt;/p&gt;

&lt;p&gt;Keep in mind that the GPU doesn&amp;#39;t automatically know anything about the structure, all it sees is the raw bytes in the bottom row of the above diagram.  The only way the GPU knows about the structure of the vertex data is because we will code our vertex shader accordingly (in a bit). As a side note, the padding bytes are added into the buffer for each vertex so that 16 bytes alignment is maintained. This has ups and downs, and can be avoided, which may be discussed in a future post.&lt;/p&gt;

&lt;p&gt;At this point we have both the pipeline and the vertex data prepared. The last code we need to write on the CPU side is to encode drawing commands for the GPU. Let&amp;#39;s return to the function &lt;code&gt;draw(in view: MTKView)&lt;/code&gt;, and look at the section between creating &lt;code&gt;renderEncoder&lt;/code&gt; and calling &lt;code&gt;renderEncoder.endEncoding()&lt;/code&gt;. Between these is where we want to place our code to encode drawing commands. First, we tell it what pipeline and what vertex data buffer to use:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight swift&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// Setup render commands to encode&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;// We tell it what render pipeline to use&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;renderEncoder&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;setRenderPipelineState&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pipelineState&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;// What vertex buffer data to use&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;renderEncoder&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;setVertexBuffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vertexBuffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;offset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We use &lt;code&gt;offset: 0&lt;/code&gt; since we want Metal to read the buffer starting at the beginning. Metal supports sending multiple vertex buffers in one render pass; we only have one vertex buffer so it must be at index 0, so we use &lt;code&gt;index: 0&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Lastly, we need to encode the actual drawing command to the GPU:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight swift&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// And what to draw&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;renderEncoder&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;drawPrimitives&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;triangle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;vertexStart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;vertexCount&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We tell Metal to draw a triangle (other options include &lt;code&gt;point&lt;/code&gt;, &lt;code&gt;line&lt;/code&gt;, &lt;code&gt;lineStrip&lt;/code&gt; and &lt;code&gt;triangleStrip&lt;/code&gt;, possibly explored in later posts), and to start drawing using the vertex at position 0 in the buffer. Clearly we have 3 vertices to draw for our one triangle. At this point, the code in &lt;code&gt;Renderer.swift&lt;/code&gt; is complete for this post!&lt;/p&gt;
&lt;h2 id=&quot;type-signatures-of-vertex-and-fragment-shaders&quot;&gt;Type Signatures of Vertex and Fragment Shaders&lt;/h2&gt;
&lt;p&gt;The only work we have left to do is in the vertex and fragment shaders: they are currently empty stubs! If we run the code right now, an error message like this is printed:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight plaintext&quot;&gt;&lt;code&gt;2018-07-03 16:12:37.685893-0700 MetalIntro1[13387:661616] Compiler failed to build request
Unable to compile render pipeline state: Error Domain=CompilerError Code=1 &quot;RasterizationEnabled is true but the vertex shader's return type is void&quot; UserInfo={NSLocalizedDescription=RasterizationEnabled is true but the vertex shader's return type is void}
Renderer failed to initialize
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is just saying that the type signatures of the vertex and fragment shader functions are not correct. What should they be instead? &lt;/p&gt;

&lt;p&gt;Well, we know that the job of the vertex shader is to pre-process per-vertex data, so its input must be some form of vertex data. In fact, the vertex shader will take the entire buffer (actually a pointer to it) and a vertex ID which indexes into this buffer as input. So when we perform our draw call with a buffer of 3 vertices, the vertex shader will be invoked once for each vertex for a total of 3 times, each time with the same buffer pointer as an argument, but with vertex indices of 0, 1, 2, respectively. Omitting the return type for now, in Metal shader code these 2 vertex shader function parameters look like:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight cpp&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;vertex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;???&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vertexShader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Vertex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vertexArray&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;buffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]],&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vid&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vertex_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Since we are using the &lt;code&gt;Vertex&lt;/code&gt; struct here, don&amp;#39;t forget to add &lt;code&gt;#include &amp;quot;ShaderDefinitions.h&amp;quot;&lt;/code&gt; to the top of the Metal shader file. Now, most of this seems clear enough: we have a constant pointer to our &lt;code&gt;MTLBuffer&lt;/code&gt; containing &lt;code&gt;Vertex&lt;/code&gt; structs, and an unsigned integer vertex ID. But there is some new syntax here to break down. &lt;/p&gt;

&lt;p&gt;First, &lt;code&gt;device&lt;/code&gt; indicates in what address space of the GPU should the &lt;code&gt;vertexArray&lt;/code&gt; be placed in. The two options are &lt;code&gt;device&lt;/code&gt; (read-write address space) and &lt;code&gt;constant&lt;/code&gt; (read-only address space). However, semantically and for efficiency one should use &lt;code&gt;device&lt;/code&gt; for data which will be accessed differently by each vertex (this is our case, we have a different vertex ID for each vertex) and use &lt;code&gt;constant&lt;/code&gt; for data which all vertices will use in the same way. Note that we don&amp;#39;t have to specify the address space of &lt;code&gt;vid&lt;/code&gt; since it is a simple type, not a pointer type.&lt;/p&gt;

&lt;p&gt;Second, Metal uses the &lt;code&gt;[[...]]&lt;/code&gt; syntax to specify Metal specific annotations. We said that we want one parameter to be the vertex buffer, which means that Metal has to call this function and pass the vertex buffer as the first argument. But Metal does not know by itself which parameter of the vertex shader function it should pass the vertex buffer to, so writing &lt;code&gt;[[buffer(0)]]&lt;/code&gt; is how we tell Metal that this specific parameter &lt;code&gt;vertexArray&lt;/code&gt; is where is should pass the first buffer. Note that the &lt;code&gt;0&lt;/code&gt; here corresponds directly to the &lt;code&gt;index: 0&lt;/code&gt; in the call &lt;code&gt;renderEncoder.setVertexBuffer(vertexBuffer, offset: 0, index: 0)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Similarly, Metal needs to pass the vertex ID to the vertex shader function, so writing &lt;code&gt;[[vertex_id]]&lt;/code&gt; on the second parameter tells Metal where to pass the vertex ID.&lt;/p&gt;

&lt;p&gt;Before we discuss the return type of the vertex shader, let&amp;#39;s decide the return type of the fragment shader, since it is easy. Recall that the fragment shader&amp;#39;s job is to receive interpolated data per-pixel from the rasterizer (which receives data from the vertex shader), and then output the final pixel color. At the point that Metal calls the fragment shader the GPU already knows the position of the pixel, just not the color. The only data the fragment shader needs to return is the color, and thus the return type can simply be &lt;code&gt;float4&lt;/code&gt;. As a side note, in the Metal shader language, &lt;code&gt;float4&lt;/code&gt; is just the same as &lt;code&gt;vector_float4&lt;/code&gt; in the &lt;code&gt;simd.h&lt;/code&gt; library.&lt;/p&gt;

&lt;p&gt;As for the return type (call it &lt;code&gt;T&lt;/code&gt;) of the vertex shader, the key realization is that the output of the vertex shader (&lt;code&gt;T&lt;/code&gt;) is fed to the rasterizer, which will interpolate this output over the appropriate destination pixels, and then call the fragment shader with this interpolated data (interpolated version of &lt;code&gt;T&lt;/code&gt;). So, the return type &lt;code&gt;T&lt;/code&gt; of the vertex shader must match the parameter of the fragment shader, plus some Metal specific interpolation magic. As for what &lt;code&gt;T&lt;/code&gt; is, it really can be arbitrary, &lt;em&gt;except that it must provide a screen-space position coordinate&lt;/em&gt;. Remember, the vertex shader must convert the positions in the vertex buffer to normalized screen-space positions. For now this is easy, since our vertex buffer already contains normalized screen-space positions. But we will still need to tell Metal where this normalized screen-space position is, using a &lt;code&gt;[[...]]&lt;/code&gt; annotation.&lt;/p&gt;

&lt;p&gt;For rendering our triangle, our vertex shader just needs to pass the color of the vertex into the rasterizer for interpolation, and then into the fragment shader. So we declare a new &lt;code&gt;struct&lt;/code&gt; type in &lt;code&gt;Shaders.metal&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight cpp&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;VertexOut&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;float4&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;float4&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pos&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;position&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]];&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The &lt;code&gt;float4 color&lt;/code&gt; is just the color data we want to pass through. There are two interesting things though about &lt;code&gt;pos&lt;/code&gt;. First, we use &lt;code&gt;[[position]]&lt;/code&gt; to tell the Metal rasterizer to use this field of the struct as the normalized screen-space position for performing the rasterizing / interpolation. Second, although I keep saying that we must use a normalized &lt;em&gt;screen-space&lt;/em&gt; (2D) coordinate for the position for rasterizing, &lt;code&gt;pos&lt;/code&gt; is actually 4 dimensional, and in fact it must be. In spirit &lt;code&gt;pos&lt;/code&gt; is 2 dimensional, but the 3rd coordinate can be used for depth: it doesn&amp;#39;t actually affect where the vertex is on screen, but can be used to track depth. The last coordinate is used to put &lt;code&gt;pos&lt;/code&gt; into 4D homogeneous space: a standard, useful, and unfortunately confusing way to store locations in 3D graphics. Fortunately, both the 3rd and 4th components we do not have to worry about in this post, we just need to keep them there to make Metal happy.&lt;/p&gt;

&lt;p&gt;We can now give final typed stubs of the vertex and fragment shader functions:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight cpp&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;VertexOut&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;float4&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;float4&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pos&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;position&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]];&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;vertex&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;VertexOut&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;vertexShader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Vertex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vertexArray&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;buffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]],&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vid&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vertex_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// TODO: Write vertex shader
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;fragment&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;float4&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fragmentShader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;VertexOut&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;interpolated&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stage_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// TODO: Write fragment shader
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;All the pieces of this have been explained, except for &lt;code&gt;[[stage_in]]&lt;/code&gt;. This is another Metal attribute, which tells Metal that parameter &lt;code&gt;interpolated&lt;/code&gt; should be fed the interpolated results of the rasterizer.&lt;/p&gt;
&lt;h2 id=&quot;writing-vertex-and-fragment-shader-code&quot;&gt;Writing Vertex and Fragment Shader Code&lt;/h2&gt;
&lt;p&gt;Finally, we get to write the actual implementations of the vertex and fragment shader, starting with the vertex shader.&lt;/p&gt;

&lt;p&gt;The first task in the vertex shader is to fetch the vertex data of the given vertex ID:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight cpp&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// Get the data for the current vertex.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Vertex&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vertexArray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now we need to create an instance of &lt;code&gt;VertexOut&lt;/code&gt;, set its properties and return it. Starting with the color is easy:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight cpp&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;VertexOut&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// Pass the vertex color directly to the rasterizer
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As for the position, we need to convert our vertex data of the form \((x_v, y_v)\) to the form \((x_s, y_s, z, w)\), where \(x_s\) and \(y_s\) must be in normalized screen-space. In our case \(x_v\) and \(y_v\) are already in normalized screen-space, so we assign them directly to \(x_s\) and \(y_s\). As for \(z\) (depth) and \(w\) (homogeneous component) it suffices to use values of 0 and 1. After that we simply return &lt;code&gt;out&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight cpp&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// Pass the already normalized screen-space coordinates to the rasterizer
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pos&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;float4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;Vertex shaders which simply pass data through mostly unchanged to the rasterizer are a very common pattern, and are called pass-through vertex shaders.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now, to code the fragment shader, all we need to do is return the final color we want. But the rasterizer has already linearly interpolated the colors of the 3 vertices among the rasterized pixels, which is exactly what we want. Thus, the only code in the fragment shader is:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight cpp&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;interpolated&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now we can finally compile and run the code, and we see:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/post_assets/metal/metal-intro-1/screen3.png&quot; alt=&quot;Red Screen With Multicolored Triangle&quot;&gt;&lt;/p&gt;

&lt;p&gt;That is almost what we wanted! We just forgot that we left the clear color at red. Go back and change the clear color to black, and it should look like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/post_assets/metal/metal-intro-1/screen4.png&quot; alt=&quot;Black Screen With Multicolored Triangle&quot;&gt;&lt;/p&gt;

&lt;p&gt;Congratulations, you have just learned most of the fundamentals behind Metal. Perhaps this was a bit long (it was a longer post than I expected), but it is pretty cool to synthesize all of the basic concepts together.&lt;/p&gt;

&lt;p&gt;For reference the complete sample project is &lt;a href=&quot;/public/post_assets/metal/metal-intro-1/MetalIntro1.zip&quot;&gt;available for download here&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&quot;concluding-remarks-3&quot;&gt;Concluding Remarks&lt;/h1&gt;
&lt;p&gt;This post covered a lot of material; to recap, we saw how to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Setup a MetalKit view in a native macOS app.&lt;/li&gt;
&lt;li&gt;Manage a &lt;code&gt;MTLCommandQueue&lt;/code&gt; and &lt;code&gt;MTLCommandBuffer&lt;/code&gt; objects to send commands to the GPU.&lt;/li&gt;
&lt;li&gt;Configure rendering properties using &lt;code&gt;MTLRenderPassDescriptor&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Setup a pipeline with custom shaders using &lt;code&gt;MTLRenderPipelineDescriptor&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Prepare our custom vertex data, and send it over to the GPU using &lt;code&gt;MTLBuffer&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Write basic pass-through shaders.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These concepts are crucial: without them it&amp;#39;s impossible to do any Metal programming. But the good news is that these concepts are mostly the same regardless of what you are doing with Metal: we will add complexity on top of this solid foundation.&lt;/p&gt;
&lt;h1 id=&quot;challenges&quot;&gt;Challenges&lt;/h1&gt;
&lt;p&gt;What&amp;#39;s really great and fun about graphics programming is that it is easy to experiment with and try things on your own. Below are some suggestions for things to try; some are more structured than others, but they are roughly sorted in increasing order of difficulty. Do as many as you please!&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Try moving around the positions of the vertices of the triangle, making sure you are comfortable with how normalized screen-space coordinates work.&lt;/li&gt;
&lt;li&gt;In graphics there are usually multiple ways to achieve the same effect, but with different tradeoffs. We don&amp;#39;t care about the tradeoffs yet, but we can experiment with different ways to do things. Make the triangle all blue &lt;em&gt;by only modifying &lt;code&gt;Renderer.swift&lt;/code&gt;&lt;/em&gt;, so it looks like this: &lt;img src=&quot;/public/post_assets/metal/metal-intro-1/screen5.png&quot; alt=&quot;All Blue Triangle&quot;&gt;.&lt;/li&gt;
&lt;li&gt;Again make the triangle all blue, but this time *by only modifying &lt;code&gt;Shaders.metal&lt;/code&gt;. After doing this, in what way could you simplify the &lt;code&gt;Vertex&lt;/code&gt; data struct, buffer, and &lt;code&gt;VertexOut&lt;/code&gt;?&lt;/li&gt;
&lt;li&gt;Flip the triangle upside down &lt;em&gt;by only modifying &lt;code&gt;Renderer.swift&lt;/code&gt;&lt;/em&gt;, so it looks like this: &lt;img src=&quot;/public/post_assets/metal/metal-intro-1/screen6.png&quot; alt=&quot;Upside down triangle&quot;&gt;.&lt;/li&gt;
&lt;li&gt;Again flip the triangle upside down &lt;em&gt;but this time only modify &lt;code&gt;Shaders.metal&lt;/code&gt;&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Render a rectangle instead of a triangle.&lt;/li&gt;
&lt;li&gt;Try to render a circle &lt;em&gt;by modifying &lt;code&gt;Shaders.metal&lt;/code&gt; only&lt;/em&gt; (you can use rectangle code from 6. in &lt;code&gt;Renderer.swift&lt;/code&gt; if you want). Hint: you can use &lt;code&gt;interpolated.pos&lt;/code&gt; to also determine color, BUT it is in un-normalized screen-space! It is fine if you code is hacky and does not work as the window resizes.&lt;/li&gt;
&lt;li&gt;Try to render a circle by using the normal code (or single color code) for &lt;code&gt;Shaders.metal&lt;/code&gt;, but by creating a ton of triangles in &lt;code&gt;Renderer.swift&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Play around and experiment with whatever you want!&lt;/li&gt;
&lt;/ol&gt;
</content>
 </entry>
 
 <entry>
   <title>Multi Variable Linear Regression</title>
   <link href="https://donaldpinckney.com/tensorflow/2018/03/21/multi-variable.html"/>
   <updated>2018-03-21T00:00:00-07:00</updated>
   <id>https://donaldpinckney.com/tensorflow/2018/03/21/multi-variable</id>
   <content type="html">&lt;h1 id=&quot;multi-variable-regression&quot;&gt;Multi Variable Regression&lt;/h1&gt;
&lt;p&gt;In &lt;a href=&quot;/books/tensorflow/book/ch2-linreg/2017-12-03-single-variable.html&quot;&gt;chapter 2.1&lt;/a&gt; we learned the basics of TensorFlow by creating a single variable linear regression model. In this chapter we expand this model to handle multiple variables. Note that less time will be spent explaining the basics of TensorFlow: only new concepts will be explained, so feel free to refer to previous chapters as needed.&lt;/p&gt;
&lt;h2 id=&quot;motivation-1&quot;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;Recall that a single variable linear regression model can learn to predict an output variable \(y\) under these conditions:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;There is only one input variable, \(x\)&lt;/li&gt;
&lt;li&gt;There is a linear relationship between \(y\) and \(x\), that is, \(y \approx ax + b\)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In practice, the above conditions are very limiting: if you have a simple data set then by all means you should try using single variable linear regression, but in most cases we have significantly more complex data. For example, consider using the following (abbreviated) &lt;a href=&quot;https://www.kaggle.com/camnugent/california-housing-prices&quot;&gt;data from the 1990 census&lt;/a&gt; to learn to predict housing prices. Note that each row represents a single housing district:&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;House Median Age&lt;/th&gt;
&lt;th&gt;Total Rooms&lt;/th&gt;
&lt;th&gt;Total Bedrooms&lt;/th&gt;
&lt;th&gt;...&lt;/th&gt;
&lt;th&gt;Median House Value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;41.0&lt;/td&gt;
&lt;td&gt;880.0&lt;/td&gt;
&lt;td&gt;129.0&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;452600.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;21.0&lt;/td&gt;
&lt;td&gt;7099.0&lt;/td&gt;
&lt;td&gt;1106.0&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;358500.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;52.0&lt;/td&gt;
&lt;td&gt;1467.0&lt;/td&gt;
&lt;td&gt;190.0&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;352100.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;52.0&lt;/td&gt;
&lt;td&gt;1274.0&lt;/td&gt;
&lt;td&gt;235.0&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;341300.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;52.0&lt;/td&gt;
&lt;td&gt;1627.0&lt;/td&gt;
&lt;td&gt;280.0&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;342200.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;52.0&lt;/td&gt;
&lt;td&gt;919.0&lt;/td&gt;
&lt;td&gt;213.0&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;269700.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;p&gt;To predict the values of houses, we have at least 3 real-valued variables (age, number of rooms, number of dedrooms) that could potentially be useful. To analyze this sort of complex, real-world data we need to learn to handle multiple input variables.&lt;/p&gt;

&lt;p&gt;One approach to handling multiple variables would be to reduce the number of input variables to only 1 variable, and then training a single variable linear regression model using that. In fact, an important area of research in machine learning (and one that will be covered later) called &lt;strong&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/dimensionality_reduction&quot;&gt;dimensionality reduction&lt;/a&gt;&lt;/strong&gt; deals with this problem of reducing the number of variables. However, it&amp;#39;s important to realize that the number of variables can only be reduced so far, and its extremely rare that you can reduce a data set to only 1 variable. For now you need to take this statement on faith, but in later chapters we will investigate it more thoroughly.&lt;/p&gt;

&lt;p&gt;So, it seems that we will have to deal with training models that can handle multiple variables. In this chapter we learn how to allow multiple input variables in our linear regression model. Such a model is called multi variable linear regression, or just linear regression.&lt;/p&gt;
&lt;h2 id=&quot;theory-1&quot;&gt;Theory&lt;/h2&gt;
&lt;p&gt;Most of the theory is similar to the theory for single variable linear regression, but we will need to augment and generalize it to handle multiple variables.&lt;/p&gt;
&lt;h3 id=&quot;data-set-format-1&quot;&gt;Data set format&lt;/h3&gt;
&lt;p&gt;Previously we defined our data set \(D\) as consisting of many example pairs of \(x\) and \(y\), where \(m\) is the number of examples:
\[
    D = \{ (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots, (x^{(m)}, y^{(m))} \}
\]&lt;/p&gt;

&lt;p&gt;Note that I have changed the notation compared to before. The notation \(x^{(i)}\) refers to the \(i\)&amp;#39;th \(x\) training example, it does &lt;em&gt;NOT&lt;/em&gt; mean \(x\) to the \(i\)&amp;#39;th power, which would be written as \(x^i\). I promise the notation change will be useful shortly.&lt;/p&gt;

&lt;p&gt;Alternatively, we can write \(D\) as 2 vectors of shape 1 x \(m\):
\[
    D_x = \begin{bmatrix}
            x^{(1)},
            x^{(2)},
            \dots,
            x^{(m)}
    \end{bmatrix} \\
    D_y = \begin{bmatrix}
            y^{(1)},
            y^{(2)},
            \dots,
            y^{(m)}
         \end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;But now, we need each \(x^{(i)}\) example to contain multiple numbers, one for each input variable.  Let \(n\) be the number of input variables. Then the easiest way to write this is to let each \(x^{(i)}\) be a vector of shape \(n\) x 1. That is,
\[
    x^{(i)} = \begin{bmatrix}
        x^{(i)}_1 \\
        x^{(i)}_2 \\
        \vdots \\
        x^{(i)}_j \\
        \vdots \\
        x^{(i)}_n
    \end{bmatrix}
\]
Note that the notation \(x^{(i)}_j\) denotes the \(j\)&amp;#39;th input variable in the \(i\)&amp;#39;th example data.&lt;/p&gt;

&lt;p&gt;Since each \(x^{(i)}\) has \(n\) rows, and \(D_x\) has \(m\) columns, each of which is an \(x^{(i)}\), we can write \(D_x\) as a massive \(n \times m\) matrix:
\[
    D_x = \begin{bmatrix}
            x^{(1)},
            x^{(2)},
            \dots,
            x^{(m)} \end{bmatrix}
        = \begin{bmatrix}
            x^{(1)}_1 &amp;amp; x^{(2)}_1  &amp;amp; \dots &amp;amp; x^{(i)}_1 &amp;amp; \dots &amp;amp; x^{(m)}_1 \\
            x^{(1)}_2 &amp;amp; x^{(2)}_2  &amp;amp; \dots &amp;amp; x^{(i)}_2 &amp;amp; \dots &amp;amp; x^{(m)}_2 \\
            \vdots &amp;amp; \vdots  &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
            x^{(1)}_j &amp;amp; x^{(2)}_j  &amp;amp; \dots &amp;amp; x^{(i)}_j &amp;amp; \dots &amp;amp; x^{(m)}_j \\
            \vdots &amp;amp; \vdots  &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
            x^{(1)}_n &amp;amp; x^{(2)}_n  &amp;amp; \dots &amp;amp; x^{(i)}_n &amp;amp; \dots &amp;amp; x^{(m)}_n \\
        \end{bmatrix}
\]
So, each column of \(D_x\) represents a single input data example. We don&amp;#39;t need to change the 1 x \(m\) vector \(D_y\), since we still only have 1 output variable.&lt;/p&gt;
&lt;h3 id=&quot;model-concept-1&quot;&gt;Model concept&lt;/h3&gt;
&lt;p&gt;So, we now have an input data matrix \(D_x\) with each column vector representing a single input data example, and we have the corresponding \(D_y\) row vector, each entry of which is an output data example. How do we define a model which can linearly estimate the output \(y&amp;#39;^{(i)}\) given the input data vector \(x^{(i)}\)? Let&amp;#39;s build it up from simple concepts, and build towards more complex linear algebra.&lt;/p&gt;

&lt;p&gt;Since we want \(y&amp;#39;^{(i)}\) to depend linearly on each \(x^{(i)}_j\) for \(1 \leq j \leq n\), we can write:
\[
    y&amp;#39;^{(i)} = a_1 x^{(i)}_1 + a_2 x^{(i)}_2 + \cdots + a_j x^{(i)}_j + \cdots + a_n x^{(i)}_n + b
\]&lt;/p&gt;

&lt;p&gt;This is fine mathematically, but it&amp;#39;s not very general. Suppose \(n = 100\): then we would have to literally write out 100 terms in our TensorFlow code. We can generalize this using linear algebra. Let \(A\) be a row vector of shape 1 x \(n\), containing each \(a_j\):
\[
    A = \begin{bmatrix}
            a_1,
            a_2,
            \cdots,
            a_j,
            \cdots,
            a_n
    \end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;Now, let&amp;#39;s see what happens if we compute \(A x^{(i)}\), as matrix multiplication. Note that \(A\) has shape 1 x \(n\) and \(x^{(i)}\) has shape \(n\) x 1. This is perfect! When performing matrix multiplication, the inner dimensions (in this case \(n\) and \(n\)) have to match, and the outer dimensions (in this case \(1\) and \(1\)) determine the output shape of the multiplication. So \(A x^{(i)}\) will have shape 1 x 1, or in other words, just a single number, in fact it is exactly \(y&amp;#39;^{(i)}\). How does this matrix multiplication exactly work? I&amp;#39;ll refer you to &lt;a href=&quot;https://www.khanacademy.org/math/precalculus/precalc-matrices/multiplying-matrices-by-matrices/v/matrix-multiplication-intro&quot;&gt;this video by Khan Academy&lt;/a&gt;, and explain it briefly in this case. Here, it is easier since \(A\) is a row vector, and \(x^{(i)}\) is a column vector. We simply multiply each corresponding entry, and add it all up:
\[
    A x^{(i)} + b
    = \begin{bmatrix}
            a_1,
            a_2,
            \cdots,
            a_j,
            \cdots,
            a_n
    \end{bmatrix} \begin{bmatrix}
        x^{(i)}_1 \\
        x^{(i)}_2 \\
        \vdots \\
        x^{(i)}_j \\
        \vdots \\
        x^{(i)}_n
    \end{bmatrix} + b
    = a_1 x^{(i)}_1 + a_2 x^{(i)}_2 + \cdots + a_j x^{(i)}_j + \cdots + a_n x^{(i)}_n + b
    = y&amp;#39;^{(i)}
\]&lt;/p&gt;

&lt;p&gt;This matrix equation, \(y&amp;#39;(x, A, b) = Ax + b\) is exactly what we want as our model. As one final note, recall that in the actual implementation, we don&amp;#39;t want \(x\) and \(y&amp;#39;\) to represent just one input data and predicted output, we want them to represent several. Since \(x\) is a column vector, the natural way to represent multiple input data points is with a matrix, very similar to the matrix \(D_x\), just not necessarily with &lt;em&gt;all&lt;/em&gt; the columns of \(D_x\), and \(y&amp;#39;\) should be a row vector. Specifically, \(A\) has shape 1 x \(n\), \(x\) has shape \(n\) x &lt;code&gt;None&lt;/code&gt;, and \(y\) has shape 1 x &lt;code&gt;None&lt;/code&gt;, using the TensorFlow convention that &lt;code&gt;None&lt;/code&gt; represents a yet-to-be-determined matrix size.&lt;/p&gt;

&lt;p&gt;Now defining the loss function is pretty much the same as before, just using the new model:
\[
     L(A, b) = \sum_{i=1}^m (y&amp;#39;(x^{(i)}, A, b) - y^{(i)})^2 = \sum_{i=1}^m (A x^{(i)} + b - y^{(i)})^2
\]&lt;/p&gt;

&lt;p&gt;To minimize the loss function, we use the same process as before, gradient descent. However, previously the gradient descent was altering 2 variables (\(a\) and \(b\)) so as to minimize the loss function, and so we could plot the loss function and gradient descent progress in terms of \(a\) and \(b\). However, now the optimization needs to alter many more variables, since \(A\) actually contains \(n\) variables, the gradient descent must be performed in \(n+1\) dimensional space, and we don&amp;#39;t have an easy way to visualize this.&lt;/p&gt;

&lt;p&gt;With the more general linear algebra formulation of linear regression under our belts, let&amp;#39;s move on to actually coding stuff.&lt;/p&gt;
&lt;h2 id=&quot;implementation-1&quot;&gt;Implementation&lt;/h2&gt;
&lt;p&gt;As before, we need to: import data, define the model, define the loss function, run gradient descent, and finally make predictions. Many steps will be similar to the single variable case, but for completeness I will walk through them briefly.&lt;/p&gt;

&lt;p&gt;For building and testing the implementation we will use a synthetic data set consisting of \(n=2\) input variables. You can download &lt;a href=&quot;/books/tensorflow/book/ch2-linreg/code/linreg-multi-synthetic-2.csv&quot;&gt;the synthetic data set here&lt;/a&gt;. By synthetic, I mean that I purposefully created a very nicely behaved data set so that we can practice implementing multi variable linear regression, and verify that we converged to the right answer. In fact, the synthetic data is generated as \(y = 2x_1 + 1.3x_2 + 4 + \varepsilon \) where \(\varepsilon\) is random noise. If we implement multi variable linear regression correctly, then we should obtain approximately \(A = \begin{bmatrix} 2, 1.3 \end{bmatrix}, b = 4\). This plot illustrates what the data looks like in 3 dimensions, essentially a plane in 3 dimensions with some random fluctuations:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/books/tensorflow/book/ch2-linreg/assets/linreg-multi-synthetic-2.png&quot; alt=&quot;scatter&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;importing-the-data-1&quot;&gt;Importing the data&lt;/h3&gt;
&lt;p&gt;As explained above, the input data set can be organized as an \(n \times m\) matrix. Since we will load the entire data set (input and output) from a single CSV file, and we have 2 input variables, the CSV file will contain 3 columns: the first 2 are the input variables, and the last one is the output variable. So, first we load the CSV file into an \(m\) x 3 matrix, and then separate the first 2 columns from the last:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight python&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# First we load the entire CSV file into an m x 3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;linreg-multi-synthetic-2.csv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;header&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# We extract all rows and the first 2 columns into X_data&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Then we flip it&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# We extract all rows and the last column into y_data&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Then we flip it&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# And make a convenient variable to remember the number of input columns&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The syntax &lt;code&gt;D[:, 0:2]&lt;/code&gt; might be new, particularly if you haven&amp;#39;t worked with NumPy before. In the single variable implementation we used Panda&amp;#39;s functionality to access the columns by column name. This is a great approach, but sometimes you might need to be more flexible in how you access columns of data.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The basic syntax for subscripting a matrix is: &lt;code&gt;D[3, 6]&lt;/code&gt; (for example), which refers to the row at index 3 and the column at index 6 in the matrix &lt;code&gt;D&lt;/code&gt;. Note that in &lt;code&gt;numpy&lt;/code&gt; the row and column indices start at 0! This means that &lt;code&gt;D[0, 0]&lt;/code&gt; refers to the top-left entry of matrix &lt;code&gt;D&lt;/code&gt;. If you are coming from a pure math background, or have used MATLAB before, it is a common error to assume the indices start at 1. &lt;br /&gt;&lt;br /&gt;
Now for slicing, the &lt;code&gt;:&lt;/code&gt; character is used to indicate a range. If it is used by itself, it indicates the entire range of rows / columns. For example, &lt;code&gt;D[:, 42]&lt;/code&gt; refers to all rows of &lt;code&gt;D&lt;/code&gt;, and the column at index 42. If it is used with indices, then &lt;code&gt;i:j&lt;/code&gt; indicates the range of rows / columns at indices &lt;code&gt;i&lt;/code&gt;, &lt;code&gt;i+1&lt;/code&gt;, ..., &lt;code&gt;j-1&lt;/code&gt;, but &lt;em&gt;not&lt;/em&gt; including &lt;code&gt;j&lt;/code&gt;. &lt;br /&gt;&lt;br /&gt;
So, &lt;code&gt;D[:, 0:2]&lt;/code&gt; means to read the values in &lt;code&gt;D&lt;/code&gt; at all rows and at columns with index &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt; (the entire first 2 columns, i.e. the input data columns). Likewise, &lt;code&gt;D[:, 2]&lt;/code&gt; means to read the values in &lt;code&gt;D&lt;/code&gt; at all rows and at the column of index &lt;code&gt;2&lt;/code&gt; (the entire last column, i.e. the output data column).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This matrix subscripting and slicing is almost what we want, but not quite. The problem is that &lt;code&gt;D[:, 0:2]&lt;/code&gt;, which contains our \(D_x\) data, is a matrix of shape \(m \times n\), but earlier we decided that we wanted \(D_x\) to be an \(n \times m\) matrix, so we need to flip it. To do so, we use the &lt;a href=&quot;https://en.wikipedia.org/wiki/Transpose&quot;&gt;&lt;strong&gt;transpose&lt;/strong&gt;&lt;/a&gt; of the matrix. Mathematically we write the transpose of a matrix \(A\) as \(A^T\), and in Python we can compute it using &lt;code&gt;A.transpose()&lt;/code&gt;. Essentially, the transpose of a matrix simply flips it along the diagonal, as shown in this animation:&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;p&gt;&lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Matrix_transpose.gif#/media/File:Matrix_transpose.gif&quot;&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/e/e4/Matrix_transpose.gif&quot; alt=&quot;Matrix transpose.gif&quot;&gt;&lt;/a&gt;&lt;br&gt;By &lt;a href=&quot;//commons.wikimedia.org/wiki/User:LucasVB&quot; title=&quot;User:LucasVB&quot;&gt;LucasVB&lt;/a&gt; - &lt;a href=&quot;https://commons.wikimedia.org/w/index.php?curid=21897854&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;So, &lt;code&gt;D[:, 0:2].transpose()&lt;/code&gt; is a matrix of shape \(n \times m\), and is our correct data input matrix \(D_x\). We save this matrix to the variable &lt;code&gt;X_data&lt;/code&gt;. Likewise, we also transpose &lt;code&gt;D[:, 2]&lt;/code&gt; to correctly compute \(D_y\), and save it in &lt;code&gt;y_data&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;At this point we have our \(m \times n\) input data matrix &lt;code&gt;X_data&lt;/code&gt; and our \(m \times 1\) output vector &lt;code&gt;y_data&lt;/code&gt; loaded. In addition, we conveniently have the number of columns stored in &lt;code&gt;n&lt;/code&gt;, so now we can start defining our model.&lt;/p&gt;
&lt;h3 id=&quot;defining-the-model-2&quot;&gt;Defining the model&lt;/h3&gt;
&lt;p&gt;As shown above, we want our model parameters to consist of a matrix \(A\) of size \(1 \times n\) and a single number \(b\). Then, we define:
\[
    y&amp;#39;(x, A, b) = Ax + b
\]&lt;/p&gt;

&lt;p&gt;First, we can define the input and correct output placeholders:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight python&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Define data placeholders&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And then we can define the trainable variables, the output prediction, and the loss function:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight python&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Define trainable variables&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;A&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;b&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define model output&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_predicted&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define the loss function&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduce_sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_predicted&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&quot;training-the-model&quot;&gt;Training the model&lt;/h3&gt;
&lt;p&gt;At this point, we have a 1 dimensional output &lt;code&gt;y_predicted&lt;/code&gt; which we compare against &lt;code&gt;y&lt;/code&gt; using &lt;code&gt;L&lt;/code&gt; to train the model, which is exactly the same situation as single variable linear regression. The remaining code to train the model is extremely similar, so I&amp;#39;ll simply display it here, and then explain the few differences:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight python&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Define optimizer object&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AdamOptimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minimize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Create a session and initialize variables&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;session&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Session&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;session&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;global_variables_initializer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Main optimization loop&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;session&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feed_dict&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_data&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;t = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;g, loss = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;g, A = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s, b = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;g&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;First, we have a different learning rate than the learning rate used in single variable regression. Even though the training algorithm is the same, since this is a different problem than single variable regression, we need find a good learning rate specific to this problem. A great way to do this for your own problems is using TensorBoard, as explained in the chapter &lt;a href=&quot;https://donaldpinckney.com/books/tensorflow/book/ch2-linreg/2017-12-27-optimization.html&quot;&gt;Optimization Convergence&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Besides this, the only other conceptual difference is that at each step of the optimizer we are modifying the entire vector &lt;code&gt;A&lt;/code&gt; (in addition to &lt;code&gt;b&lt;/code&gt;), rather than just a single number. However, TensorFlow abstracts this away for us, and conceptually we just need to know that we are training the variable &lt;code&gt;A&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The final print statements should output something close to:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight plaintext&quot;&gt;&lt;code&gt;t = 1994, loss = 1.44798e+06, A = [[ 2.00547647  1.3020972 ]], b = 3.95038
t = 1995, loss = 1.44798e+06, A = [[ 2.00547647  1.3020972 ]], b = 3.95038
t = 1996, loss = 1.44798e+06, A = [[ 2.00547647  1.3020972 ]], b = 3.95038
t = 1997, loss = 1.44798e+06, A = [[ 2.00547647  1.3020972 ]], b = 3.95038
t = 1998, loss = 1.44798e+06, A = [[ 2.00547647  1.3020972 ]], b = 3.95038
t = 1999, loss = 1.44798e+06, A = [[ 2.00547647  1.3020972 ]], b = 3.95038
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;At this point we have converged to our approximate solution of \(A \approx \begin{bmatrix}
            2.005,
            1.302
    \end{bmatrix}, b \approx 3.95\). Note that this is not exactly the same as the expected answer of \(A = \begin{bmatrix}
            2,
            1.3
    \end{bmatrix}, b \approx 4\), primarily because some random noise was added to each point in the data set.&lt;/p&gt;

&lt;p&gt;The model is fully trained, so now given a new input \(x\) we could now predict the output \(y&amp;#39; = Ax + b\), using all the learned information from all input variables.&lt;/p&gt;
&lt;h1 id=&quot;concluding-remarks-2&quot;&gt;Concluding Remarks&lt;/h1&gt;
&lt;p&gt;Linear regression with multiple variables is only slightly different in essence from single variable linear regression. The main difference is abstracting the linear operation \(ax\) where \(a\) and \(x\) are single numbers to the linear operation \(Ax\), where now \(A\) is a matrix, \(x\) is a vector. In addition, at the implementation level we also have to deal with loading data in a more sophisticated manner, but otherwise the code is mostly the same. In later chapters we will use this abstraction we have built to define even more powerful models.&lt;/p&gt;
&lt;h1 id=&quot;challenge-problems-2&quot;&gt;Challenge Problems&lt;/h1&gt;
&lt;p&gt;So far this chapter has used a synthetic data set, &lt;code&gt;linreg-multi-synthetic-2.csv&lt;/code&gt;, for easy demonstration. The exercises are primarily concerned with getting practice at applying this model to real-world data. Note that in real-world data not all columns are useful, and some might not have a linear relationship with the MPG. Including these unhelpful columns in your model might decrease the accuracy of your model. You should try plotting various columns vs. the output column to determine which seem most helpful in predicting the output, and then only include these useful columns as your input.&lt;/p&gt;

&lt;p&gt;In addition, many data sets will have so called &lt;em&gt;messy data&lt;/em&gt;, which require you to do some manipulation in Python to make sure the data is imported cleanly and properly. For example, some rows might containg missing data: for these your code can not crash or incorrectly import the data. Instead, you need to adopt a strategy to still import the data as best as you can: for example, you can simply ignore any rows that have incomplete data.&lt;/p&gt;

&lt;p&gt;Note that we have not discussed how to rigorously evaluate how good a model is yet. For now you can use the value of the loss function, along with some intuition and creating plots. Evaluation will be discussed more in chapter 2.7.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Download &lt;a href=&quot;https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009&quot;&gt;this red wine quality data set&lt;/a&gt;, and try to predict the quality of the wine (last column) from the physicochemical input data (other columns).&lt;/li&gt;
&lt;li&gt;Download &lt;a href=&quot;https://archive.ics.uci.edu/ml/datasets/Auto+MPG&quot;&gt;this car MPG data set&lt;/a&gt;, and try to predict the MPG (first column) based on some of the other columns.&lt;/li&gt;
&lt;li&gt;Download &lt;a href=&quot;https://www.kaggle.com/camnugent/california-housing-prices&quot;&gt;this California 1990 Housing Value data set&lt;/a&gt;, and try to predict the house values based on various factors.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&quot;complete-code-2&quot;&gt;Complete Code&lt;/h1&gt;
&lt;p&gt;The &lt;a href=&quot;https://github.com/donald-pinckney/donald-pinckney.github.io/blob/src/books/tensorflow/src/ch2-linreg/code/multi_var_reg.py&quot;&gt;complete example code is available on GitHub&lt;/a&gt;, as well as directly here:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight python&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# First we load the entire CSV file into an m x 3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;linreg-multi-synthetic-2.csv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;header&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# We extract all rows and the first 2 columns into X_data&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Then we flip it&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# We extract all rows and the last column into y_data&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Then we flip it&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# And make a convenient variable to remember the number of input columns&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define data placeholders&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define trainable variables&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;A&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;b&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define model output&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_predicted&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define the loss function&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduce_sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_predicted&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define optimizer object&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AdamOptimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minimize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Create a session and initialize variables&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;session&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Session&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;session&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;global_variables_initializer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Main optimization loop&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;session&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feed_dict&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_data&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;t = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;g, loss = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;g, A = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s, b = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;g&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content>
 </entry>
 
 <entry>
   <title>Exploring Optimization Convergence</title>
   <link href="https://donaldpinckney.com/tensorflow/2017/12/27/optimization.html"/>
   <updated>2017-12-27T00:00:00-08:00</updated>
   <id>https://donaldpinckney.com/tensorflow/2017/12/27/optimization</id>
   <content type="html">&lt;h1 id=&quot;exploring-optimization-convergence&quot;&gt;Exploring Optimization Convergence&lt;/h1&gt;
&lt;p&gt;In the previous chapter we created a simple single variable linear regression model to fit a data set. While the Python code was actually fairly short and simple, I did gloss over some details related to the optimization, and I hope to use this short chapter to answer some dangling questions about it. Since this chapter doesn&amp;#39;t introduce new models or concepts you can skip it (or come back later) if you prefer. However, getting a feel for optimization is useful for training just about any model, not just the single variable linear regression model, and this chapter should give you insight that is useful for the rest of this guide and just about any machine learning you do.&lt;/p&gt;

&lt;p&gt;To explore optimization we are going to exactly copy the code from the previous chapter, and experiment with it. To review, let&amp;#39;s look at the part of the code from before that performed the optimization (just look at the previous chapter if you need to review the entire thing):&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight python&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Define optimizer object&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# L is what we want to minimize&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AdamOptimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minimize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Create a session and initialize variables&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;session&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Session&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;session&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;global_variables_initializer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Main optimization loop&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;session&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feed_dict&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_data&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;t = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;g, loss = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;g, a = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;g, b = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;g&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The main unanswered questions to address are:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;I said that the learning rate affects how large of steps the optimization algorithm takes in one unit of time. But how do we choose an appropriate value for the learning rate, such as &lt;code&gt;0.2&lt;/code&gt;?&lt;/li&gt;
&lt;li&gt;What is this &lt;code&gt;AdamOptimizer&lt;/code&gt; exactly, are there other choices for optimizers, and how do they differ?&lt;/li&gt;
&lt;li&gt;Currently this code runs for &lt;code&gt;10000&lt;/code&gt; iterations, and that seems good enough to fully optimize &lt;code&gt;L&lt;/code&gt;. But how do we choose this appropriate amount of time for training?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There aren&amp;#39;t exact or easy answers to the above questions, but answering these questions is made even harder by the fact that we can not effectively visualize the training progress with the code we have.  Currently we have some &lt;code&gt;print(...)&lt;/code&gt; statements, which is good enough to see that the training error is decreasing, but not much more than that. Let&amp;#39;s start with learning how to visualize training, since this will help us address the other questions and give us a deeper intuition about optimization.&lt;/p&gt;
&lt;h2 id=&quot;how-to-visualize-training-progress&quot;&gt;How to visualize training progress&lt;/h2&gt;&lt;h3 id=&quot;extracting-hyperparameters&quot;&gt;Extracting hyperparameters&lt;/h3&gt;
&lt;p&gt;One of the simplest ways to visualize training progress is to plot the value of the loss function over time. We could certainly plot the value of the loss function using matplotlib, like we plotted the data set. But TensorFlow actually has a tool built-in for plotting training progress, called TensorBoard. It&amp;#39;s pretty handy, but first we need to do some work to refactor our current code.&lt;/p&gt;

&lt;p&gt;The first thing I&amp;#39;d like to do is move &lt;strong&gt;hyperparameters&lt;/strong&gt; to the top of the script. What exactly are &lt;strong&gt;hyperparameters&lt;/strong&gt;? They are parameters that affect how the training of the model proceeds, but are not part of the model itself. For example, \(a\) and \(b\) are parameters, while the learning rate \(\alpha\) is a hyperparameter. The hyperparameters we have are:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The learning rate, currently &lt;code&gt;0.2&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The number of training iterations, currently &lt;code&gt;10000&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The choice of optimization routine, currently &lt;code&gt;tf.train.AdamOptimizer&lt;/code&gt; (This isn&amp;#39;t often thought of as a hyperparameter, but here will think of it as one).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So, we can just put these into constants at the top of the code. Also, we are going to change these values to give us a simpler starting point:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight python&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;### Hyperparameters ###&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;LEARNING_RATE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.000001&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Much smaller training rate, to make sure the optimization is at least reliable.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;NUM_ITERS&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20000&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Twice as many training iterations, just gives us more room to experiment later.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;OPTIMIZER_CONSTRUCTOR&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GradientDescentOptimizer&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# This is the simplest optimization algorithm.&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# .. all the rest of the code, hyperparameter constants properly substituted&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&quot;setting-up-tensorboard&quot;&gt;Setting up TensorBoard&lt;/h3&gt;
&lt;p&gt;Now, we need to add some code to configure TensorBoard for our model. We can setup TensorFlow to automatically keep track of the training progress of &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;, and &lt;code&gt;L&lt;/code&gt; by using a &lt;code&gt;tf.summary.scalar&lt;/code&gt;. Since we need to setup summaries for &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;, and &lt;code&gt;L&lt;/code&gt;, but before we actually start training, insert this code after &lt;code&gt;L&lt;/code&gt; is defined, but before we start training the model:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight python&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# ... above this is where L is defined&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;### Summary setup ###&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scalar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'a'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scalar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scalar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'L'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;summary_node&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;merge_all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;log_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;g, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LEARNING_RATE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OPTIMIZER_CONSTRUCTOR&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;summary_writer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FileWriter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'/tmp/tensorflow/single_var_reg/'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Open /tmp/tensorflow/single_var_reg/ with tensorboard&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# ... below this is where we create the optimizer and session&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The first thing this code does is setup &lt;code&gt;tf.summary.scalar&lt;/code&gt; nodes for &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;, and &lt;code&gt;L&lt;/code&gt;, and then merge these 3 summaries into a single TensorFlow node, &lt;code&gt;summary_node&lt;/code&gt;.  Later we will need to run &lt;code&gt;summary_node&lt;/code&gt; with our TensorFlow session.&lt;/p&gt;

&lt;p&gt;The second thing this code does is setup where TensorFlow will write the logs to. We create a pretty log name by combining &lt;code&gt;LEARNING_RATE&lt;/code&gt; and &lt;code&gt;OPTIMIZER_CONSTRUCTOR&lt;/code&gt; (&lt;code&gt;__name__&lt;/code&gt; gets the name of the Python class), so that we can later compare logs that used different hyperparameters. Then, we create a &lt;code&gt;tf.summary.FileWriter&lt;/code&gt; using the path &lt;code&gt;/tmp/tensorflow/single_var_reg/&lt;/code&gt; concatenated with the log name. This &lt;code&gt;summary_writer&lt;/code&gt; is what we will use to write the &lt;code&gt;summary_node&lt;/code&gt; to log files. Finally we print out a useful message so we remember where the logs are.&lt;/p&gt;

&lt;p&gt;The last thing we need to do is make TensorFlow actually write the summary (&lt;code&gt;summary_node&lt;/code&gt;) to the log (using &lt;code&gt;summary_writer&lt;/code&gt;) as training progresses. In our training code before we had a &lt;code&gt;print&lt;/code&gt; statement that showed how training was going, but now we can replace it (or keep it, you choose):&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight python&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Main optimization loop&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NUM_ITERS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# We don't need to run L, a, b, just the summary_node.&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# The output of summary_node is stored in summary.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;session&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;summary_node&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feed_dict&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_data&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# We write this summary to the log file (print statement was here).&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;summary_writer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_summary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And as a final touch, we can also delete or comment out the code that plots the data set, since it reduces the amount of clicking we have to do when trying different hyperparameters. The complete modified code should look something like this:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight python&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;### Hyperparameters ###&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;LEARNING_RATE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.000001&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;OPTIMIZER_CONSTRUCTOR&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GradientDescentOptimizer&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;NUM_ITERS&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20000&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Load the data, and convert to 1x30 vectors&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;homicide.csv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;age&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_homicide_deaths&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;c&quot;&gt;### Model definition ###&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define x (input data) placeholder&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define the trainable variables&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;a&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;b&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define the prediction model&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_predicted&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;


&lt;span class=&quot;c&quot;&gt;### Loss function definition ###&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define y (correct data) placeholder&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define the loss function&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduce_sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_predicted&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;c&quot;&gt;### Summary setup ###&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;log_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;g, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LEARNING_RATE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OPTIMIZER_CONSTRUCTOR&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scalar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'a'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scalar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scalar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'L'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;summary_node&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;merge_all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;summary_writer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FileWriter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'/tmp/tensorflow/single_var_reg/'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Open /tmp/tensorflow/single_var_reg/ with tensorboard&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;c&quot;&gt;### Training the model ###&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define optimizer object&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OPTIMIZER_CONSTRUCTOR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LEARNING_RATE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minimize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Create a session and initialize variables&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;session&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Session&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;session&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;global_variables_initializer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Main optimization loop&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NUM_ITERS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;session&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;summary_node&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feed_dict&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_data&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;summary_writer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_summary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&quot;using-tensorboard&quot;&gt;Using TensorBoard&lt;/h3&gt;
&lt;p&gt;With the Python code prepared, we can now run it and use TensorBoard to visualize the training. First, run the Python code as usual, using Terminal, or your IDE. This will write logs to &lt;code&gt;/tmp/tensorflow/single_var_reg/{log_name}&lt;/code&gt;. Now, we can open this up with TensorBoard, by running this command in Terminal (make sure to activate the virtual environment first):&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight shell&quot;&gt;&lt;code&gt;python &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; tensorboard.main &lt;span class=&quot;nt&quot;&gt;--logdir&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/tmp/tensorflow/single_var_reg/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Once you run that command, go to &lt;a href=&quot;http://localhost:6006&quot;&gt;http://localhost:6006&lt;/a&gt; in your web browser, and you should see plots of &lt;code&gt;L&lt;/code&gt;, &lt;code&gt;a&lt;/code&gt;, and &lt;code&gt;b&lt;/code&gt; as the training progressed. With the hyperparameter choices from above, the plots should look like:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/books/tensorflow/book/ch2-linreg/assets/tensorboard_1.png&quot; alt=&quot;TensorBoard 1&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;interpreting-the-tensorboard-plots&quot;&gt;Interpreting the TensorBoard plots&lt;/h2&gt;
&lt;p&gt;Ok, so now we have some plots showing the progression of &lt;code&gt;L&lt;/code&gt;, &lt;code&gt;a&lt;/code&gt;, and &lt;code&gt;b&lt;/code&gt;. What can these plots tell us? Well, with the current choice of &lt;code&gt;LEARNING_RATE = 0.000001&lt;/code&gt;, the plot for &lt;code&gt;L&lt;/code&gt; clearly continues to decrease continually during training. This is good, since this means that the &lt;code&gt;tf.train.GradientDescentOptimizer&lt;/code&gt; is doing it&amp;#39;s job of decreasing the value of the loss function. However, the loss function continues to decrease quickly even towards the end of training: it is reasonable to expect that the loss function would continue to decrease substantially if we continued to train for more iterations. Therefore, we have not found the minimum of the loss function.&lt;/p&gt;

&lt;p&gt;To remedy this, we could do one of 3 things: run the training for more iterations, increase the learning rate, or experiment with another optimization algorithm. We don&amp;#39;t want to train for more iterations unless we have to, since that just takes more time, so we will start with increasing the learning rate.&lt;/p&gt;
&lt;h2 id=&quot;increasing-the-learning-rate&quot;&gt;Increasing the learning rate&lt;/h2&gt;
&lt;p&gt;Currently the learning rate is &lt;code&gt;0.000001&lt;/code&gt;, and is too small. A pretty easy way to try new learning rates is to go roughly by powers of 10. For now, we can try &lt;code&gt;0.000001&lt;/code&gt;, &lt;code&gt;0.000005&lt;/code&gt;, &lt;code&gt;0.00001&lt;/code&gt;. For each of these, you can just tweak the &lt;code&gt;LEARNING_RATE&lt;/code&gt; constant, and re-run the code. You don&amp;#39;t need to re-run the TensorBoard command (but you can), but make sure to reload the &lt;a href=&quot;http://localhost:6006&quot;&gt;http://localhost:6006&lt;/a&gt; page once you do all the runs. You should get plots that look like:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/books/tensorflow/book/ch2-linreg/assets/tensorboard_2.png&quot; alt=&quot;TensorBoard 2&quot;&gt;&lt;/p&gt;

&lt;p&gt;We can clearly see the improvement by increasing the learning rate. The final loss obtained with a learning rate of &lt;code&gt;0.00001&lt;/code&gt; is much smaller than our original loss obtained with a learning rate of &lt;code&gt;0.000001&lt;/code&gt;. In addition, we can see that the loss is decreasing more slowly at the end of training. However, the loss function still hasn&amp;#39;t converged, as it is still decreasing significantly. Again, to fix this we could train for longer, but as before we can try increasing the learning rate even more. We can try a learning rate of &lt;code&gt;0.00005&lt;/code&gt;, and we get this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/books/tensorflow/book/ch2-linreg/assets/tensorboard_3.png&quot; alt=&quot;TensorBoard 3&quot;&gt;&lt;/p&gt;

&lt;p&gt;Huh? No plot!?!?? If you mouse over the &lt;code&gt;L&lt;/code&gt; plot, TensorBoard will say that the value of &lt;code&gt;L&lt;/code&gt; is NaN (not a number). What gives? Well, if the learning rate is too big then &lt;code&gt;tf.train.GradientDescentOptimizer&lt;/code&gt; can explode: &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; (and consequently &lt;code&gt;L&lt;/code&gt;) increase to infinity and become NaN. What &amp;quot;too big&amp;quot; is depends on the specific problem.&lt;/p&gt;

&lt;p&gt;At this point there isn&amp;#39;t a lot more that we can do by just tweaking the learning rate: it&amp;#39;s either too big and causes the optimization to explode, or is too small to achieve convergence in &lt;code&gt;20000&lt;/code&gt; iterations. We can certainly try more learning rates between &lt;code&gt;0.00001&lt;/code&gt; and &lt;code&gt;0.00005&lt;/code&gt;, but it won&amp;#39;t be a ton better than what we already have.&lt;/p&gt;
&lt;h2 id=&quot;using-different-optimization-algorithms&quot;&gt;Using different optimization algorithms&lt;/h2&gt;
&lt;p&gt;So far we have been using &lt;code&gt;tf.train.GradientDescentOptimizer&lt;/code&gt;. It is the simplest, classic way to iteratively train machine learning models. As discussed in the previous chapter, it is like moving a ball downhill, according to the current slope (aka the derivative). Generally, gradient descent is part of a class of optimization algorithms called &lt;strong&gt;first-order methods&lt;/strong&gt;, since it uses only information from the first derivative of the loss function, and not higher-order derivatives. First-order methods are currently the dominant way to train most machine learning models.&lt;/p&gt;

&lt;p&gt;In fact, there are many first order methods, other than simple gradient descent. Most of them are designed to offer an advantage over other first-order methods via speed to find convergence, reliability, ease of use, etc. For a fairly in-depth exploration, see &lt;a href=&quot;http://ruder.io/optimizing-gradient-descent/index.html&quot;&gt;this blog post&lt;/a&gt;. To see the different optimization algorithms that are built-in to TensorFlow, see &lt;a href=&quot;https://www.tensorflow.org/api_guides/python/train#Optimizers&quot;&gt;the documentation here&lt;/a&gt;. In this list, you can see the &lt;code&gt;tf.train.AdamOptimizer&lt;/code&gt; that we used before, and the classic &lt;code&gt;tf.train.GradientDescentOptimizer&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We can experiment with any number of these. Here, I&amp;#39;&amp;#39;ll demonstrate experimenting with &lt;code&gt;tf.train.AdagradOptimizer&lt;/code&gt;, but feel free to play around with any of them. To use &lt;code&gt;tf.train.AdagradOptimizer&lt;/code&gt; we just need to change &lt;code&gt;OPTIMIZER_CONSTRUCTOR&lt;/code&gt;, and set &lt;code&gt;LEARNING_RATE&lt;/code&gt; back to &lt;code&gt;0.000001&lt;/code&gt; for good measure:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight python&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;LEARNING_RATE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.000001&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;OPTIMIZER_CONSTRUCTOR&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AdagradOptimizer&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Running this we see:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/books/tensorflow/book/ch2-linreg/assets/tensorboard_4.png&quot; alt=&quot;TensorBoard 4&quot;&gt;&lt;/p&gt;

&lt;p&gt;Well, this is disappointing: &lt;code&gt;L&lt;/code&gt; did not seem to decrease at all during training. The problem is that the learning rate used by gradient descent is really an entirely different learning rate from the Adagrad one: conceptually they are similar, but are on entirely different scales numerically. So, we just need to try different learning rates for Adagrad now. Since the value of &lt;code&gt;L&lt;/code&gt; stayed constant with this very small learning rate, we expect that we need to try much larger learning rates for Adagrad. In fact, by trying learning rates of 0.5, 1, and 5, we get these plots:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/books/tensorflow/book/ch2-linreg/assets/tensorboard_5.png&quot; alt=&quot;TensorBoard 5&quot;&gt;&lt;/p&gt;

&lt;p&gt;Now this is looking like progress! For the first time we start to get a sense of the loss rapidly decreasing, and then slowing down substantially. In addition, the final value of &lt;code&gt;a&lt;/code&gt; is now negative (which we know is correct) compared to previous runs which ended either positive or close to zero. However, by looking at the plots for &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; (and to a lesser degree &lt;code&gt;L&lt;/code&gt;) we can see that we still haven&amp;#39;t achieved convergence: &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; haven&amp;#39;t stopped changing substantially at the end of training. So, time to increase the learning rate even more! Before doing so, I am going to delete the logs of previous runs, except for the Adagrad run with a learning rate of 5, so that we can read the plots more clearly:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight shell&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# This will delete logs of all the runs&lt;/span&gt;
rm &lt;span class=&quot;nt&quot;&gt;-rf&lt;/span&gt; /tmp/tensorflow/single_var_reg/
&lt;span class=&quot;c&quot;&gt;# Or, you can delete a specific run, for example:&lt;/span&gt;
rm &lt;span class=&quot;nt&quot;&gt;-rf&lt;/span&gt; /tmp/tensorflow/single_var_reg/5e-05,&lt;span class=&quot;se&quot;&gt;\ &lt;/span&gt;GradientDescentOptimizer/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;By trying learning rates of 10 and 50, we finally achieve convergence:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/books/tensorflow/book/ch2-linreg/assets/tensorboard_6.png&quot; alt=&quot;TensorBoard 6&quot;&gt;&lt;/p&gt;

&lt;p&gt;Qualitatively, this looks like convergence (with a learning rate of 10, and certainly with a learning rate of 50) since the progress that Adagrad is making on decreasing &lt;code&gt;L&lt;/code&gt; (and adjusting &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;) has hit a brick wall: no matter how long we run Adagrad, we can&amp;#39;t seem to get a loss function value lower than about \(3.9296 \cdot 10^4 \), and similarly for the values of &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;. We&amp;#39;ve finally trained our model completely.&lt;/p&gt;

&lt;p&gt;Unfortunately, I don&amp;#39;t know of an easy way to intuitively understand the differences between Adagrad, Adam, and other first-order methods. &lt;a href=&quot;http://ruder.io/optimizing-gradient-descent/index.html&quot;&gt;This blog post&lt;/a&gt; does give some mathematical analysis that explains what each algorithm tries to improve upon, and some reasoning for choosing an algorithm, but it can be tricky to apply to real problems. In general, you can always start with the simplest algorithm (gradient descent), and if it isn&amp;#39;t converging quickly enough for you, then you can switch to a more sophisticated algorithm, such as Adagrad, Adam, or others.&lt;/p&gt;
&lt;h1 id=&quot;concluding-remarks-1&quot;&gt;Concluding Remarks&lt;/h1&gt;
&lt;p&gt;The experimental nature of this chapter should illustrate the practicalities of machine learning: a lot of cutting-edge machine learning currently involves running multiple experiments to try to find the best combination of hyperparameters. There isn&amp;#39;t a golden rule for choosing the optimization algorithm and hyperparameters, but hopefully this chapter demonstrates how to alter the algorithm and hyperparameters in TensorFlow and monitor convergence using TensorBoard. The most important takeaways are:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Learning how to use TensorBoard&lt;/li&gt;
&lt;li&gt;Recognizing convergence&lt;/li&gt;
&lt;li&gt;Recognizing the symptoms of too small of a learning rate&lt;/li&gt;
&lt;li&gt;Recognizing the symptoms of too large of a learning rate&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In future chapters I won&amp;#39;t include the code specifically for TensorBoard (unless it is important to that chapter) since I don&amp;#39;t want it to get in the way of actual models, but &lt;em&gt;I would highly encourage you to insert your own TensorBoard summary code&lt;/em&gt;, and monitor plots of convergence in TensorBoard, since it is useful both educationally and practically.&lt;/p&gt;
&lt;h1 id=&quot;challenge-problems-1&quot;&gt;Challenge Problems&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Experiment on your own with a few other built-in TensorFlow optimization algorithms, and try different learning rates. If you prefer a more focused goal, try to beat my configuration of an Adagrad optimizer with a learning rate of 50, and converge faster. Also note that some optimization algorithms have additional hyperparameters other than the learning rate. See the TensorFlow documentation for information about these.&lt;/li&gt;
&lt;li&gt;One other cause of slow convergence for the homicide rate linear regression is the somewhat extreme scaling of the problem. The \(y\) variable is a whole order of magnitude greater than the \(x\) variable, and this affects optimization. We will actually look at this problem specifically in chapter 2.4, but for now you can experiment on your own with one solution: instead of using the \(x\) and \(y\) data directly from the data set, modify them first to rescale them. A quick, hacky way is to modify the code that loads the data, so that \(x\) and \(y\) vary between 0 and 1:&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight python&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Load the data, and convert to 1x30 vectors&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;homicide.csv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# 21 and 50 are the min and max of x_data&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;age&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;21.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;50.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;21.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# 196 and 653 are the min and max of y_data&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_homicide_deaths&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;196.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;653.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;196.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;On your own, add this code and see if you can achieve convergence using only gradient descent. You can also see how quickly you can achieve convergence using a more advanced algorithm such as Adam.&lt;/p&gt;
&lt;h1 id=&quot;complete-code-1&quot;&gt;Complete Code&lt;/h1&gt;
&lt;p&gt;The complete code including TensorBoard summaries, and using Adagrad is &lt;a href=&quot;https://github.com/donald-pinckney/donald-pinckney.github.io/blob/src/books/tensorflow/src/ch2-linreg/code/single_var_reg_optim.py&quot;&gt;available on GitHub&lt;/a&gt; and directly below. Note that this code lacks the plotting of the data and the linear regression line:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight python&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;### Hyperparameters ###&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;LEARNING_RATE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;50.0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;OPTIMIZER_CONSTRUCTOR&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AdagradOptimizer&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;NUM_ITERS&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20000&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Load the data, and convert to 1x30 vectors&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;homicide.csv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;age&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_homicide_deaths&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;c&quot;&gt;### Model definition ###&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define x (input data) placeholder&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define the trainable variables&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;a&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;b&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define the prediction model&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_predicted&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;


&lt;span class=&quot;c&quot;&gt;### Loss function definition ###&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define y (correct data) placeholder&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define the loss function&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduce_sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_predicted&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;c&quot;&gt;### Summary setup ###&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;log_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;g, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LEARNING_RATE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OPTIMIZER_CONSTRUCTOR&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scalar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'a'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scalar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scalar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'L'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;summary_node&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;merge_all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;summary_writer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FileWriter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'/tmp/tensorflow/single_var_reg/'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Open /tmp/tensorflow/single_var_reg/ with tensorboard&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;c&quot;&gt;### Training the model ###&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define optimizer object&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OPTIMIZER_CONSTRUCTOR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LEARNING_RATE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minimize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Create a session and initialize variables&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;session&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Session&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;session&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;global_variables_initializer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Main optimization loop&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NUM_ITERS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;session&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;summary_node&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feed_dict&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_data&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;summary_writer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_summary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# print(&quot;t = %g, loss = %g, a = %g, b = %g&quot; % (t, current_loss, current_a, current_b))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content>
 </entry>
 
 <entry>
   <title>Single Variable Linear Regression</title>
   <link href="https://donaldpinckney.com/tensorflow/2017/12/03/single-variable.html"/>
   <updated>2017-12-03T00:00:00-08:00</updated>
   <id>https://donaldpinckney.com/tensorflow/2017/12/03/single-variable</id>
   <content type="html">&lt;h1 id=&quot;single-variable-regression&quot;&gt;Single Variable Regression&lt;/h1&gt;
&lt;p&gt;Since this is the very first tutorial in this guide and no knowledge is assumed about machine learning or TensorFlow, this tutorial is a bit on the long side. This tutorial will give you an overview of how to do machine learning work in general, a mathematical understanding of single variable linear regression, and how to implement it in TensorFlow. If you already feel comfortable with the mathematical concept of linear regression, feel free to skip to the TensorFlow &lt;a href=&quot;#implementation&quot;&gt;implementation&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;Single variable linear regression is one of the fundamental tools for any interpretation of data. Using linear regression, we can predict continuous variable outcomes given some data, if the data has a roughly linear shape, i.e. it generally has the shape a line. For example, consider the plot below of 2015 US homicide deaths per age&lt;sup id=&quot;fnref1&quot;&gt;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;, and the line of best fit next to it.&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align: center&quot;&gt;Original data&lt;/th&gt;
&lt;th style=&quot;text-align: center&quot;&gt;Result of single variable linear regression&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/books/tensorflow/book/ch2-linreg/assets/homicide.png&quot; alt=&quot;Homicide Plot&quot;&gt;&lt;/td&gt;
&lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/books/tensorflow/book/ch2-linreg/assets/homicide_fit.png&quot; alt=&quot;Homicide Regression Plot&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;p&gt;Visually, it appears that this data is approximated pretty well by a &amp;quot;line of best fit&amp;quot;. This is certainly not the only way to approximate this data, but for now it&amp;#39;s pretty good. Single variable linear regression is the tool to find this line of best fit. The line of best fit can then be used to guess how many homicide deaths there would be for ages we don&amp;#39;t have data on. By the end of this tutorial you can run linear regression on this homicide data, and in fact solve any single variable regression problem.&lt;/p&gt;
&lt;h2 id=&quot;theory&quot;&gt;Theory&lt;/h2&gt;
&lt;p&gt;Since we don&amp;#39;t have any theory yet to understand linear regression, first we need to develop the theory necessary to program it.&lt;/p&gt;
&lt;h3 id=&quot;data-set-format&quot;&gt;Data set format&lt;/h3&gt;
&lt;p&gt;For regression problems, the goal is to predict a continuous variable output, given some input variables (also called &lt;strong&gt;features&lt;/strong&gt;). For single variable regression, we only have one input variable, called \(x\), and our &lt;em&gt;desired&lt;/em&gt; output \(y\). Our data set \(D\) then consists of many examples of \(x\) and \(y\), so:
\[
    D = \{ (x_1, y_1), (x_2, y_2), \cdots, (x_m, y_m) \}
\]
where \(m\) is the number of examples in the data set. For a concrete example, the homicide data set plotted above looks like:
\[
    D = \{ (21, 652), (22, 633), \cdots, (50, 197) \}
\]
We will write code to load data sets from files later.&lt;/p&gt;
&lt;h3 id=&quot;model-concept&quot;&gt;Model concept&lt;/h3&gt;
&lt;p&gt;So, how can we mathematically model single linear regression? Since the goal is to find the perfect line, let&amp;#39;s start by defining the &lt;strong&gt;model&lt;/strong&gt; (the mathematical description of how predictions will be created) as a line:
\[
    y&amp;#39;(x, a, b) = ax + b
\]
where \(x\) is an input, \(a, b\) are constants, and \(y&amp;#39;\) is the prediction for the input \(x\). Note that although this is an equation for a line with \(x\) as the variable, the values of \(a\) and \(b\) determine what specific line it is. To find the best line, we just need to find the best values for \(a\) (the slope) and \(b\) (the y-intercept). For example, the line of best fit for the homicide data above has a slope of about \(a \approx -17.69\) and a y-intercept of \(b \approx 1000\). How we find the magic best values for \(a\) and \(b\) we don&amp;#39;t know yet, but once we find them, prediction is easy, since we just use the formula above.&lt;/p&gt;

&lt;p&gt;So, how do we find the correct values of \(a\) and \(b\)? First, we need a way to define what the &amp;quot;best line&amp;quot; is exactly. To do so, we define a &lt;strong&gt;loss function&lt;/strong&gt; (also called a cost function), which measures how bad a particular choice of \(a\) and \(b\) are. Values of \(a\) and \(b\) that seem poor (a line that does not fit the data set) should result in a large value of the loss function, whereas good values of \(a\) and \(b\) (a line that fits the data set well) should result in small values of the loss function. In other words, the loss function should measure how far the predicted line is from each of the data points, and add this value up for all data points. We can write this as:
\[
    L(a, b) = \sum_{i=1}^m (y&amp;#39;(x_i, a, b) - y_i)^2
\]
Recall that there are \(m\) examples in the data set, \(x_i\) is the i&amp;#39;th input, and \(y_i\) is the i&amp;#39;th desired output. So, \((y&amp;#39;(x_i, a, b) - y_i)^2\) measures how far the i&amp;#39;th prediction is from the i&amp;#39;th desired output. For example, if the prediction \(y&amp;#39;\) is 7, and the correct output \(y\) is 10, then we would get \((7 - 10)^2 = 9.\) Squaring it is important so that it is always positive.  Finally, we just add up all of these individual losses. Since the smallest possible values for the squared terms indicate that the line fits the data as closely as possible, the line of best fit (determined by the choice of \(a\) and \(b\)) occurs exactly at the smallest value of \(L(a, b)\). For this reason, the model is also called &lt;a href=&quot;https://en.wikipedia.org/wiki/Least_squares&quot;&gt;least squares regression&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The choice to square \(y&amp;#39;(x_i, a, b) - y_i\) is somewhat arbitrary. Though we need to make it positive, we could achieve this in many ways, such as taking the absolute value. In a sense, the choice of models and loss functions is one of the creative aspect of machine learning, and often a certain loss function is chosen simply because it produces satisfying results. Manipulating the loss function to achieve more satisfying results will be done in a later chapter.
However, there is also a concrete mathematical reason for why we specifically square it. It&amp;#39;s too much to go into here, but it will be explored in &lt;a href=&quot;/books/tensorflow/book/ch2-linreg/mle.html&quot;&gt;bonus chapter 2.8&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Creating loss functions (and this exact loss function) will continue to be used throughout this guide, from the most simple to more complex models.&lt;/p&gt;
&lt;h3 id=&quot;optimizing-the-model&quot;&gt;Optimizing the model&lt;/h3&gt;
&lt;p&gt;At this point, we have fully defined both our model:
\[
    y&amp;#39;(x, a, b) = ax + b
\]
and our loss function, into which we can substitute the model:
\[
    L(a, b) = \sum_{i=1}^m (y&amp;#39;(x_i, a, b) - y_i)^2 = \sum_{i=1}^m (a x_i + b - y_i)^2
\]
We crafted \(L(a, b)\) so that it is smallest exactly when each predicted \(y&amp;#39;\) is as close as possible to actual data \(y\). When this happens, since the distance between the data points and predicted line is as small as possible, using \(a\) and \(b\) produces the line of best fit. Therefore, our goal is to find the values of \(a\) and \(b\) that minimize the function \(L(a, b)\). But what does \(L\) really look like? Well, it is essentially a 3D parabola which looks like:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/books/tensorflow/book/ch2-linreg/assets/minimum.png&quot; alt=&quot;Minimum Plot&quot;&gt;&lt;/p&gt;

&lt;p&gt;The red dot marked on the plot of \(L\) shows where the desired minimum is. We need an algorithm to find this minimum. From calculus, we know that at the minimum \(L\) must be entirely flat, that is the derivatives are both \(0\):
\[
    \frac{\partial L}{\partial a} = \sum_{i=1}^m 2(ax_i + b - y_i)x_i = 0 \\
    \frac{\partial L}{\partial b} = \sum_{i=1}^m 2(ax_i + b - y_i) = 0 \
\]
If you need to review this aspect of calculus, I would recommend &lt;a href=&quot;https://www.khanacademy.org/math/differential-calculus/analyzing-func-with-calc-dc&quot;&gt;Khan Academy videos&lt;/a&gt;. Now, for this problem it is possible to solve for \(a\) and \(b\) using the equations above, like we would in a typical calculus course. But for more advanced machine learning this is impossible, so instead we will learn to use an algorithm called &lt;strong&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Gradient_descent&quot;&gt;gradient descent&lt;/a&gt;&lt;/strong&gt; to find the minimum. The idea is intuitive: place a ball at an arbitrary location on the surface of \(L\), and it will naturally roll downhill towards the flat valley of \(L\) and thus find the minimum. We know the direction of &amp;quot;downhill&amp;quot; at any location since we know the derivatives of \(L\): the derivatives are the direction of greatest upward slope (this is known as the &lt;a href=&quot;https://en.wikipedia.org/wiki/Gradient&quot;&gt;gradient&lt;/a&gt;), so the opposite (negative) derivatives are the most downhill direction. Therefore, if the ball is currently at location \((a, b)\), we can see where it would go by moving it to location \((a&amp;#39;, b&amp;#39;)\) like so:
\[
    a&amp;#39; = a - \alpha \frac{\partial L}{\partial a} \\
    b&amp;#39; = b - \alpha \frac{\partial L}{\partial b} \\
\]
where \(\alpha\) is a constant called the &lt;strong&gt;learning rate&lt;/strong&gt;, which we will talk about more later. If we repeat this process then the ball will continue to roll downhill into the minimum. An animation of this process looks like:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/books/tensorflow/book/ch2-linreg/assets/descent_fast.gif&quot; alt=&quot;Gradient Descent Animation&quot;&gt;&lt;/p&gt;

&lt;p&gt;When we run the gradient descent algorithm for long enough, then it will find the optimal location for \((a, b)\). Once we have the optimal values of \(a\) and \(b\), then that&amp;#39;s it, we can just use them to predict a rate of homicide deaths given any age, using the model:
\[
    y&amp;#39;(x) = ax + b
\]&lt;/p&gt;
&lt;h2 id=&quot;implementation&quot;&gt;Implementation&lt;/h2&gt;
&lt;p&gt;Let&amp;#39;s quickly review what we did when defining the theory of linear regression:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Describe the data set&lt;/li&gt;
&lt;li&gt;Define the model&lt;/li&gt;
&lt;li&gt;Define the loss function&lt;/li&gt;
&lt;li&gt;Run the gradient descent optimization algorithm&lt;/li&gt;
&lt;li&gt;Use the optimal model to make predictions&lt;/li&gt;
&lt;li&gt;Profit!&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;When coding this we will follow the exact same steps. So, create a new file &lt;code&gt;single_var_reg.py&lt;/code&gt; in the text editor or IDE of your choice (or experiment in the Python REPL by typing &lt;code&gt;python&lt;/code&gt; at command line), and download the &lt;a href=&quot;/books/tensorflow/book/ch2-linreg/code/homicide.csv&quot;&gt;homicide death rate data set&lt;/a&gt; into the same directory.&lt;/p&gt;
&lt;h3 id=&quot;importing-the-data&quot;&gt;Importing the data&lt;/h3&gt;
&lt;p&gt;First, we need to import all the modules we will need:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight python&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We use Pandas to easily load the CSV homicide data:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight python&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;homicide.csv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;age&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_homicide_deaths&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that &lt;code&gt;x_data&lt;/code&gt; and &lt;code&gt;y_data&lt;/code&gt; are &lt;em&gt;not&lt;/em&gt; single numbers, but are actually &lt;a href=&quot;https://en.wikipedia.org/wiki/Vector_space&quot;&gt;vectors&lt;/a&gt;. The vectors are 30 numbers long, since there are 30 data points in the CSV file. So, &lt;code&gt;(x_data[0], y_data[0])&lt;/code&gt; would be \((x_1, y_1) = (21, 652)\). When we look at multi variable regression later, we will have to work much more with vectors, matrices and linear algebra, but for now you can think of &lt;code&gt;x_data&lt;/code&gt; and &lt;code&gt;y_data&lt;/code&gt; just as lists of numbers. Also, we have to use &lt;code&gt;np.matrix(...)&lt;/code&gt; to convert the array of numbers &lt;code&gt;D.age.values&lt;/code&gt; to an actual numpy vector (likewise for &lt;code&gt;D.num_homicide_deaths.values&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;Whenever possible, I would recommend plotting data, since this helps you verify that you loaded the data set correctly and gain visual intuition about the shape of the data. This is also pretty easy using matplotlib:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight python&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'x'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# The 'x' means that data points will be marked with an x&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Age'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'US Homicide Deaths in 2015'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Relationship between age and homicide deaths in the US'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;When we converted the data to vectors using &lt;code&gt;np.matrix()&lt;/code&gt;, numpy created vectors with the shape 1 x 30. That is, &lt;code&gt;x_data&lt;/code&gt; consists of only 1 row of numbers, and 30 columns. This is actually great for us when working with TensorFlow, but matplotlib wants vectors that have the shape 30 x 1 (30 rows and 1 column). Writing &lt;code&gt;x_data.T&lt;/code&gt; calculates the &lt;a href=&quot;https://en.wikipedia.org/wiki/Transpose&quot;&gt;transpose&lt;/a&gt; of &lt;code&gt;x_data&lt;/code&gt;, which flips it from a 1 x 30 vector to a 30 x 1 vector. It&amp;#39;s fine if you don&amp;#39;t understand this now, as we will learn more linear algebra later. Anyways, the plot should look like this:
&lt;img src=&quot;/books/tensorflow/book/ch2-linreg/assets/homicide.png&quot; alt=&quot;Homicide Plot&quot;&gt;&lt;/p&gt;

&lt;p&gt;You need to close the plot for your code to continue executing.&lt;/p&gt;
&lt;h3 id=&quot;defining-the-model&quot;&gt;Defining the model&lt;/h3&gt;
&lt;p&gt;We have our data prepared and plotted, so now we need to define our model. Recall that the model equation is:
\[
    y&amp;#39; = ax + b
\]
Before, we thought of \(x\) and \(y&amp;#39;\) as single numbers. However, we just loaded our data set as vectors (lists of numbers), so it will be much more convenient to define our model using vectors instead of single numbers. If we use the convention that \(x\) and \(y&amp;#39;\) are vectors, then we don&amp;#39;t need to change the equation, just our interpretation of it. Multiplying the vector \(x\) by the single number \(a\) just multiplies every number in \(x\) by \(a\), and likewise for adding \(b\). So, the above equation interpreted using vectors is the same thing as:
\[
    \begin{bmatrix}
           y_{1}&amp;#39;, &amp;amp;
           y_{2}&amp;#39;, &amp;amp;
           \dots, &amp;amp;
           y_{m}&amp;#39;
    \end{bmatrix} = \begin{bmatrix}
           ax_{1} + b, &amp;amp;
           ax_{2} + b, &amp;amp;
           \dots, &amp;amp;
           ax_{m} + b
         \end{bmatrix}
\]&lt;/p&gt;

&lt;p&gt;Fortunately, TensorFlow does the work for us of interpreting the simple equation \(y&amp;#39; = ax + b\) as the more complicated looking vector equation. We just have to tell TensorFlow which things are vectors (\(x\) and \(y&amp;#39;\)), and which are not vectors (\(a\) and \(b\)). First, we define \(x\):&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight python&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This says that we create a &lt;strong&gt;placeholder&lt;/strong&gt; that stores floating-point numbers, and has a &lt;strong&gt;shape&lt;/strong&gt; of 1 x None. The shape of 1 x None tells TensorFlow that \(x\) is a vector with 1 row, and some unspecified number of columns. Although we don&amp;#39;t tell TensorFlow the number of columns, this is enough to tell TensorFlow that \(x\) is a vector.&lt;/p&gt;

&lt;p&gt;Secondly, note that we create a &lt;code&gt;tf.placeholder&lt;/code&gt;: &lt;code&gt;x&lt;/code&gt; does not have a numerical value right now. Instead, we will later feed the values of &lt;code&gt;x_data&lt;/code&gt; into &lt;code&gt;x&lt;/code&gt;. In short, use a &lt;code&gt;tf.placeholder&lt;/code&gt; whenever there are values you wish to fill in later (usually data).&lt;/p&gt;

&lt;p&gt;Now, we define \(a\) and \(b\):&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight python&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;a&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;b&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Unlike &lt;code&gt;x&lt;/code&gt;, we create &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; to be a &lt;strong&gt;variable&lt;/strong&gt;, instead of a placeholder. The main difference between a variable and a placeholder is that TensorFlow will automatically find the best values of variables by using gradient descent (later). In other words, a placeholder changes values whenever we choose to feed it different numeric values. A variable changes values continually and automatically during gradient descent. Use a variable for something that is &lt;strong&gt;trainable&lt;/strong&gt;, that is, something whose optimal value will be found by an algorithm such as gradient descent.  Since the goal of linear regression is to find the best values of \(a\) and \(b\), the (only) TensorFlow variables in our model are &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;. The conceptual difference between a TensorFlow placeholder and variable is crucial to using TensorFlow properly.&lt;/p&gt;

&lt;p&gt;The parameters &lt;code&gt;(&amp;quot;a&amp;quot;, shape=())&lt;/code&gt; indicate the name of the variable, and that &lt;code&gt;a&lt;/code&gt; is a single number, &lt;em&gt;not&lt;/em&gt; a vector. In comparison to &lt;code&gt;x&lt;/code&gt;, note that a shape of &lt;code&gt;(1, None)&lt;/code&gt; indicates a vector, while a shape of &lt;code&gt;()&lt;/code&gt; indicates a single number.&lt;/p&gt;

&lt;p&gt;With &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;a&lt;/code&gt; and  &lt;code&gt;b&lt;/code&gt; defined, we can define \(y&amp;#39;\):&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight python&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;y_predicted&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, &lt;code&gt;y_predicted&lt;/code&gt; is not a TensorFlow variable, since it is not directly trainable, nor is it a TensorFlow placeholder, since we will not directly feed values into it. More generically, we call &lt;code&gt;y_predicted&lt;/code&gt; a TensorFlow &lt;strong&gt;node&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;And that&amp;#39;s it to define the model!&lt;/p&gt;
&lt;h3 id=&quot;defining-the-loss-function&quot;&gt;Defining the loss function&lt;/h3&gt;
&lt;p&gt;We have the model defined, so now we need to define the loss function. Recall that the loss function is how the model is evaluated (smaller loss values are better), and it is also the function that we need to minimize in terms of \(a\) and \(b\).  Since the loss function compares the model&amp;#39;s output \(y&amp;#39;\) to the correct output \(y\) from the data set, we need to define \(y\) in TensorFlow. Since \(y\) consists of outside data (and we don&amp;#39;t need to train it), we create it as a &lt;code&gt;tf.placeholder&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight python&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Like &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt; is also a vector, since after all &lt;code&gt;y&lt;/code&gt; must store the correct output for each value stored in &lt;code&gt;x&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Now, we are ready to setup the loss function. Recall that the loss function is:
\[
    L(a, b) = \sum_{i=1}^m (y&amp;#39;(x_i, a, b) - y_i)^2
\]&lt;/p&gt;

&lt;p&gt;However, \(y&amp;#39;\) and \(y\) are now being interpreted as vectors. We can rewrite the loss function as:
\[
    L(a, b) = \mathrm{sum}((y&amp;#39; - y)^2)
\]
Note that since \(y&amp;#39;\) and \(y\) are vectors, \(y&amp;#39; - y\) is also a vector that just contains every number stored in \(y&amp;#39;\) minus every corresponding number in \(y&amp;#39;\). Likewise, \((y&amp;#39; - y)^2\) is also a vector, with every number individually squared.  Then, the \(\mathrm{sum}\) function (which I just made up) adds up every number stored in the vector \((y&amp;#39; - y)^2\). This is the same as the original loss function, but is a vector interpretation of it instead. We can code this directly:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight python&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduce_sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_predicted&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The &lt;code&gt;tf.reduce_sum&lt;/code&gt; function is an operation which adds up all the numbers stored in a vector. It is called &amp;quot;reduce&amp;quot; since it reduces a large vector down to a single number (the sum). The word &amp;quot;reduce&amp;quot; here has nothing to do with the fact that we will minimize the loss function.&lt;/p&gt;

&lt;p&gt;With just these two lines of code we have defined our loss function.&lt;/p&gt;
&lt;h3 id=&quot;minimizing-the-loss-function-with-gradient-descent&quot;&gt;Minimizing the loss function with gradient descent&lt;/h3&gt;
&lt;p&gt;With our model and loss function defined, we are now ready to use the gradient descent algorithm to minimize the loss function, and thus find the optimal \(a\) and \(b\). Fortunately, TensorFlow as already implemented the gradient descent algorithm for us, we just need to use it. The algorithm acts almost like a ball rolling downhill into the minimum of the function, but it does so in discrete time steps. TensorFlow does not handle this aspect, we need to be responsible for performing each time step of gradient descent. So, roughly in pseudo-code we want to do this:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight python&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# Tell TensorFlow to do 1 time step of gradient descent&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We can&amp;#39;t do this yet, since we don&amp;#39;t yet have a way to tell TensorFlow to perform 1 time step of gradient descent. To do so, we create an optimizer with a learning rate (\(\alpha)\) of \(0.2\):&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight python&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AdamOptimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minimize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The &lt;code&gt;tf.train.AdamOptimizer&lt;/code&gt; knows how to perform the gradient descent algorithm for us (actually a faster version of gradient descent). Note that this &lt;em&gt;does not yet minimize \(L\)&lt;/em&gt;. This code only create an optimizer object which we will use later to minimize \(L\). For an explanation of the &lt;code&gt;learning_rate=0.2&lt;/code&gt; parameter (and the &lt;code&gt;10000&lt;/code&gt; loop iterations), see the end of this tutorial.&lt;/p&gt;

&lt;p&gt;The second problem we have is we don&amp;#39;t know how to make TensorFlow run actual computations. Everything so far has been only &lt;em&gt;defining&lt;/em&gt; things for TensorFlow, not computing things with concrete numbers. To do so, we also need to create a &lt;strong&gt;session&lt;/strong&gt;:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight python&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;session&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Session&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;A TensorFlow session is how we always have to perform actual computations with TensorFlow. We actually need to perform a computation right now, before doing gradient descent. Previously, we defined the variables &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;, but they don&amp;#39;t have any numeric value right now. They need to have some initial value so gradient descent can work. To solve this, we have TensorFlow initialize &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; with random values:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight python&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;session&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;global_variables_initializer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The &lt;code&gt;session.run&lt;/code&gt; function is how we always have to run computations with TensorFlow: the parameter is what computation we want to perform.&lt;/p&gt;

&lt;p&gt;Finally, we are ready to run the optimization loop pseudo-code that we originally wanted. Using &lt;code&gt;session.run&lt;/code&gt; it looks like:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight python&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;session&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feed_dict&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_data&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;t = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;g, loss = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;g, a = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;g, b = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;g&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Let&amp;#39;s break this down. We use &lt;code&gt;session.run&lt;/code&gt;, but we pass it an array of computations that we want to perform. Specifically, we want to perform 4 computations:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;code&gt;optimizer&lt;/code&gt;: performs 1 time step of gradient descent, and updates &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;L&lt;/code&gt;: returns the current value of the loss function&lt;/li&gt;
&lt;li&gt;&lt;code&gt;a&lt;/code&gt;: returns the current value of &lt;code&gt;a&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;b&lt;/code&gt;: likewise returns the current value of &lt;code&gt;b&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Of these computations, only the &lt;code&gt;optimizer&lt;/code&gt; does not return a value. So, &lt;code&gt;session.run&lt;/code&gt; will return 3 values for us, which we store into variables using the syntax:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight python&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;session&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Ok, but what is all of this &lt;code&gt;feed_dict&lt;/code&gt; stuff? Recall that &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are placeholders, and have no actual numerical value on their own. To perform 1 time step of gradient descent, we need to &amp;quot;feed&amp;quot; our actual data (&lt;code&gt;x_data&lt;/code&gt; and &lt;code&gt;y_data&lt;/code&gt;) into the &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; placeholders. So, we use the &lt;code&gt;feed_dict&lt;/code&gt; parameter of &lt;code&gt;session.run&lt;/code&gt; to feed &lt;code&gt;x_data&lt;/code&gt; into &lt;code&gt;x&lt;/code&gt;, and &lt;code&gt;y_data&lt;/code&gt; into &lt;code&gt;y&lt;/code&gt;, by means of a dictionary.&lt;/p&gt;

&lt;p&gt;Finally, the last line of the loop prints out the current values of &lt;code&gt;t&lt;/code&gt;, &lt;code&gt;L&lt;/code&gt;, &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;. We don&amp;#39;t need to print these values out, but it is helpful to observe how the training is progressing.&lt;/p&gt;

&lt;p&gt;What we want to see from the print statements is that the gradient descent algorithm &lt;strong&gt;converged&lt;/strong&gt;, which means that the algorithm stopped making significant progress because it found the minimum location of the loss function. When the last few print outputs look like:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight plaintext&quot;&gt;&lt;code&gt;t = 9992, loss = 39295.8, a = -17.271, b = 997.281
t = 9993, loss = 39295.8, a = -17.271, b = 997.282
t = 9994, loss = 39295.9, a = -17.271, b = 997.282
t = 9995, loss = 39295.9, a = -17.271, b = 997.283
t = 9996, loss = 39295.8, a = -17.2711, b = 997.283
t = 9997, loss = 39295.8, a = -17.2711, b = 997.284
t = 9998, loss = 39295.9, a = -17.2711, b = 997.284
t = 9999, loss = 39295.8, a = -17.2711, b = 997.285
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;then we can tell that we have achieved convergence, and therefore found the best values of \(a\) and \(b\).&lt;/p&gt;
&lt;h3 id=&quot;using-the-trained-model-to-make-predictions&quot;&gt;Using the trained model to make predictions&lt;/h3&gt;
&lt;p&gt;At this point we have a fully trained model, and know the best values of \(a\) and \(b\). In fact, the equation of the line of best fit is just:
\[
    y&amp;#39; = -17.2711x + 997.285
\]&lt;/p&gt;

&lt;p&gt;The last remaining thing for this tutorial is to plot the predictions of the model on top of a plot of the data. First, we need to create a bunch of input ages that we will predict the homicide rates for. We could use &lt;code&gt;x_data&lt;/code&gt; as the input ages, but it is more interesting to create a new vector of input ages, since then we can predict homicide rates even for ages that were not in the data set. Outside of the training &lt;code&gt;for&lt;/code&gt; loop, we can use the numpy function &lt;code&gt;linspace&lt;/code&gt; to create a bunch of evenly spaced values between 20 and 55:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight python&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# x_test_data has values similar to [20.0, 20.1, 20.2, ..., 54.9, 55.0]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x_test_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;55&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then, we can feed &lt;code&gt;x_test_data&lt;/code&gt; into the &lt;code&gt;x&lt;/code&gt; placeholder, and save the outputs of the prediction in &lt;code&gt;y_test_data&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight python&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;y_test_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;session&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_predicted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feed_dict&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_test_data&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Finally, we can plot the original data and the line together:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight python&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'x'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_test_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Age'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'US Homicide Deaths in 2015'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Age and homicide death linear regression'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This yields a plot like:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/books/tensorflow/book/ch2-linreg/assets/homicide_fit.png&quot; alt=&quot;Homicide Linear Regression Plot&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;concluding-remarks&quot;&gt;Concluding Remarks&lt;/h1&gt;
&lt;p&gt;Through following this post you have learned two main concepts. First, you learned the &lt;em&gt;general form of supervised machine learning workflows&lt;/em&gt;:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Get your data set&lt;/li&gt;
&lt;li&gt;Define your model (the mechanism for how outputs will be predicted from inputs)&lt;/li&gt;
&lt;li&gt;Define your loss function&lt;/li&gt;
&lt;li&gt;Minimize your loss function (usually with a variant of gradient descent, such as &lt;code&gt;tf.train.AdamOptimizer&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Once your loss function is minimized, use your trained model to do cool stuff&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Second, you learned how to implement linear regression (following the above workflow) using TensorFlow. Let&amp;#39;s briefly discuss the above 5 steps, and where to go to improve on them.&lt;/p&gt;
&lt;h2 id=&quot;the-data-set&quot;&gt;1. The Data Set&lt;/h2&gt;
&lt;p&gt;This one is pretty simple: we need data sets that contain both input and output data. However, we need a data set that is large enough to properly train our model. With linear regression this is fairly easy: this data set only had 33 data points, and the results were pretty good. With larger and more complex models that we will look at later, this becomes much more of a challenge.&lt;/p&gt;
&lt;h2 id=&quot;defining-the-model-1&quot;&gt;2. Defining the model&lt;/h2&gt;
&lt;p&gt;For single variable linear regression we used the model \(y&amp;#39; = ax + b\). Geometrically, this means that the model can only guess lines. Since the homicide data is roughly in the shape of a line, it worked well for this problem. But there are very few problems that are so simple, so soon we will look at more complex models. One other limitation of the current model is it only accepts one input variable. But if our data set had both age and ethnicity, for example, perhaps we could more accurately predict homicide death rate. We will also discuss soon a more complex model that handles multiple input variables.&lt;/p&gt;
&lt;h2 id=&quot;defining-the-loss-function-1&quot;&gt;3. Defining the loss function&lt;/h2&gt;
&lt;p&gt;For single variable regression, the loss function we used, \(L = \sum (y&amp;#39; - y)^2\), is the standard. However, there are a few considerations: first, this loss functions is suitable for this simple model, but with more advanced models this loss function isn&amp;#39;t good enough. We will see why soon. Second, the optimization algorithm converged pretty slowly, needing about \(10000\) iterations. One cause is that the surface of the loss function is almost flat in a certain direction (you can see this in the 3D plot of it). Though this isn&amp;#39;t inherently a problem with the formula for the loss function, the problem surfaces in the loss function. We will also see how to address this problem soon, and converge muster faster.&lt;/p&gt;
&lt;h2 id=&quot;minimizing-the-loss-functions&quot;&gt;4. Minimizing the loss functions&lt;/h2&gt;
&lt;p&gt;Recall that we created and used the optimizer like so:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight python&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AdamOptimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minimize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# Run one step of optimizer&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You might be wondering what the magic numbers of &lt;code&gt;learning_rate=0.2&lt;/code&gt; (\( \alpha \)) and &lt;code&gt;10000&lt;/code&gt; are.  Let&amp;#39;s start with the learning rate. In each iteration, gradient descent (and variants of it) take one small step that is determined by the derivative of the loss function. The learning rate is just the relative size of the step. So to take larger steps, we can use a larger learning rate. A larger learning rate can help us to converge more quickly, since we cover more distance in each iteration. But a learning rate too large can cause gradient descent to diverge that is, it won&amp;#39;t reliably find the minimum.&lt;/p&gt;

&lt;p&gt;So, once you have chosen a learning rate, then you need to run the optimizer for enough iterations so it actually converges to the minimum. The easiest way to make sure it runs long enough is just to monitor the value of the loss function, as we did in this tutorial.&lt;/p&gt;

&lt;p&gt;Lastly, we didn&amp;#39;t use normal gradient descent for optimization in this tutorial. Instead we used &lt;code&gt;tf.train.AdamOptimizer&lt;/code&gt;. With small scale problems like this, there isn&amp;#39;t much of a qualitative difference to intuit. In general the &lt;a href=&quot;https://medium.com/@nishantnikhil/adam-optimizer-notes-ddac4fd7218&quot;&gt;Adam optimizer&lt;/a&gt; is faster, smarter, and more reliable than vanilla gradient descent, but this comes into play a lot more with harder problems.&lt;/p&gt;
&lt;h2 id=&quot;use-the-trained-model&quot;&gt;5. Use the trained model&lt;/h2&gt;
&lt;p&gt;Technically, using the trained model is the easiest part of machine learning: with the best parameters \(a\) and \(b\), you can simply plug new age values into \(x\) to predict new homicide rates. However, trusting that these predictions are correct is another matter entirely. Later in this guide we can look at various statistical techniques that can help determine how much we can trust a trained model, but for now consider some oddities with our trained homicide rate model.&lt;/p&gt;

&lt;p&gt;One rather weird thing is that it accepts negative ages: according to the model, 1083 people who are -5 years old die from homicide every year in the US. Now, clearly this makes no sense since people don&amp;#39;t have negative ages. So perhaps we should only let the model be valid for people with positives ages. Ok, so then 980 people who are 1 year old die from homicide every year. While this isn&amp;#39;t impossible, it does seem pretty high compared to the known data of 652 for 21 year olds. It might seem possible (likely even) that fewer homicides occur for 1 year olds than 21 year olds: but we don&amp;#39;t have the data for that, and even if we did, our model could not predict it correctly since it only models straight lines. Without more data, we have no basis to conclude that the number of \(1\) year old homicides is even close to 980.&lt;/p&gt;

&lt;p&gt;While this might seem like a simple observation in this case, this problem manifests itself continually in machine learning, causing a variety of ethical problems. For example, in 2016 Microsoft released a chatbot on Twitter and &lt;a href=&quot;https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist&quot;&gt;it quickly learned to say fairly horrible and racist things&lt;/a&gt;. More seriously, machine learning is now being used to predict and guide police in cracking down on crime. While the concept might be well-intentioned, the results are despicable, as shown in &lt;a href=&quot;https://theconversation.com/why-big-data-analysis-of-police-activity-is-inherently-biased-72640&quot;&gt;an article by The Conversation&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Our recent study, by Human Rights Data Analysis Group’s Kristian Lum and William Isaac, found that predictive policing vendor PredPol’s purportedly race-neutral algorithm targeted black neighborhoods at roughly twice the rate of white neighborhoods when trained on historical drug crime data from Oakland, California.
[...]
But estimates – created from public health surveys and population models – suggest illicit drug use in Oakland is roughly equal across racial and income groups. If the algorithm were truly race-neutral, it would spread drug-fighting police attention evenly across the city.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;With examples like these, we quickly move from a technical discussion about machine learning to a discussion about ethics. While the study of machine learning is traditionally heavily theoretical, I strongly believe that to effectively and &lt;em&gt;fairly&lt;/em&gt; apply machine learning in society, we must spend significant effort evaluating the ethics of machine learning models.&lt;/p&gt;

&lt;p&gt;This is an open question, and one that I certainly don&amp;#39;t have an answer to right now. For the short term we can focus on the problem of not trusting a simple linear regression model to properly predict data outside of what it has been trained on, while in the long term keeping in mind that &amp;quot;with great power comes great responsibility.&amp;quot;&lt;/p&gt;
&lt;h1 id=&quot;challenge-problems&quot;&gt;Challenge Problems&lt;/h1&gt;
&lt;p&gt;Feel free to complete as many of these as you wish, to get more practice with single variable linear regression. Note that for different problems you might have to adjust the learning rate and / or the number of training iterations.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Learn how to use numpy to generate an artificial data set that is appropriate for single variable linear regression, and then train a model on it. As a hint, for any \(x\) value you could create an artificial \(y\) value like so: \(y = ax + b + \epsilon \), where \(\epsilon\) is a random number that isn&amp;#39;t too big, and \(a\) and \(b\) are fixed constants of your choice. If done correctly, your trained model should learn by itself the numbers you chose for \(a\) and \(b\).&lt;/li&gt;
&lt;li&gt;Run single variable linear regression on a data set of your choice. You can look at &lt;a href=&quot;https://donaldpinckney.com/ml.html#regression&quot;&gt;my list of regression data sets&lt;/a&gt; for ideas, you can search &lt;a href=&quot;https://www.kaggle.com/datasets&quot;&gt;Kaggle&lt;/a&gt;, or you can search online, such as I did for the homicide data set. Many data sets might have multiple input variables, and right now you only know how to do single variable linear regression. We will deal with multiple variables soon, but for now you can always use only 1 of the input variables and ignore the rest.&lt;/li&gt;
&lt;li&gt;Experiment with altering the loss function, and observe the effects on the trained model. For example, you could change \((y&amp;#39; - y)^2\) to \(\mid y&amp;#39; - y \mid \) (use &lt;code&gt;tf.abs(...)&lt;/code&gt;), or really anything you can think of.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&quot;complete-code&quot;&gt;Complete Code&lt;/h1&gt;
&lt;p&gt;The &lt;a href=&quot;https://github.com/donald-pinckney/donald-pinckney.github.io/blob/src/books/tensorflow/src/ch2-linreg/code/single_var_reg.py&quot;&gt;complete example code is available on GitHub&lt;/a&gt;, as well as directly here:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight python&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Load the data, and convert to 1x30 vectors&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;homicide.csv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;age&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_homicide_deaths&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Plot the data&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'x'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Age'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'US Homicide Deaths in 2015'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Relationship between age and homicide deaths in the US'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;



&lt;span class=&quot;c&quot;&gt;### Model definition ###&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define x (input data) placeholder&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define the trainable variables&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;a&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;b&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define the prediction model&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_predicted&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;


&lt;span class=&quot;c&quot;&gt;### Loss function definition ###&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define y (correct data) placeholder&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define the loss function&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduce_sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_predicted&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;c&quot;&gt;### Training the model ###&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define optimizer object&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AdamOptimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minimize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Create a session and initialize variables&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;session&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Session&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;session&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;global_variables_initializer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Main optimization loop&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;session&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feed_dict&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_data&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;t = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;g, loss = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;g, a = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;g, b = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;g&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;


&lt;span class=&quot;c&quot;&gt;### Using the trained model to make predictions ###&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# x_test_data has values similar to [20.0, 20.1, 20.2, ..., 79.9, 80.0]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x_test_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;55&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Predict the homicide rate for each age in x_test_data&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_test_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;session&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_predicted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feed_dict&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_test_data&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Plot the original data and the prediction line&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'x'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_test_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Age'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'US Homicide Deaths in 2015'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Age and homicide death linear regression'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;div class=&quot;footnotes&quot;&gt;
&lt;hr&gt;
&lt;ol&gt;

&lt;li id=&quot;fn1&quot;&gt;
&lt;p&gt;Centers for Disease Control and Prevention, National Center for Health Statistics. Underlying Cause of Death 1999-2015 on CDC WONDER Online Database, released December, 2016. Data are from the Multiple Cause of Death Files, 1999-2015, as compiled from data provided by the 57 vital statistics jurisdictions through the Vital Statistics Cooperative Program. Accessed at &lt;a href=&quot;https://wonder.cdc.gov/ucd-icd10.html&quot;&gt;https://wonder.cdc.gov/ucd-icd10.html&lt;/a&gt; on Nov 22, 2017 2:18:46 PM.&amp;nbsp;&lt;a href=&quot;#fnref1&quot; rev=&quot;footnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;

&lt;/ol&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>ML Resources</title>
   <link href="https://donaldpinckney.com/machine%20learning/2017/01/01/ml-resources.html"/>
   <updated>2017-01-01T00:00:00-08:00</updated>
   <id>https://donaldpinckney.com/machine%20learning/2017/01/01/ml-resources</id>
   <content type="html">&lt;p&gt;Easily the best way to improve at machine learning is to just get practice at it. Practice helps you improve at machine learning in two ways:
1. You get familiar with implementing common algorithms. Whether you use machine learning libraries or roll your own, becoming comfortable with these algorithms is critical.
2. By working with a variety of data sets, you can also gain an intuition of how data is structured, so you know when to apply what algorithms.&lt;/p&gt;

&lt;p&gt;So, the most effective way to gain experience at machine learning is to experience implementing a &lt;em&gt;variety&lt;/em&gt; of algorithms over &lt;em&gt;differently behaved&lt;/em&gt; data sets.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve consolidated the machine learning resources I am familiar with, both from school and outside, into a list below. I&amp;#39;ve marked each data set with what model(s) I used for the data set, but please experiment with any algorithm that interests you!&lt;/p&gt;
&lt;h1 id=&quot;regression&quot;&gt;Regression&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Predict food truck profit from city population. Download: &lt;a href=&quot;public/files/ml_data/population_profit.csv&quot;&gt;population_profit.csv&lt;/a&gt;. Dataset from Andrew Ng&amp;#39;s Coursera course. Models I used: linear regression.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Predict house prices from area and number of bedrooms. Download: &lt;a href=&quot;public/files/ml_data/area_bedrooms_price.csv&quot;&gt;area_bedrooms_price.csv&lt;/a&gt;. Dataset from Andrew Ng&amp;#39;s Coursera course. Models I used: linear regression.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Predict car MPG from 7 variables. Download: &lt;a href=&quot;public/files/ml_data/auto_data.csv&quot;&gt;auto_data.csv&lt;/a&gt;. Dataset from &lt;a href=&quot;https://archive.ics.uci.edu/ml/datasets/Auto+MPG&quot;&gt;UCI repository&lt;/a&gt;{:target=&amp;quot;_blank&amp;quot;}. Models I used: linear regression.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Predict E. coli bacterial growth rate from many factors. For this, use only the gene expressions as attributes. Download: &lt;a href=&quot;public/files/ml_data/ecoli_data.zip&quot;&gt;ecoli_data.zip&lt;/a&gt;. Dataset from &lt;a href=&quot;https://faculty.engineering.ucdavis.edu/tagkopoulos/&quot;&gt;Ilias Tagkopoulos&lt;/a&gt;{:target=&amp;quot;_blank&amp;quot;} at UC Davis. Models I used: linear regression with regularization.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;classification&quot;&gt;Classification&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Note: For any supervised classification problem, you can also use it to practice unsupervised clustering by ignoring the class labels.&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Predict college admission. Download: &lt;a href=&quot;public/files/ml_data/score1_score2_admit.csv&quot;&gt;score1_score2_admit.csv&lt;/a&gt;. Dataset from Andrew Ng&amp;#39;s Coursera course. Models I used: logistic regression.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Predict microchip acceptance. Download: &lt;a href=&quot;public/files/ml_data/score1_score2_accept.csv&quot;&gt;score1_score2_accept.csv&lt;/a&gt;. Dataset from Andrew Ng&amp;#39;s Coursera course. Models I used: logistic regression.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Classify handwritten 0-9 digit. Download: &lt;a href=&quot;public/files/ml_data/MNIST_classification.zip&quot;&gt;MNIST_classification.zip&lt;/a&gt;. Dataset from Andrew Ng&amp;#39;s Coursera course. Models I used: logistic regression, fully-connected neural network with 1 hidden layer.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Classify yeast protein localization site based on 8 features. Download: &lt;a href=&quot;public/files/ml_data/yeast_data.csv&quot;&gt;yeast_data.csv&lt;/a&gt;. Dataset from &lt;a href=&quot;https://archive.ics.uci.edu/ml/datasets/Yeast&quot;&gt;UCI repository&lt;/a&gt;{:target=&amp;quot;_blank&amp;quot;}. Models I used: fully-connected neural network with various numbers of layers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Classify E. coli bacterial characteristics. Classify the strain type, medium type, environmental type and gene perturbation. Download: &lt;a href=&quot;public/files/ml_data/ecoli_data.zip&quot;&gt;ecoli_data.zip&lt;/a&gt;. Dataset from &lt;a href=&quot;https://faculty.engineering.ucdavis.edu/tagkopoulos/&quot;&gt;Ilias Tagkopoulos&lt;/a&gt;{:target=&amp;quot;_blank&amp;quot;} at UC Davis. Models I used: SVM.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;unsupervised-learning&quot;&gt;Unsupervised Learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Simple synthetic datasets to practice k-means clustering. Download: &lt;a href=&quot;public/files/ml_data/kmeans_simple.zip&quot;&gt;kmeans_simple.zip&lt;/a&gt;. Dataset from &lt;a href=&quot;https://www.math.ucdavis.edu/%7Estrohmer/&quot;&gt;Thomas Strohmer&lt;/a&gt;{:target=&amp;quot;_blank&amp;quot;} at UC Davis. Models I used: k-means clustering with BIC and AIC.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Iris species clustering: try to cluster irises into species based on 4 features. Download: &lt;a href=&quot;public/files/ml_data/iris.csv&quot;&gt;iris.csv&lt;/a&gt;. Dataset from &lt;a href=&quot;https://archive.ics.uci.edu/ml/datasets/iris&quot;&gt;UCI repository&lt;/a&gt;{:target=&amp;quot;_blank&amp;quot;}. Models I used: k-means clustering.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&amp;quot;Crescents&amp;quot; synthetic dataset which is impossible with k-means clustering. Download: &lt;a href=&quot;public/files/ml_data/crescents.csv&quot;&gt;crescents.csv&lt;/a&gt;. Dataset from &lt;a href=&quot;https://www.math.ucdavis.edu/%7Estrohmer/&quot;&gt;Thomas Strohmer&lt;/a&gt;{:target=&amp;quot;_blank&amp;quot;} at UC Davis. Models I used: diffusion maps.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;dimensionality-reduction&quot;&gt;Dimensionality Reduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Reduce the dimensionality of 32x32 grayscale face images. Download: &lt;a href=&quot;public/files/ml_data/yale_faces.zip&quot;&gt;yale_faces.zip&lt;/a&gt;. Dataset originally from Yale, collected by &lt;a href=&quot;https://www.math.ucdavis.edu/%7Estrohmer/&quot;&gt;Thomas Strohmer&lt;/a&gt;{:target=&amp;quot;_blank&amp;quot;} at UC Davis.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 

</feed>
